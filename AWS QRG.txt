	===================================================
	General notes:
	===================================================
	CloudFront SSL:
		HTTPS between viewers and CloudFront
			– You can use a certificate that was issued by a 3rd party trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers.
			– You can use a certificate provided by AWS Certificate Manager (ACM)
		HTTPS between CloudFront and a custom origin
			– If your origin is an ELB load balancer, you can use a certificate provided by 3rd party and ACM
			– If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a 3rd party trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.
			
	
	===================================================
	IAM Policy Evaluation: ESR-PSI:
	===================================================
		1. Explicit Deny
		2. Service Control Policies
		3. Resource-based Policies (ALLOW)
		4. Permissions Boundaries
		5. Session Policies 
		6. Identity Policies
		
	===================================================
	Signature Version 4 signing process
	===================================================
		- SigV4 is the process to add authentication information to AWS API requests sent by HTTP
		- For security, most requests to AWS must be "SIGNED" with an access key (= Access key ID + Secret access key)
		- SIGNING PROCESS:
			step 1: Create a canonical request.
					Canonical request = HTTPRequestMethod +  	// GET
										CanonicalURI + 			// /documents%2520and%2520settings/
										CanonicalQueryString + 	// Action=ListUsers&Version=2010-05-08
										CanonicalHeaders + 		// 
										SignedHeaders + 
										HexEncode(Hash(RequestPayload))
			step 2: Use the canonical request and additional metadata to create a request string that will be signed
			step 3: Derive a signing key from your AWS secret access key. 
			step 4: Then use the signing key, and the payload string from step-2, to create a signature.
			step 5: Add the resulting signature to the HTTP request in a header or as a query string parameter.


	===================================================
	AWS Config:
	===================================================
		- Key Use-cases:
			- View current snapshot of configuration of resources in your AWS account 
			- Track historical how resource configurations were modified over a time-scale and by whom 
			- Relationship & impact analysis by tracking relationship between resources in your AWS accounts (like 3 EC2s using a specific security group)
			- Provide visibility into resource consumption by receiving notifications whenever a resource is created, modified, or deleted
			- View AWS resource compliance history (compliance as it pertains to Config Rules for desired settings)
			- For On-premise SSM managed instances, AWS config can record applications, AWS components (CLI, SDK), Instance Info (OS, Domain), Network config (IP, Mask), Windows Updates
			
		- Resources includes all of the major AWS resources like compute, storage, CI/CD, security etc. including IAM user, IAM groups
										And
										AWS SSM managed-instances
		
		- How does Config work?
				~ AWS Config has a functionality called "CONFIG RECORDER" that can be enabled at an AWS account level
				~ When you turn on AWS Config Recorder, it first discovers the supported AWS resources that exist in your account 
				~ And, generates a "Configuration Item" for each discovered resource
				~ And, generates a "Configuration Item" for each resource related to the discovered resource
				~ When a resource is modified, the recorder generates a new "Configuration Item" for that resource and new "Configuration Item"(s) for each of the related resource
				~ the STREAM of "Configuration Items" is delivered to DELIVERY CHANNEL. The channels are an S3 bucket and an SNS topic
				~ Therefore, 
					a resource = 
						Config Item (t) + Config Item (t-1) + Config Item (t-2) + Config Item (t-3) ..... + Config Item (when recorder was started)
					related res. (1) = 
						Config Item (t) + Config Item (t-1) + Config Item (t-2) + Config Item (t-3) ..... + Config Item (when recorder was started)
					related res. (1) = 
						Config Item (t) + Config Item (t-1) + Config Item (t-2) + Config Item (t-3) ..... + Config Item (when recorder was started)
				~ EXAMPLE: 
					AWS: 
						removing an egress rule from a VPC security group causes AWS Config to invoke a Describe API call on the security group. AWS Config then invokes a Describe API call on all of the instances associated with the security group. The updated configurations of the security group (the resource) and of each instance (the related resources) are recorded as configuration items.
					On-premises: 
						You can also use AWS Config rules to monitor software configuration changes and be notified whether the changes are compliant or noncompliant against your rules. For example, if you create a rule that checks whether your managed instances have a specified application, and an instance doesn't have that application installed, AWS Config flags that instance as noncompliant against your rule.
		
		- What is a "Configuration Item"?
				  Metadata = Version Id + Time of CI capture + State ID indicating ordering of the CI + Status indicating whether CI was captured corerectly
				+ Attributes = Resource ID + Resource Type + ARN + AZ + Time the resource was created + Key-Value tags
				+ Relationships = Description such as Amazon EBS volume vol-1234567 is attached to an Amazon EC2 instance i-a1b2c3d4
				+ Current Configuration = Information returned through a call to the Describe or List API of the resource	
		
		- What is AWS Config Rule?
				~ When a rule is defined, AWS Config will evaluate your resource for DESIRED SETTINGS (remember it is not a state, but desired settings)
				~ Rule is triggered periodically or when a configuration changes
				~ Rule is associated to an evaluation Lambda function
				~ The trigger calls the Lambda function that compares the current and desired settings
				~ The lambda function returns compliance status of the resource for that rule
		
		- Multi-Account Multi-Region Configuration Items Aggregation
				- An aggregator is an object within the aggregator AWS account
				- Multiple Source AWS account or a Source organization sends its stream of Config Items to the aggregator object within the aggregator account
				- As a source AWS account owner, authorization refers to the permissions you grant to an aggregator account and region to collect your AWS Config configuration and compliance data. Authorization is not required if you are aggregating source accounts that are part of AWS Organizations.
				- Two step process:
					** create an aggregator object in the aggregator AWS account
					** grant authorization to the aggregator account on the specific AWS accounts and their source regions from which data should be aggregated
						
		- CW Events:
				event type (detaile type) = API Call via Cloudtrail, Config Item Change, Rule Compliance Change, Rules Re-evaluation Status, Config Snapshot Delivery Status, Config History Delivery Status
						- Choose Config Configuration Item Change to get notifications when a resource in your account changes.
						- Choose Config Rules Compliance Change to get notifications when a compliance check to your rules fails.
						- Choose Config Rules Re-evaluation Status to get re-evaluation status notifications.
						- Choose Config Configuration Snapshot Delivery Status to get configuration snapshot delivery status notifications.
						- Choose Config Configuration History Delivery Status to get configuration history delivery status notifications.
		
		- Which resources are recorded?
				- By default, AWS Config records changes for all supported types in a region
				- AWS Config can also record global resources - IAM users, groups, roles, and customer managed policies
				- AWS Config can also record software inventory for managed instances
				- If you don't want AWS Config to record the changes for all supported resources, you can customize it to record changes for only specific types
				- If a resource is not recorded, AWS Config captures only the creation and deletion of that resource, and no other details, at no cost to you
				- If a recorded resource is related to a nonrecorded resource, that relationship is provided in the details page of the recorded resource
		
		- What if recorder is turned OFF?
				- After AWS Config stops recording a resource, it retains the configuration information that was previously captured, and you can continue to access this information
		
	===================================================
	Systems Manager (SSM):
	===================================================
		- Capabilities (OpAp AcShIn)
				~ Operations Management		OpsCenter, CloudWatch Dashboard, Explorer, Trusted Advisor, Personal Health Dashboard, Incident Mgr.
				~ Application Management	Resource Groups, Param Store, AppConfig
				~ Actions & Change			Automation, Maintenance Windows
				~ Shared Resources			Documents (CAPSPC - doc-types: Command, Automation, Policy, Session, Package, Change Calendar, CloudFormation)
				~ Instances & Nodes			Inventory, Activations, Managed Instances, Run, State Mgr, Patch Mgr, Session Mgr, Distributor Mgr, Config Mgr (MAI RSP SDC)
		
		- Managed Instance:
					- Machine configured for use with Systems Manager is managed instance
					- Machine Could be EC2, On-premise servers, VMs in other cloud environments
					- OS = Windows, macOS, Linux distributions, Raspbian
		
		- IAM users / groups managing Systems Manager:
			Example permissions that can be assigned to IAM users or user-groups are as follows: 
				AmazonSSMFullAccess 
				AmazonSSMReadOnlyAccess 
				AWSHealthFullAccess
				AWSConfigUserAccess
				CloudWatchReadOnlyAccess
		
		- EC2 Instance Profile for SSM:
			By default, SSM has no access to EC2 instances and other AWS resources
			Permission policies (resource based policies) to be assigned to EC2 instances via Instance Profiles for SSM management are as follows:
				AmazonSSMManagedInstanceCore (required) : 	allows an instance to use Systems Manager service core functionality
				AmazonSSMDirectoryServiceAccess 		: 	if you plan to join the instance to an AD managed by AWS DS
				CloudWatchAgentServerPolicy 			: 	If you plan to use EventBridge or CloudWatch Logs to manage or monitor your instance
		
		- Service-linked role for SSM:
			The Systems Manager service-linked role can be used for the following:
				~ The Systems Manager Inventory capability uses the service-linked role to collect inventory metadata from tags and resource groups
				~ The Maintenance Windows capability can use the service-linked role in some situations 
		
		- SSM Agent 
					- Software installed on managed instances
					- Agent <--> SSM Service communication:
							- The agent requests instructions from the SSM service, runs them, sends status and exec info to SSM service via Amazon Message Delivery Service 
							- If you monitor traffic, you will see your EC2s & on-premises servers or VMs in your hybrid environment communicating with ec2messages.* endpoints
					- To update/upgrade the SSM agent versions, options are as follows:
							- Managed instances checks for updates and downloads new version 
							* Run State Manager associations to automatically check (periodially) download and install SSM agents
							* Create a SSM maintenance window and execute RUN scripts to contain disruption 
					- SSM agent runs locally under the user "ssm-user"
							- ssm-user is created the first time a session is instantiated
							- ssm-user is placed in Linux sudoers file or Windows Administrators group 
					- Credentials required for the SSM agent agent to connect to SSM Service:
							- if agent is installed on EC2, agent gets permissions from instance profile
							- if agent is installed on on-prem servers or other cloud vms, agent uses IAM user credentials stored in /root/.aws/credentials and a service role is assigned to that IAM user
		
		- Incident Manager:
					- Manage IT incidents 
					- Components are :
							~ Engagement and escalation
							~ Runbook automation
							~ Response plans 
							~ Active collaboration 
							~ Incident tracking
		
		- State Manager:
					- Desired state management service for managed instances (EC2 and on-premise)
					- An association defines the "desired state" to be maintained on the instances
					- And the state can usually be achieved by one of the existing SSM documents
					- An association is defined as a combination of 
							~ the SSM document to apply along with parameters to pass
							~ the schedule for when the state is checked and the desired state is applied once or (re-)applied multiple times 
							~ target (=EC2, resource groups, auto scaling groups)
					- Association can control rate of concurrency (i.e. only certain number of instances can process an association simultaneously) and error thresholds (how many association executions are allowed to fail before Systems Manager stops the execution and waits for the next schedule) 
					- Example:
							1. Run Managed Object Format (MOF) files to enforce a desired state on Windows Server managed instances using the AWS-ApplyDSCMofs SSM document
							2. Run Ansible playbooks by using the AWS-ApplyAnsiblePlaybooks SSM document
							3. Run Chef recipes by using the AWS-ApplyChefRecipes SSM document
							4. Automatically update SSM Agent 
							5. Automatically update PV drivers on EC2 instances for Windows Server
							6. AWS-InstallApplications
							7. AWS-ConfigureAWSPackages
		
		- State Manager v/s Maintenance Windows
				- State Manager is the recommended capability if you want to maintain all managed instances in your account in a consistent state
				- Maintenance window is used when you want one or more tasks or documents to be run or PATCHING to happen at particular times
				- https://docs.aws.amazon.com/systems-manager/latest/userguide/state-manager-vs-maintenance-windows.html
		
		- Patch Manager
					- Patches both OS and Applications on an EC2 instance
					- Patches include Security fix (default) and other types
					- Install patches regularly within a maintenance window
					- Scan instances and report compliance i.e. patch status
					- A patch baseline defines which patches are approved for installation on your instances
					- Predefined patch baselines (non-customizable) 
							AWS-DefaultPatchBaseline (Windows)	
							AWS-WindowsPredefinedPatchBaseline-OS (Windows)	
							AWS-WindowsPredefinedPatchBaseline-OS-Applications (Windows)	
							AWS-AmazonLinuxDefaultPatchBaseline
							AWS-AmazonLinux2DefaultPatchBaseline	
							AWS-CentOSDefaultPatchBaseline	
							AWS-DebianDefaultPatchBaseline	
							AWS-MacOSDefaultPatchBaseline	
							AWS-OracleLinuxDefaultPatchBaseline	
							AWS-RedHatDefaultPatchBaseline
							AWS-SuseDefaultPatchBaseline	
							AWS-UbuntuDefaultPatchBaseline

					- How is patch selected?
						- Patch to be applied is determined by the OS of the EC2 instance
						- A Patch repository stores Patch Baseline for each Operating System (Amazon Linux, CentOS, Debian, RHEL, SLES, Ubuntu, Windows)
						- The SSM agent downloads the patches from the corresponding repos and uses the package manager to install the patch
					- How does it work?
						- Relies on the existence of an SSM agent on the EC2 instance
						- SSM agent on Linux   requires network connectivity to patch repositories
						- SSM agent on Windows requires network connectivity to WSUS or Windows Update Catalog
					
		
		- AWS Personal Health Dashboard
					- Displays recent, upcoming events organized by category AND full event log for past 90days for account
					- See section on Health integration with CW Events
		
		- AWS Trusted Advisor: #### CoPe SeSe Ft
					- Trusted Advisor inspects your AWS environment, makes recommendations on BEST PRACTICES for these areas: 
							1. Cost optimization	: like delete unused, idle resource
							2. Performance			: like provisioned throughput, and monitoring for overutilized Amazon EC2 instances
							3. Service Limits		: like max # of resources or ops that apply to an account or a Region
							4. Security				: like recommending enablement of AWS security features, and review your permissions
							5. Fault Tolerance		: like recommending to take advantage of auto scaling, health checks, multi-AZ Regions, and backup capabilities
					- Support plans:
							Basic, Developer: 		Checks in Service Limits and 6 security checks are available
													1. Service Limits
													2. S3 Bucket Permissions
													3. Security Groups – Specific Ports Unrestricted
													4. IAM Use
													5. MFA on Root Account
													6. EBS Public Snapshots
													7. RDS Public Snapshots
							Business, Enterprise: 	All checks, Cloudwatch events
					- Notifications are sent every week to identified set of individuals
					- Color code for check results
						Green 	– Trusted Advisor doesn't detect an issue for the check.
						Yellow 	– Trusted Advisor detects a possible issue for the check.
						Red 	– Trusted Advisor recommends an action for the check.
					- Trusted Advisor uses IAM service-linked role called "AWSServiceRoleForTrustedAdvisorReporting" to execute against organization & its members  
		
		- Automation:
			- Service operates by using the context of the user who invoked the execution
			- Primary component:
				~ Automation Runbook 
					- is a SSM document of type "Automation"
					- defines the steps that SSM performs on your managed instances and AWS resources
					- each step is known as a 'Runbook action'
			- Other Runbook components:
				- Action: has the input, behavior and output of the step
				- Quota: Each AWS account can run 100 automation runbooks concurrently
				- Queue Quota: Request for runbook runs beyond the concurrent quota is queued. Upto 1000 requests in the queue 
			- Sample runbooks:
					~ AWS-StopEC2InstanceWithApproval: runbook to request one or more IAM users to approve instance stop action
					~ AWS-StopEC2Instance: runbook to stop instances on a schedule via EventBridge/CW-Events or Maintenance Tasks
					~ AWS-UpdateCloudFormationStackWithApproval: runbook to update stack created by CloudFormation with approval from 1 or more IAM users
					~ AWS-RestartEC2InstanceWithApproval: runbook to target an AWS resource group that includes multiple instances with velocity control 
					~ AWS-UpdateLinuxAmi: runbook to create golden AMIs from a source Linux AMI 
					~ AWS-UpdateWindowsAmi: runbook to create golden AMIs from source Windows AMI
					~ AWSSupport-TroubleshootSSH: runbook installs EC2Rescue tool and uses it to check and attempt to fix remote SSH connection issue
					~ AWSSupport-ExecuteEC2Rescue: runbook to run the EC2Rescue tool to recover impaired instances
	
	
	===================================================
	EC2Rescue
	===================================================
		- gathering syslog and package manager logs, collecting resource utilization data, and diagnosing/remediating known problematic kernel parameters and common OpenSSH issues.
	
	
	===================================================
	12factor: (CoDeCo BaBu ProPor ConDi LoDev Ad)
	===================================================
		Codebase:			One codebase tracked in revision control, many deploys
		Dependencies:		Explicitly declare and isolate dependencies
		Config:				Store config in the environment
		Backing services:	Treat backing services as attached resources
		Build,release,run: 	Strictly separate build and run stages
		Processes:			Execute the app as one or more stateless processes
		Port binding:		Export services via port binding
		Concurrency:		Scale out via the process model
		Disposability:		Maximize robustness with fast startup and graceful shutdown
		Dev/prod parity:	Keep development, staging, and production as similar as possible
		Logs:				Treat logs as event streams
		Admin processes:	Run admin/management tasks as one-off processes
	===================================================
	6Rs of Migration:
	===================================================
		1. Retain
		2. Rehost	- lift and shift
		3. Replatform- lift, tinker and shift
		4. Refactor / Rearchitect
		5. Repurchase
		6. Retire
	===================================================
	Default credential provider chain looks for credentials in this order:
	===================================================
		- specified in the code
		- environment variables
		- .aws directory
		- ECS container credentials
		- EC2 instance role
	===================================================
	ElasticbeanStalk deployment policy
	===================================================
		- Key objects: App (high level container), App Version, Environment, Env Tier (Web or Worker), Saved Configurations, Platform
		- 5 deployment strategies
			- All at once
			- Rolling
			- Rolling with additional batches
			- Immutable
			- Traffic splitting
						
	===================================================
	CodeBuild
	===================================================
		In addition to CW:
			~ codecommit can natively send notifications to sns for branch & tags-CRUD, comments-commits & on-pull-requests, pull requests-source upated/created/status changed/merged
			~ codecommit can natively send trigger-info to sns, lambda for all-repo-events, create/delete of branch/tag, push-existing-branch
		- Build status tracking:
				~ SUBMITTED, QUEUED, PROVISIONING, DOWNLOAD_SOURCE, INSTALL, PRE_BUILD, BUILD, POST_BUILD, UPLOAD_ARTIFACTS, FINALIZING, COMPLETED 
		- buildspec.yml
						version: 0.2
						run-as: Linux-user-name
						env:
							  shell: shell-tag
							  variables:
								key: "value"
								key: "value"
							  parameter-store:
								key: "value"
								key: "value"
							  exported-variables:
								- variable
								- variable
							  secrets-manager:
								key: secret-id:json-key:version-stage:version-id
							  git-credential-helper: no | yes
						proxy:
							  upload-artifacts: no | yes
							  logs: no | yes
						batch:
							  fast-fail: false | true
							  # build-list:
							  # build-matrix:
							  # build-graph:
						phases:
							  install:
								run-as: Linux-user-name
								on-failure: ABORT | CONTINUE
								runtime-versions:
								  runtime: version
								  runtime: version
								commands:
								  - command
								  - command
								finally:
								  - command
								  - command
							  pre_build:
								run-as: Linux-user-name
								on-failure: ABORT | CONTINUE
								commands:
								  - command
								  - command
								finally:
								  - command
								  - command
							  build:
								run-as: Linux-user-name
								on-failure: ABORT | CONTINUE
								commands:
								  - command
								  - command
								finally:
								  - command
								  - command
							  post_build:
								run-as: Linux-user-name
								on-failure: ABORT | CONTINUE
								commands:
								  - command
								  - command
								finally:
								  - command
								  - command
						reports:
							  report-group-name-or-arn:
								files:
								  - location
								  - location
								base-directory: location
								discard-paths: no | yes
								file-format: report-format
						artifacts:
							  files:
								- location
								- location
							  name: artifact-name
							  discard-paths: no | yes
							  base-directory: location
							  exclude-paths: excluded paths
							  enable-symlinks: no | yes
							  s3-prefix: prefix
							  secondary-artifacts:
								artifactIdentifier:
								  files:
									- location
									- location
								  name: secondary-artifact-name
								  discard-paths: no | yes
								  base-directory: location
								artifactIdentifier:
								  files:
									- location
									- location
								  discard-paths: no | yes
								  base-directory: location
						cache:
							  paths:
								- path
								- path

	===================================================
	CodeDeploy	
	===================================================
		- Similar to CodeBuild, in addition to CW:
			~ CodeDeploy can natively trigger notifications to SNS Topics for all events related to a Deployment or a Deployment Group
		- CodeDeploy does not provision resources AND therefore, does not support Blue/Green deployment on On-premise environment
		- CodeDeploy agent continuously polls CodeDeploy service for deployment triggers and polls S3 for actual application code
		- CodeDeploy agent install: update, get ruby, get binary from aws-mgd s3, install, sudo service codedeploy-agent status
		- Application is the top-level container in CodeDeploy
			- Deployment is the actual deployment of the application (or version) to the Deployment Group
			- Compute platform = EC2/On-premise, ECS, Lambda
			- Deployment Group is essentially your environment. 
					~For EC2/ON-premise compute platform: Use tags to identify group members or could point to ASG or On-premise instances
			- Deployment type (how deployment should happen) and Deployment configurations (how traffic is to be shifted)
					- In-place deployment type
						- EC2/On-premise: All-at once/Half-at-a-time/One-at-a-time/Custom
						- ECS: All-at once/Half-at-a-time/One-at-a-time/Custom
						
					- Blue/Green deployment type
						- EC2: Requires ASG, ALB with two target groups
						- ECS: Requires ASG, ALB with two target groups
		- Environment variables:
				~ APPLICATION_NAME
				~ DEPLOYMENT_ID
				~ DEPLOYMENT_GROUP_NAME
				~ DEPLOYMENT_GROUP_ID
				~ LIFECYCLE_EVENT
		~ Rollback Options:
				~ Manual: manually deploy the previous version
				~ Automatic: rollback to prior version when deployment fails on one or more instances, OR if deployment succeeds but certain alarms are breached
		~ Rollback behavior with existing content:
				##### When automatic rollback is initiated, or when you manually initiate a redeployment or manual rollback, CodeDeploy first tries to remove from each participating instance all files that were last successfully installed. 
				~ CodeDeploy does this by checking the cleanup file:
							~ /opt/codedeploy-agent/deployment-root/deployment-instructions/deployment-group-ID-cleanup  
							~ C:\ProgramData\Amazon\CodeDeploy\deployment-instructions\deployment-group-ID-cleanup  (for Windows Server instances)
				~ If it exists, CodeDeploy uses the cleanup file to remove from the instance all listed files before starting the new deployment
				~ Now if files that weren’t part of a previous deployment appear in target deployment locations, you can choose what CodeDeploy does with them during the next deployment
						~ Fail the deployment — An error is reported and the deployment status is changed to Failed.
						~ Overwrite the content — The version of the file from the application revision replaces the version already on the instance.
						~ Retain the content — The file in the target location is kept and the version in the application revision is not copied to the instance.
	
		~ CodeDeploy on On-Premise Instances:
				~ To register an On-premise instance, 
					1. create an IAM user CodeDeployUser-OnPrem with permissions on S3 bucket to pull code
					2. use "register-on-premises-instance" command with an IAM User credential 
						$ aws deploy register --instance-name AssetTag12010298EX --iam-user-arn arn:aws:iam::444455556666:user/CodeDeployUser-OnPrem --tags Key=Name,Value=CodeDeployDemo-OnPrem --region us-west-2
					3. create following file to store credentials:
						Linux: /etc/codedeploy-agent/conf/codedeploy.onpremises.yml
						Windows: c:\ProgramData\Amazon\CodeDeploy\codedeploy.onpremises.yml
				~ To register multiple On-premise instances,use "register-on-premises-instance" command to register  and obtain IAM Role STS tokens periodically
				~ Once the instances are registered, create Deployment Group using the registered instances
				~ Thereafter, create and trigger Deployment 
						
		~ CodeDeploy on Lambda:
				- Uses Aliases to two versions (similar to the ALB having two target groups)
				- Lambda Traffic Shift 
					- All-at-once: traffic is shifted all at once to the new version the alias is pointing to
					- Canary configuration: traffic is shifted in  two-step process (10P-5M, 10P-10M, 10P-15M, 10P-30M)
					- Linear: traffic is shifted in equal increments after a set number of minutes (10P-1M, 10P-2M, 10P-3M, 10P-10M) 
				~ Deploy hooks:
					~ BeforeAllowTraffic: call another Lambda function to validate the new version before traffic is diverted
					~ AllowTraffic 		: not-callable
					~ AfterAllowTraffic	: call another Lambda function after the traffic starts flowing in (like monitoring etc)
				
	===================================================
	CodeDeploy Monitoring:
	===================================================
			- Cloudwatch alarms:
						You can create a CloudWatch alarm for an instance or Amazon EC2 Auto Scaling group you are using in your CodeDeploy operations.You can associate up to ten CloudWatch alarms with a CodeDeploy deployment group. If any of the specified alarms are activated, the deployment stops, and the status is updated to Stopped
			- Cloudwatch events:
						see below
			- Cloudwatch trail:
						CodeDeploy is integrated with CloudTrail, a service that captures API calls made by or on behalf of CodeDeploy in your AWS account and delivers the log files to an Amazon S3 bucket you specify. CloudTrail captures API calls from the CodeDeploy console, from CodeDeploy commands through the AWS CLI, or from the CodeDeploy APIs directly. Using the information collected by CloudTrail, you can determine which request was made to CodeDeploy, the source IP address from which the request was made, who made the request, when it was made, and so on. CodeDeploy records are written together with other AWS service records in a log file. Every log entry contains information about who generated the request. The user identity information in the log helps you determine whether the request was made with root or IAM user credentials, with temporary security credentials for a role or federated user, or by another AWS service.
			- SNS TOpic:
						You can add triggers to a CodeDeploy deployment group to receive notifications about events related to deployments or instances in that deployment group. These notifications are sent to recipients who are subscribed to an Amazon SNS topic you have made part of the trigger's action.

	===================================================
	CodeDeploy Hooks
	===================================================
		- Lambda (Hooks call Lambda function)
				- Start
				- BeforeAllowTraffic
				- AllowTraffic
				- AfterAllowTraffic
				- End
		- ECS (Hooks call Lambda function)
				- Start
				- BeforeInstall
				- Install
				- AfterInstall
				- AllowTestTraffic
				- AfterAllowTestTraffic
				- BeforeAllowTraffic
				- AllowTraffic
				- AfterAllowTraffic
				- End
		- EC2 (Hooks call scripts)
				- Start
				- BeforeBlockTraffic	(IN-PLACE)
				- BlockTraffic			(IN-PLACE)
				- AfterBlockTraffic		(IN-PLACE)
				- ApplicationStop
				- DownloadBundle
				- BeforeInstall
				- Install
				- AfterInstall 
				- ApplicationStart
				- ValidateService
				- BeforeAllowTraffic
				- AllowTraffic
				- AfterAllowTraffic
				- BeforeBlockTraffic	(BLUE-GREEN)
				- BlockTraffic			(BLUE-GREEN)
				- AfterBlockTraffic		(BLUE-GREEN)
				- End
 
	===================================================
	CodePipeline:
	===================================================
			- Pipeline is a series of Stages
			- Stages is composed of serial or parallel Actions	

			- CodePipeline Trigger:
					- Source of the trigger could be ECR, S3 (ideal = versioned S3), GitHub, CodeCommit
					- The actual trigger mechanism is either a 
							~ CloudWatch Event [*** RECOMMENDED ***]
							~ due to Pipeline periodically checking for changes
					- When Amazon S3 is the source provider for your pipeline, you may zip your source file or files into a single .zip and upload the .zip to your source bucket. You may also upload a single unzipped file; however, downstream actions that expect a .zip file will fail.
					
			- Action:
					- Serial actions have different runOrder
					- Parallel actions have same runOrder and are grouped into an "ActionGroup"
					- Actions are of 6 types: SBT DAI
								1. Source
								2. Build 
								3. Test 
								4. Deploy
								5. Approval 
								6. Invoke	'' Invoke action lets you invoke Lambda function
					- An Action usually executes in the region where pipeline runs, but an Action may execute in a region different from where the pipeline runs
					- An action has a provider (like CodeBuild, Jenkins, Custom, etc.)
					- Action Failure:
							~ failure is when an action is not completed successfully
							~ You can retry the latest failed actions in a stage without having to run a pipeline again from the beginning
							~ You retry the stage that contains the actions. All actions that are still in progress continue their work, and failed actions are triggered once again
					- Manual Approvals:
							~ When you add an approval action to a stage in the pipeline, the pipeline execution stops. And, someone with the required IAM permissions can approve or reject the action
							~ Examples: 
								You want someone to perform a code review or change management review before a revision is allowed into the next stage.
								You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released.
								You want someone to review new or updated text before it is published to a company website.
							~ IAM user can be notified via notification to SNS Topic:
								The message delivery can be email or SMS or SQS message or HTTP/HTTPS endpoints or AWS Lambda functions
								The message contains Specify a URL for Review, Comments for Approvers  or No Configuration Options
							~ IAM user needs to have "AWSCodePipelineApproverAccess" policy for approving or rejecting approval actions
			
			- Artifacts & Artifact Bucket(s):
					- Each action can ingest one or more input artifacts from a specific S3 bucket
					- Each action can egest one or more output artifacts into a specific S3 bucket
					- Artifacts are usually zipped up before storing
					- The S3 bucket with special name "codepipeline-<region>-<12-digit random number>"
								like codepipeline-us-west-2-899384784820
					- Cross-region actions in a pipeline: You must have one artifact bucket per Region where you plan to execute an action
					- Every input artifact for an action must match the output artifact of an action earlier in the pipeline, whether that action is immediately before the action in a stage or runs in a stage several stages earlier
					- AWS CFN: If you use S3 as a source repository, you must zip the template and template configuration files into a single file before you upload them to an S3 bucket. For other repositories, such as GitHub and AWS CodeCommit, upload artifacts without zipping them
			
			- Cross-Region Action:
					- A pipeline is scoped to executing in a region. Its Artifact bucket is also in a specific region
					- But you can have AWS-provided actions running in a different region in your pipeline
					- DEFINITON: When an AWS service is the provider for an action, and this action type/provider type are in a different AWS Region from your pipeline, this is a cross-Region action
					- When you create or edit a pipeline, you must have an artifact bucket in the pipeline Region and then you must have one artifact bucket per Region where you plan to execute an action
					- If you use the console to create a pipeline or cross-Region actions, default artifact buckets are configured by CodePipeline in the Regions where you have actions. When you use the AWS CLI, AWS CloudFormation, or an SDK to create a pipeline or cross-Region actions, you provide the artifact bucket for each Region where you have actions.
					- You CANNOT create cross-Region actions for the following action types:
						Source actions
						Third-party actions
						Custom actions
					- You CAN create cross-region actions for :
						Build
						Test
						Deploy
						Approve 
						Invoke
					- When a pipeline includes a cross-Region action as part of a stage, CodePipeline replicates the input artifacts of the cross-Region action from the pipeline Region to the action's Region.

			- Action type:
					- A pre-configure action that you develop and provide to customers as a service is called as an "Action type"
					- One of the key components of an action type is the Executor configuration i.e. how does Pipeline execute that action. There are two models of integration between the Pipeline and action type:
						~ Lambda: The action type owner writes the integration as a Lambda function, which is invoked by CodePipeline whenever there is a job available for the action.
						~ JobWorker: The action type owner writes the integration as a job worker that polls for available jobs on customer pipelines. The job worker then runs the job and submits the job result back to CodePipeline by using CodePipeline APIs. This is a great way to get On-premise actions included in the pipeline.
			
			- Custom Action:
					- It is similar to Action Type, but is built for your own internal consumption
					
			
			- Invoke AWS Lambda function in an Action:
					- This seems similar to a custom action 
					- Action provider = Lambda function
					- Provide the following permissions to the Lambda function in addition to regular stuff like log access:
							~ codepipeline:PutJobSuccessResult (your fn calls codepipeline.putJobSuccessResult api call with the continuationToken)
							~ codepipeline:PutJobFailureResult (your fn calls codepipeline.putJobFailureResult api call with the continuationToken)
							
			- Invoke Cloudformation in an Action:
					- Action provider = CloudFormation
					- Actions could be 
							Create a stack 
							Update a stack
							Delete a stack 
							Replace a failed stack 
							Create change set 
							Replace change set 
							Execute change set 
					- Outputs from the CloudFormation stack are converted into a json-formatted file and then zipped within the pipeline outout artifact
					- You identify two files in the Action configuration: 
							~ Cloudformation stack template file: which defines your stack 
							~ Cloudformation template configuration file: which defines Parameters, Tags and Stack Policies for your stack
							~ You also identify to the action: 
									~ CAPABILITY_IAM, CAPABILITY_NAMED_IAM (IAM resources with custom names), CAPABILITY_AUTO_EXPAND (nested stacks)
									~ If you don't specify either of these capabilities, AWS CloudFormation returns an InsufficientCapabilities error.
							~ If you use S3 as a source repository, you must zip the template and template configuration files into a single file before you upload them to an S3 bucket. For other repositories, such as GitHub and AWS CodeCommit, upload artifacts without zipping them
					- Parameter overrides let you specify template parameter values that override values in a template configuration file. 
					***************** The overriding values come from the CodePipeline artifact *******************************
					There are two functions that fetch information from the CodePipeline artifact
						~ Fn::GetArtifactAtt 	: gets the attribute of the CodePipeline artifact
								"Fn::GetArtifactAtt" : [ "artifactName", "attributeName" ] 
								They keyes are as follows: BucketName, ObjectKey, URL
						~ Fn::GetParam			: gets the value associated to the key name in the 
								"Fn::GetParam" : [ "artifactName", "JSONFileName", "keyName" ]
								example: "Fn::GetParam" : ["WebStackOutput", "stack-output.json", "URL"]

		
	===================================================
	CodeBuild Artifacts v/s CodePipeline Artifacts
	===================================================
			- These artifacts are different remember
			- CodeBuild artifact is the end result of the CodeBuild. It usually lands up in an S3 bucket that CodeBuild defines 
			- But then the same code is additionally copied to CodePipeline artifacts bucket as well
					
	
	===================================================
	Jenkins on AWS:
	===================================================
			- CI/CD tool that can replace one or more of the following CodeBuild, CodeDeploy, CodePipeline
			- Due to deep integration with AWS, Jenkins can replace one or more of the native CI/CD components
			
			- Jenkins on EC2
					~ Architecture includes Master & Slave nodes
							- Master is also called Server node
							- You can have one or more Master nodes
							- Slave is also called Build node or Worker node 
							~ A Build node contains an agent that communicates with the master server
					
					~ Each Jenkins node can get deployed to EC2 instance
							~ All nodes can run on the same machine 
							~ Nodes could be distributed across many machines
							~ A Build node contains an agent that communicates with the master server
							
					~ Master node configuration: 
							- Master node configuration stored in $JENKINS_HOME directory and not a database. Hence, redundancy is difficult
									- If failures detected on EC2 instance, 
											- then Cloudwatch alarm for "metric=StatusCheckFailed:System and statistic=Average" can trigger Instance recovery  - OR ASG of size=1 with a Golden AMI could be used
									- Alternatively, $JENKINS_HOME can be mounted on an EFS
					
					~ Worker node:
							- The Worker node can be run on Spot instances 
							- The Worker node runs a Worker agent. This eliminates the need to install full Jenkins package 
							- The agent enables bi-directional traffic between Master and Worker
							- Plan for Two Executors per Core 
							
					~ Build Process:
							- Master node schedules the jobs based on the "Jenkinsfile" and assigns a Build node to execute the same
			
			- Jenkins on ECS
					
					~ Create a Jenkins Master Service 
							- Use official Jenkins Master docker image as the base
							- Push the image to ECR 
							- Persist the $JENKINS_HOME to data volume container
							
					~ Create a Jenkins Worker Service	
							- Use official JNLP docker image as the base 
							- Label the Jenkins worker
							- When creating a job in the Jenkins master you can specify which type of Amazon ECS worker you want for the job. Just specify the label used in the Amazon ECS worker template configuration in the Restrict where this Project can run field
			
			- Jenkins with AWS 
				Jenkins provides many plugins for AWS services 
					~ S3 plugin can be used to pull and push data to s3 (example: backup $JENKINS_HOME directory to S3)
					~ CodeBuild plugin enables build jobs to be sent to CodeBuild instead of Jenkins worker nodes. Plugin uses IAM users
					~ ECR plugin can be used to pull and push docker images to ECR
					~ ECS plugin can be used to start ECS task/service using a docker image
					~ Jenkins can push application and tests to AWS Device farm for testing across pool of physical devices
					~ CodeCommit plugin can be used to pull code/configs (example: Security specified IP addresses, allowed AMIs)
				AWS Services calling Jenkins
					~ CodePipeline can invoke Jenkins to build applications 
	
	
	===================================================
	Blue/Green deployment on AWS
	===================================================
			- 


	
	===================================================
	CloudWatch Logs Subscriptions:
	===================================================
		- A subscription filter could be created on a cloudwatch log and the resuting events could be streamed to downstream recipients
		- Downstream receipients are the FOLLOWING THREE:
			- trigger Lambda
			- send to Kinesis Stream
			- send to Kinesis Firehose
	
	===================================================
	CloudWatch Events can trigger:
	===================================================
		Downstream receipients could be:
			- trigger Lambda
			- send to Kinesis stream
			- send to Kinesis Firehose
			- and other EC2 services

	===================================================
	Cloudwatch Event type:
	===================================================
				Event triggered at a particular day/time
				Event triggered on a schedule
				AWS Management Console Sign-in can trigger CW Events
				CodePipeline:
					state changes at following levels = Pipeline, Stage, Action:
						state change event = STARTED, FAILED, SUCEEDED, RESUMED, CANCELED, SUPERSEDED
				EC2 Instance:
					event type = EC2 Instance State-change Notification
							This example is for an instance in the pending state. The other possible values for state include running, shutting-down, stopped, stopping, and terminated.
				ECR:
					event type = ECR Image Action - PUSH  [values = SUCCESS, FAILURE]
					event type = ECR Image Action - DELETE  [values = SUCCESS, FAILURE]
					event type = ECR Image Scan [returns count for various severity levels in the "finding-severity-counts" file]
				ASG:
					Launch successful, Launch unsuccessful, Terminate successful, Terminate unsuccessful, 
					Launch lifecycle Action(i.e. PENDING:WAIT state / Detail type = EC2 Instance-launch Lifecycle Action), 
					Terminate lifecycle Action(TERMINATING:WAIT / Detail type = EC2 Instance-terminate Lifecycle Action), 
					EC2 Auto Scaling Instance Refresh Checkpoint Reached
				Config:
					event type = API Call via Cloudtrail, Config Item Change, Rule Compliance Change, Rules Re-evaluation Status, Config Snapshot Delivery Status, Config History Delivery Status
							Choose Config Configuration Item Change to get notifications when a resource in your account changes.
							Choose Config Rules Compliance Change to get notifications when a compliance check to your rules fails.
							Choose Config Rules Re-evaluation Status to get re-evaluation status notifications.
							Choose Config Configuration Snapshot Delivery Status to get configuration snapshot delivery status notifications.
							Choose Config Configuration History Delivery Status to get configuration history delivery status notifications.
				Health:
						- does not deliver public events from service health dashboard that affects all AWS customers
						- only delivers account specific Health events
								- event rules for region specific Health events must be created in that region
								- event rules for global Health events must be created in US-EAST region
						- AWS_RISK_CREDENTIALS_EXPOSED is an event generated when AWS proactively detects IAM keyes in popular repositories
						- You can choose the following types of targets when using CloudWatch Events as a part of your AWS Health workflow:
								~ AWS Lambda functions
								~ Amazon Kinesis Data Streams
								~ Amazon Simple Queue Service (Amazon SQS) queues
								~ Built-in targets (CloudWatch alarm actions)
								~ Amazon Simple Notification Service (Amazon SNS) topics
									For example, you can use a Lambda function to pass a notification to a Slack channel when an AWS Health event occurs. Or, you can use Lambda and CloudWatch Events to send custom text or SMS notifications with Amazon SNS when an AWS Health event occurs.
				OpsWorks:
					Event-type= OpsWorks Instance State Change, OpsWorks Command State Change, OpsWorks Deployment State Change, OpsWorks Alert
				Code Build:
					Event-type= CodeBuild Build State Change (IN_PROGRESS,SUCCEEDED,FAILED,STOPPED)
								CodeBuild Build Phase Change
									completed-phase:SUBMITTED,PROVISIONING,DOWNLOAD_SOURCE,INSTALL,PRE_BUILD,BUILD,POST_BUILD,UPLOAD_ARTIFACTS,FINALIZING
									completed-phase-status: TIMED_OUT,STOPPED,FAILED,SUCCEEDED,FAULT,CLIENT_ERROR
				CodeDeploy:
						- Event-type = state changes in a Deployment or any of the instances in a Deployment Group
						- You can choose the following types of targets when using CloudWatch Events as a part of your AWS CodeDeploy workflow:
								~ AWS Lambda functions
								~ Amazon Kinesis Data Streams
								~ Amazon Simple Queue Service (Amazon SQS) queues
								~ Built-in targets (CW Alarm Actions,EC2 CreateSnapshot API call, EC2 RebootInstances API call, EC2 StopInstances API call ,EC2 TerminateInstances API call)
								~ Amazon Simple Notification Service (Amazon SNS) topics
										For example, Use a Lambda function to pass a notification to a Slack channel whenever deployments fail.Push data about deployments or instances to a Kinesis stream to support comprehensive, real-time status monitoring.Use CloudWatch alarm actions to automatically stop, terminate, reboot, or recover Amazon EC2 instances when a deployment or instance event you specify occurs.
				AWS Account or AWS Organization can send cloudwatch events to another account:
					A "sender" AWS account or "sender" AWS organization can send cloudwatch events to a "receiver" AWS account
					For this to happen, 3 key steps are to be undertaken:
						- the "receiver" AWS account grants permissions to its "default event bus" to the "sender" account-id and/or the "sender" organization-id
						- the "sender" AWS account, in it's cw event rule, mentions the target as "Event bus of another AWS account" and mentions the receiver account-id
						- while creating the cw rule in the "receiver" AWS account, edit the cw event pattern rule to manually include the account-id of the "sender" accounts
		
				AWS RDS:
					- Four categories (below) with multiple event types under it:
							DB Instance	
							DB Snapshot 
							DB Security Group
							DB Parameter Group
		
	===================================================
	***THREE OPTIONS*** for S3 Event notifications:
	===================================================
			s3 natively 
				gives object level event notification
				DOES NOT give bucket level notification

			cw events gives S3 notifications at two levels
				** bucket level event notification
				*--* object level event notification **** requires cloudtrail on the S3 bucket to be enabled *****
						dependency on cloudtrail (see following point)

			cloudtrail provides notification of three types and can have downstream targets like S3, Lambda, DynamoDB in the same or different account
				** management events for s3
				*--* data events for S3 exclusively. This setup is needed if you want CW events for object-level activity
						in the cloudtrail definition for S3:
					- 5 event selectors can be defined on the S3 buckets. Each event selector can have upto 50 destination buckets
				

	===================================================
	CloudTrail supports:
	===================================================
	3 types of events:
		- Data events for S3 exclusively
		- Management events for all  AWS services including S3
		- Insights
	
	- CloudTrail MANAGEMENT EVENTS vs Cloudwatch EVENTS
			CloudTrail 
				- captures events regarding API calls
				- cloudtrail does not emit these events to cloudwatch (this is one of the few services that does not send data)
				
			Cloudwatch
				- most services emit events regarding changes in them to cloudwatch, but there are exceptions like cloudtrail, receiving public events from service health dashboard
				- for exceptions like cloudtrail which do not emit its captured events to cloudwatch, you can still use the event-type = "AWS API Call via CloudTrail" to fetch the cloudtrail events. Rules created using the method works only in the region where the service created the event.
				- cloudwatch event rule:
					- rule is triggered by a matching "event pattern" or by a time-based scheduled. A triggered rule calls the "target": 
						- event pattern = AWS service name + event type + operations
						- target

	
	===================================================
	AWS service-specific service-linked roles:
	===================================================
			- A service-linked role is a unique type of IAM role that is linked directly to an AWS service
			- Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf
			- The linked service also defines how you create, modify, and delete a service-linked role. A service might automatically create or delete the role. It might allow you to create, modify, or delete the role as part of a wizard or process in the service. Or it might require that you use IAM to create or delete the role.
			- To allow an IAM entity to create a specific service-linked role, the action is iam:CreateServiceLinkedRole
			- To allow an IAM entity to attach a specific service-linked role, the actions are iam:AttachRolePolicy, iam:PutRolePolicy
			- To allow an IAM entity to delete a specific service-linked role, the actions are iam:DeleteServiceLinkedRole, iam:GetServiceLinkedRoleDeletionStatus
			- To allow an IAM entity to pass a specific service-linked role, the actions are iam:ListRoles, iam:PassRole
 
	AWS Organization service's access on the member accounts:
			- when you create an account in an organization or you accept an invitation to join your existing account to an organization, AWS Organizations provisions the member account with a service-linked role named "AWSServiceRoleForOrganizations".
			- this "AWSServiceRoleForOrganizations" role exists in every member account that is connected to the organization
			- only the AWS Organizations service itself can assume this "AWSServiceRoleForOrganizations" role. (trust policy)
			- the "AWSServiceRoleForOrganizations" role has permissions that allow AWS Organizations to create service-specific "IAM service-linked roles" for other AWS services
	
	AWS Services accessing AWS Organization and its member accounts:
			- "Trusted" AWS services can have "trusted" access to an AWS org and its member accounts on your behalf.
			- To grant an AWS service "trusted" access to an organization and its member account, 
				the user 
					- must possess following permissions: 'organizations:EnableAWSServiceAccess', 'organizations:ListAWSServiceAccessForOrganization' 
					- logs into AWS organization console for the target organization 
					- selects the to-be trusted service and
					- chooses "enable trusted access"
			- When trusted access is enabled for "trusted" service, the "trusted" service-specific "IAM service-linked role" is created by the AWS Organization in each of its member accounts
			- And this "trusted" service-specific "IAM service-linked role" grants the requisite permission to the "trusted" service to perform its duties in the member accounts
			
	
	===================================================
	AWS Inspector templates:
	===================================================
		Common Vulnerabilities & Exposure
		CIS OS Security Configuration Benchamr
		Network reachbility
		Security best practices
	
	
	===================================================
	ASG:
	===================================================
		Spot instances can be terminated/stopped when avail capacity drops at 2-minutes interruption notice
		Lifecycle Hooks:
			- Scale-out event
				- New instance launched
				- Lifecycle hook puts into PENDING:WAIT state
				- Custom script executes
				- Custom script sends complete-lifecycle-action CLI command or Wait timesout (1 hour=default)
				- Instance enters PENDING:PROCEED state
				- Instance enters In-service state
				- Instance counted within aggregated cloud watch metrics
				- ASG health check grace period starts
				- Simple scaling cooldown period starts
				- If ASG associated to LB, instance registered with LB
				- LB health check starts
				- ASG health check grace period expires
				- ASG health checks begin
			- Scale-in event
				- Instance termination begins (processes terminate)
				- Instance deregistered from LB and stops accepting new connections
				- Connection draining begins
				- Drain period finishes
				- Instance enters TERMINATING:WAIT state
				- Custom script executed or Wait timesout (1 hour=default)
				- Instance enters TERMINATING:PROCEED state
				- Instance NOT counted within aggregated cloud watch metrics
				- Instance enters TERMINATED state
			Standby state:
					~ InService -> StandBy -> (ExitStandby) -> Pending -> InService
					~ InService -> StandBy -> Terminated  
					~ Instances in StandBy are still part of the Auto Scaling group, but de-registered from the LB and Health Check and do not actively handle application traffic
			Detached state:
					~ InService -> Detached
			Auto scaling processes:
				These can be suspended during an autoscaling operation
					~ Launch			: adds a new Amazon EC2 instance to an Auto Scaling group
					~ Terminate 		: removes an Amazon EC2 instance from the group
					~ AddToLoadBalancer : adds instances to the attached load balancer target group or Classic Load Balancer when they are launched
					~ AlarmNotification : Accepts notifications from CloudWatch alarms that are associated with the group's scaling policies
					~ AZRebalance 		: Balances EC2 instances evenly across all of the specified Availability Zones when the group becomes unbalanced
					~ HealthCheck		: Checks the health of the instances and marks an instance as unhealthy
					~ InstanceRefresh	: Terminates and replaces instances using the instance refresh feature
					~ ReplaceUnhealthy	: Terminates instances that are marked as unhealthy and then creates new instances to replace them
					~ ScheduledActions	: Performs the scheduled scaling actions and turn on predictive scaling

	
	===================================================
	AWS Step Functions:
	===================================================
		- A Step Function is a serverless orchestration service that combines Lambda functions and other AWS services to build business-critical applications
		- Step Functions is composed of state machines and tasks
		- A state machine is a workflow 
		- Each step in the workflow is state
		- A state represents a single unit of work that another AWS service performs
		- An Execution is an instance of running your state machine		
		- Two types of executions -
		
				Standard: exactly-once execution which can run upto 1 year; priced per state transition
				Express: atleast-once execution which can run upto 5 minutes; priced per number and duration of executions
		- Types of state:
				~ Task:		actual work done within a lambda function or calling of a AWS service (like DynamoDB)
					~ Activity: type of Task where work is performed by an "activity worker"  hosted on EC2, ECS, etc
				~ Pass:		pass input to output i.e. a passthrough
				~ Choice:	binary choice between two paths
				~ Retry:	retry a state
				~ Catch:	catch if a state fails
				~ Parallel:	invoke known number of multiple states in parallel
				~ Map:		invoke unknown number of multiple states in parallel (like pack-ship each item in the order. The number of items is not fixed)
				~ Succeed:	stops an execution successfully
				~ Fail:		stops the execution of the state machine and marks it as a failure
				~ Wait:		delays the state machine from continuing for a specified time
		- Transitions:
				~StartAt points to the first state in the state machine
				~Next points to the next state in the state machine 
		- Service integration patterns:
				1. Request a response (default)
					Call a service, and let Step Functions progress to the next state after it gets an HTTP response.
				2. Run a job (.sync)
					Call a service, and have Step Functions wait for a job to complete.
				3. Wait for a callback with a task token (.waitForTaskToken)
					Call a service with a task token, and have Step Functions wait until the task token returns with a callback. Ideal for human interventions
	
	
	===================================================
	CloudFormation:
	===================================================
			- Concepts: 
				- Stacks are REGION bound
				- Logical Resource in a Template, Physical Resource in a Stack
				- Stack defined in a template
						
			- Sections of Template:
				Format Version:
					'
				Description:
					'
				Metadata:		// Include arbitrary JSON, YAML. Keys are AWS::CloudFormation::Init, AWS::CloudFormation::Interface, AWS::CloudFormation::Designer
					'
				Parameters:
					'
				Rules:
					'
				Mappings:
					'
				Conditions: 		// Conditions are defined in this section. Conditions are referenced in the Resources section
					'
				Transform:
					'
				Resources: 		// **** the ONLY mandatory section ****
					Interface	// Defines grouping and ordering of input parameters when they are displayed in the AWS CloudFormation console
					Designer	// Defines ordering of resource in the Cloudformation Designer
					Resource-1:
						CreationPolicy
						DeletionPolicy
						MetaData:
							AWS::CloudFormation::Init:
								configSets: 
									ascending: 
										- "config1"
										- "config2"
									descending: 
										- "config2"
										- "config1"
								config1: 
									packages: 
									:			'download and install pre-packaged applications and components. On Windows systems, supports only MSI installer.
									groups: 
									:			' create Linux/UNIX groups and to assign group IDs. The groups key isn't supported for Windows systems
									users: 
									:			' create Linux/UNIX users on the EC2 instance. The users key isn't supported for Windows systems
									sources: 
									:			' download an archive file and unpack it in a target directory on the EC2 instance
									files: 
									:			' create files on the EC2 instance- content can be either inline OR from a URL
									commands: 
									:			' run commands on the EC2 instance.
									services: 
									:			' which services should be enabled or disabled when the instance is launched. On Linux systems, this key is supported by using sysvinit. On Windows systems, it's supported by using the Windows service manager.
						UpdatePolicy
						UpdateReplacePolicy
						DependsOn
					Resource-2:
						CreationPolicy
						DeletionPolicy
						MetaData
						UpdatePolicy
						UpdateReplacePolicy
						DependsOn					
				Outputs:
					'
					
			- Transform section:
				- The optional Transform section specifies one or more macros that AWS CloudFormation uses to process your template
				- 5 macro types:
						AWS::CodeDeployBlueGreen transform: 	Enable ECS blue/green deployments through CodeDeploy on your stack.
						AWS::Include transform:					Insert template snippet in an Amazon S3 bucket
						AWS::SecretsManager transform:			Specify a Lambda function to perform secrets rotation
						AWS::Serverless transform:				Takes template written in AWS SAM syntax and transforms and expands it into a compliant CloudFormation template
						AWS::ServiceCatalog transform:			Enables users to reference outputs from an existing Service Catalog provisioned product
				
			- Pseudo Parameters:
				AWS::StackId
				AWS::StackName
				AWS::AccountId
				AWS::Partition
				AWS::Region
				AWS::URLSuffix
				AWS::NotificationARNs
				AWS::NoValue

			- Resources have attributes. 
				- special category of attribute is a Policy. For example: CreationPolicy, DeletionPolicy,UpdatePolicy, UpdateReplacePolicy,DependsOn,MetaData

			- Multiple stacks referencing each other is a 'Cross-stack reference scenario'
				- Use the intrinsic function Fn::ImportValue to import outputs exported by another stack
							Stack A Export
									Outputs:
									  PublicSubnet:
										Description: The subnet ID to use for public web servers
										Value:
										  Ref: PublicSubnet
										Export:
										  Name:
											'Fn::Sub': '${AWS::StackName}-SubnetID'  '''''LOOK AT THIS LINE '''''''''
							Stack B Import:
									Resources:
									  WebServerInstance:
										Type: 'AWS::EC2::Instance'
										Properties:
										  InstanceType: t2.micro
										  ImageId: ami-a1b23456
										  NetworkInterfaces:
											- GroupSet:
												- !ImportValue 
												  'Fn::Sub': '${NetworkStackNameParameter}-SecurityGroupID'
											  AssociatePublicIpAddress: 'true'
											  DeviceIndex: '0'
											  DeleteOnTermination: 'true'
											  SubnetId: !ImportValue 
												'Fn::Sub': '${NetworkStackNameParameter}-SubnetID'   '''''LOOK AT THIS LINE '''''''''
								
			- WaitCondition in a stack:
							- The CreationPolicy attribute of a specific resource can cause wait condition during the creation of the resource
							- A stack resource called AWS::CloudFormation::WaitCondition can introduce wait condition in a stack template
							- A WaitCondition resource has two required properties: Handle is a reference to a WaitConditionHandle declared in the template and Timeout is the number seconds for AWS CloudFormation to wait. 
							- EXAMPLE:
											"myWaitCondition" : {
												"Type" : "AWS::CloudFormation::WaitCondition",
												"DependsOn" : "Ec2Instance",				// trigger: This wait condition kicks in as soon as Ec2Instance resource is in CREATE COMPLETE
												"Properties" : {
													"Handle" : { "Ref" : "myWaitHandle" },	// resolves to a pre-signed URL that you can use to signal success or failure 
													"Count"  : 5							// default count is 1
													"Timeout" : "4500"						// seconds
												}
											}
											"myWaitHandle" : {								// definition of handle
												 "Type" : "AWS::CloudFormation::WaitConditionHandle",
												 "Properties" : {
												 }
											}
							- /opt/aws/bin/cfn-signal can be used to signal SUCCESS or FAILURE from an EC2 instance
					
			- Updating a stack:
					- When you initiate a stack update, AWS CloudFormation updates resources based on differences between what you submit and the stack's current template and parameters. 
					- If you update a resource property that requires the resource to be updated, it is updated but ....
					- If you update a resource property that requires that the resource be replaced, CloudFormation recreates the resource during the update. 
						~ Recreating the resource generates a new physical ID. 
						~ CloudFormation creates the replacement resource first, and then changes references from other dependent resources to point to the replacement resource. 
						~ By default, CloudFormation then deletes the old resource. Using the UpdateReplacePolicy, you can specify that CloudFormation retain or snapshot or just delete 
			- EC2 Helper scripts:
					- These are PYTHON scripts available for EC2 Instances 
					- /opt/aws/bin
							- cfn-init:			script retrieves and interprets resource metadata, install packages, create files, and start services from AWS::CloudFormation::Init
							- cfn-signal: 		script signals a CreationPolicy or WaitCondition to indicate whether EC2 instances have been successfully created or updated. 
														Best used to indicate readiness of software for service
							- cfn-get-metadata:	script retrieves metadata for a resource and prints it on standard out
							- cfn-hup:			script for a daemon that checks for updates to metadata (via UpdateStack) and execute custom user-specified-hooks (i.e. shell commands)
															/etc/cfn/cfn-hup.conf daemon configuration file, format follows
																	[main]
																	stack=<stack-name-or-id> 	
																	role=<IAM role associated with EC2>
																	region=<region-name>
																	umask=<Octal integer between 0 and 0777>
																	interval=<minutes|15minutes default>
																	verbose=<true|false>
															/etc/cfn/hooks.conf configuration file, format follows
																	[hookname]
																	triggers=post.add or post.update or post.remove
																	path=Resources.<logicalResourceId> (.Metadata or .PhysicalResourceId)(.<optionalMetadatapath>)
																	action=<arbitrary shell command>
																	runas=<runas user> 
															/etc/cfn/hooks.conf/hooks.d/<multiple config files>
																	To support composition of several applications deploying change notification hooks, you can place one or more additional hooks configuration files in the hooks.d directory. Format is same as /etc/cnf/hooks.conf file
				
			- Resource Attributes:
					1. CreationPolicy: 
						Associate the CreationPolicy attribute with a resource to prevent its status from reaching "CREATE_COMPLETE" until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded. To signal a resource, you can use the cfn-signal helper script or SignalResource API. Resources supporting CreationPolicy attrbutes are EC2::Instance, AppStream::Fleet, AutoScaling::AutoScalingGroup, CloudFormation::WaitCondition
					2. DeletionPolicy:
						If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. This capability doesn't apply to resources whose physical instance is replaced during stack update operations.
						Delete:		delete the resource (default) (for S3, all objects must be deleted for the bucket to be deleted)
						Retain:		retain the resource
						Snapshot: 	snapshot and then delete (Default for RDS::DBCluster, RDS::DBInstance) (Other resources:EC2::Volume,Neptune::DBCluster,Redshift::Cluster,EalstiCache::CacheCluster,ElastiCache::ReplicationGroup)
					3. DependsOn:
						Creation of a specific resource follows another
					4. MetaData:
						 Associate structured data with a resource. Use intrinsic functions (such as GetAtt and Ref), parameters, and pseudo parameters within the Metadata attribute to add those interpreted values
					5. UpdatePolicy:
						- For AutoScalingGroup, two types of policies are AutoScalingReplacingUpdate and AutoScalingRollingUpdate. 
								- If both policies are specified and if WillReplace is True, AutoScalingReplacingUpdate is executed. 
								- AutoScalingReplacingUpdate 
										During replacement, CloudFormation retains the old group until it finishes creating the new one. If the update fails, CloudFormation can roll back to the old Auto Scaling group and delete the new Auto Scaling group.
								- AutoScalingRollingUpdate updates instances in batches
										"MaxBatchSize" : 
											maximum number of instances in a batch
										"MinInstancesInService" : 
											minimum # of instances that must be in service within the Auto Scaling group while 	CloudFormation updates old instances
										"MinSuccessfulInstancesPercent" : 
											Specifies the percentage of instances in an Auto Scaling rolling update that must signal success for an update to succeed. You can specify a value from 0 to 100. CloudFormation rounds to the nearest tenth of a percent. For example, if you update five instances with a minimum successful percentage of 50, three instances must signal success.
										"PauseTime" : 
											Amount of time that CloudFormation pauses after making a change to a batch of instances to give those instances time to start software applications.
										"SuspendProcesses" : 
											Specifies the Auto Scaling processes to suspend during a stack update. Suspending processes prevents Auto Scaling from interfering with a stack update
										"WaitOnResourceSignals" :
											Specifies whether the Auto Scaling group must receive a signal from each new instance in the batch within the specified PauseTime before continuing the update.
					6. UpdateReplacePolicy:
						- UpdateReplacePolicy is only executed if you update a resource property whose update behavior is specified as Replacement, thereby causing CloudFormation to replace the old resource with a new one with a new physical ID
						- UpdateReplacePolicy differs from the DeletionPolicy attribute in that it only applies to resources replaced during stack updates.
						- 3 values are similar to the DeletionPolicy
							Delete:		delete the old resource (default) (for S3, all objects must be deleted for the bucket to be deleted)
							Retain:		retain the old resource
							Snapshot: 	snapshot the old resource and then delete it
						
			- Intrinsic Functions:
					Fn::Base64: valueToEncode
					Fn::Cidr [ ipBlock, count, cidrBits ]
					Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]
					Fn::GetAtt: [ logicalNameOfResource, attributeName ]
					Fn::GetAZs: region
					Fn::Join: [ delimiter, [ comma-delimited list of values ] ]
					Fn::Select: [ index, listOfObjects ] 
					Fn::Split: [ delimiter, source string ]
					Fn::Sub:
						  - String
						  - Var1Name: Var1Value
							Var2Name: Var2Value
					Fn::Transform:
						  Name : macro name
						  Parameters :
								  Key : value
					Conditions:
								Fn::If 
								Fn::Not 
								Fn::Equals
								Fn::And 
								Fn::Or
								!Condition conditionName
					Ref:
						The intrinsic function Ref returns the value of the specified parameter or resource.
						When you specify a parameter's logical name, it returns the value of the parameter.
						When you specify a resource's logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID.									
					
	===================================================
	AWS Serverless Application Model (SAM):
	===================================================
		- Serverless Application = 
									Lambda Functions + 
									Event Sources + 
									other AWS resources like APIs, Layer, S3 Apps, Application in Serverless App Repository, DynamoDB table
		- SAM is an open source extension to Cloudformation for serverless applications
		- TWO COMPONENTS:
				1. SAM Template File:
						- The template file follows the format of AWS Cloudformation file (format above in prior section)
								- The "Globals" is SAM-specific template section that defines the properties common to all your serverless functions, APIs, and dynamodb tables. All the AWS::Serverless::Function, AWS::Serverless::Api, and AWS::Serverless::SimpleTable resources inherit the properties that are defined in the section
						- File, when developed, represents the application architecture
						- Used to define the packaging and deployment of a single serverless application
						- YAML or JSON format
						- SAM Template is distinguished from the CloudFormation Template since SAM will have the following line:
							Transform:'AWS::Serverless-2016-10-31'
				2. SAM CLI:
						- SAM Template file is the INPUT to CLI
						- CLI packages, deploys and tests serverless applications
						- It can be used to test and step-through Lambda functions locally 
						- Sample commands:
								sam build
								sam deploy
								sam local invoke 
								sam local start-api
						- SAM CLI can integrate with other AWS tools:
								- AWS Cloud9 can be used to author, test and debug SAM apps
								- AWS CodePipeline, CodeBuild, CodeDeploy can orchestrate, build, and deploy a SAM app
								- AWS CodeStar can be used to setup project structure, code repository and CI/CD 
		
	===================================================
	Logs:
	===================================================
		Amazon S3 can store access logs for following services:
			CloudTrail
			CloudFront
			ALB
			S3
		Amazon CloudWatch can store access logs for following compute:
			CloudTrail logs
			VPC flow logs
			EC2
			ECS
			EKS
			Lambda
		
			
	===================================================
	Lambda:
	===================================================
		- Lambda Service invokes your functions in an execution environment via APIs
		  Lambda Service <=======> API endpoints <=======> Runtime + Extensions + Functions
									- Runtime API
									- Extensions API
									- Logs API

		 - Execution Environment = API Endpoints + Runtime + Extensions
				- API - 3 types:
					~ Logs API, Extensions API, Runtime API 
				- Runtime
					- Runtime = Amazon Linux OS + Programming Language binaries + SW Libraries
					- GoJaR - No.Net - PyCu: Go Java Ruby Nodejs .Net Pythin Custom
					- The runtime is paired with one of the Amazon Linux distributions
				- Extensions 
		- Phases in the Environment lifecycle:
				- Init: 
					Creates or unfreezes an environment with configured resources (like RAM, CPU, Network, etc), download code (fn & layers), initializes extensions, initializes runtime, and then runs the function’s initialization code
				- Invoke:
					Lambda invokes the function handler
				- Shutdown:
					This phase is triggered if the Lambda function does not receive any invocations for a period of time. In the Shutdown phase, Lambda shuts down the runtime, alerts the extensions to let them stop cleanly, and then removes the environment

		- Function
				- Function code is available in the Deployment package. Deployment package comes in two flavors:
						- is either a .zip file containing code and dependencies
							- Layer augments the deployment package
							- Layer is a zip file package containing libraries and other dependencies like configuration etc
							- Max 5 layers per function
							- Lambda extracts the layer contents into the /opt directory of the execution environment of the function
						- is either an OCI compatible container image containing OS, runtime, code and dependencies
							- Container Deployment packages DO NOT use layers 
							- All dependencies must be configured within the container image
							- AWS provides base container images 
							- You can your own container images and install software like AWS CLI, Docker Desktop to make them Lambda compatible
							- Function code is installed <project_dir>/app 
				- Add/Delete layer versions to a function:
						$ aws lambda update-function-configuration --function-name my-function --layers arn:aws:lambda:us-east-2:123456789012:layer:my-layer:3 arn:aws:lambda:us-east-2:210987654321:layer:their-layer:2 // ADD LAYERS
						$ aws lambda update-function-configuration --function-name my-function --layers [] // DELETE LAYERS
						$ aws lambda list-layers --compatible-runtime python3.8
				- Programming model:
						- Function has a HANDLER
						- Handler is invoked by the Runtime 
						- When the handler finishes processing the first event, the Runtime sends it another
						- Between invocations, functions' class and SDK clients initialized outside of handler stays in memory and hence gets reused
						- Runtime records separate XRAY subsegments for initialization and execution

		- Function invocation can have two qualifiers:
				- VERSION or 
				- ALIAS		
		- Function Version specific configuration:
				- "Latest" version is the unpublished version of the function
				- You can modify the following settings in the "Latest" version:
						- Code
						- Runtime
						- Handler 
						- Environment variables 
						- Tags
						- Execution role
						- Description 
						- Memory
						- Timeout 
						- VPC 
						- Database proxies 
						- Active tracing 
						- Concurrency (reserved, provisioned)
						- Asynchronous invocation
				- When you publish a version, the code and the settings above are locked to ensure a consistent experience for users of that version
				- Lambda Alias: 
						Q: Determining which version has been invoked?
						A: When you configure traffic weights between two function versions, there are two ways to determine the Lambda function version that has been invoked:
							~ CloudWatch Logs – Lambda automatically emits a START log entry that contains the invoked version ID to Amazon CloudWatch Logs for every function invocation. The following is an example: 19:44:37 START RequestId: request id Version: $version
							~ Response payload (synchronous invocations) – Responses to synchronous function invocations include an x-amz-executed-version header to indicate which function version has been invoked.

		
		- Concurrency:
				- Provisioned concurrency: to avoid latency due to initialization, provisioned concurrency pre-initializes and keeps some instances running all times
				- Reserved concurrency	 : cap on the number of concurrent instances of a function
		- Quotas
				- 1000 concurrent execution per region per account shared by all functions
				- 75GB max storage for deployment package of the uploaded function, versions, layers 
				- ECR limits apply to deployment package of the container image 
				- 250 ENIs per VPC shared with other AWS services
				- 128MB to 10GB in 1MB increments 
				- Allocates CPU equivalent to memory allocated (1769 MB = 1vCPU)
				- 15 minutes function timeout (3 seconds for default timeout)
				- 4KB environment variables 
				- 20KB resource based policies 
				- 6MB invocation payload (synchronous)
				- 256KB invocation payload (asynchronous)
				- 50MB .zip deployment package  
				- 250MB unzipped deployment package
				- 10GB container image deployment package
				- 512MB /tmp storage 


		- Functions invocation pattern: Synchronous, Asynchronous, Event Source Mapping
					***** Synchronous invocation:
						( AWS CLI writes the response from the function is response.json. If the function exits without error, the response is null. With erro, error object in json in the file)
							CLI
							Console
							API Gateway
							ALB
							CloudFront (Lambda@Edge)
							Cognito
							Lex
							Alexa
							Kinesis Data Firehose
					***** Asynchronous invocation: 
						(Uses special event queue; Error handling=2 more times,1 min-2mins spacing || if throtling on the queue, retry for upto 6 hours. Error handling can be defined for a function, alias or version)
							S3
							SNS
							SES
							CloudFormation
							CloudWatch Logs (subscription filters)
							CloudWatch Events
							CodeCommit
							[OPTIONAL] On-success or On-failure destinations (Failure is defined as message expiry or max number of processing retries attempted]
								Amazon SQS
								Amazon SNS
								AWS Lambda
								Amazon EventBridge
							[DLQs]
								A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed. However, a dead-letter queue is part of a function's version-specific configuration, so it is locked in when you publish a version.
					***** Event source mapping:
						("Event source mapping" = Special Lambda resources that reads streams and invokes a lambda function with a batch of one/more items. For streams, an event source mapping creates an iterator for each shard in the stream and processes items in each shard in order. If your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire. To ensure in-order processing, processing for the affected shard is paused until the error is resolved)
							Amazon SQS
							Amazon DynamoDB streams
							Amazon Kinesis streams
							Amazon MQ
							Amazon Managed Streaming for Apache Kafka
							self-managed Apache Kafka 
		- Lambda Execution Roles:
						AWSLambdaBasicExecutionRole – Permission to upload logs to CloudWatch.
						AWSLambdaDynamoDBExecutionRole – Permission to read records from an Amazon DynamoDB stream.
						AWSLambdaKinesisExecutionRole – Permission to read events from an Amazon Kinesis data stream or consumer.
						AWSLambdaMQExecutionRole – Permission to read records from an Amazon MQ broker.
						AWSLambdaMSKExecutionRole – Permission to read records from an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster.
						AWSLambdaSQSQueueExecutionRole – Permission to read a message from an Amazon Simple Queue Service (Amazon SQS) queue.
						AWSLambdaVPCAccessExecutionRole – Permission to manage elastic network interfaces to connect your function to a virtual private cloud (VPC).
						AWSXRayDaemonWriteAccess – Permission to upload trace data to X-Ray.
						CloudWatchLambdaInsightsExecutionRolePolicy – Permission to write runtime metrics to CloudWatch Lambda Insights.
		- Lambda Resource-based policies can be used to grant function and layer access to other AWS services, accounts
		- IAM Permissions that AWS Services can assume to invoke Lambda functions:
						AWSLambdaRole – Grants permissions to invoke Lambda functions.
						AWSLambda_FullAccess – Grants full access to Lambda actions and other AWS services used to develop and maintain Lambda resources. 
						AWSLambda_ReadOnlyAccess – Grants read-only access to Lambda resources. 
		
		- Lambda functions and VPCS:
			SCENARIO #1 - Lambda Accessing resources in a VPC?
							- If you want Lambda function to be able to access resources in your VPC, you can connect the function to the VPC
							- Traffic emanates from the Lambda function to access VPC resources
							- When you connect a function to a VPC, Lambda creates an ENI for each subnet in your function's VPC configuration. This process can take several minutes
							- Multiple functions can share a network interface, if the functions share the same subnet and security group
							- Lambda functions can't connect directly to a VPC with dedicated instance tenancy. To connect to resources in a dedicated VPC, peer it to a second VPC with default tenancy
							- Permissions required: ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface
							- Example: Lambda function connects to EFS in the AZ where EFS has mount points using NFS proto on port 2049
			SCENARIO #2	- VPC Resources accessing Lambda functions over private connections?
							- If you want your VPC resources to involke your Lambda function over private link, you expose Lambda as a VPC interface endpoint 
							- Traffic emanates from the VPC resources to invoke a Lambda function (Lambda function cannot use privatelink to access VPC resources)
							- VPC interface endpoint injects ENI into the VPC subnets
							- This will establish private connection between your VPC and Lambda function and your VPC resources can invoke the function 
							- Each interface endpoint is represented by one or more elastic network interfaces in your subnets. A network interface provides a private IP address that serves as an entry point for traffic to Lambda.
							- Lambda purges idle connections over time, so you must use a keep-alive directive to maintain persistent connections. Attempting to reuse an idle connection when invoking a function results in a connection error.
		- Rolling deployments for Lambda functions
				- CodeDeploy (and SAM uses CodeDeploy) uses Lambda Functions ALIAS to manage the rolling deployment and rollback
				
	===================================================	
	X-Ray Daemon:
	===================================================
		- Daemon runs on port 2000 listens on UDP for X-Ray trace/segment information from compute components. Daemon relays data to X-Ray AWS Service and hence requires permissions to write to AWS X-ray service
		- Integrates with EC2, ECS, Lambda, ElasticBeanstalk
			- EC2: Daemon binary for installation can be found in S3. Daemon runs on EC2 instance and uses permissions in EC2 instance profile. Add managed policies to EC2 instance profile
			- Lambda: Daemon is automatically run by Lambda
			- ECS: Daemon runs in its own container. Daemon container definition included in the task definition. Daemon container image in ECR or DockerHub. Add managed policies to your task role to grant the daemon permission to upload trace data to X-Ray. App containers can call Daemon container directly if the networking mode of the task is bridge(default). 
			- ElasticBeanstalk: In the .ebextensions/xray-daemon.config, set the following option:
					option_settings:
						aws:elasticbeanstalk:xray:
							XRayEnabled: true 
	===================================================
	General Notes:					
	===================================================
			Tags use-cases:
				Access control
				Billing
				Filtering
				Systems Manager / Automation
			IOPS measures the number of read and write operations per second, while throughput measures the number of bits read or written per second. 
			MTDT 
				= Maximum tolerable down-time 
				= RTO + WRT
				RTO = Recovery time needed to get the system back to Recovery Point Objective
				WRT = Work Recovery Time to bring things back from RPO to desired state (just before the disaster struck)
			In the AWS world, the resources are shared:
				Compute can be dedicated (but co-located) or shared 
				Storage is always shared with other tenants (if you want dedicated, go for the likes of NetApp storage in the AWS marketplace)
				Networking is always shared (if you want dedicated, go back to On-premise as AWS cannot server you)

			Edge locations: 
				cloudfront, R53, Lambda at Edge, Anycast IP addresses
				glacier policies - cannot change from 90 years to 50 years for existing archived objects. But net new objects can be archived at 50 years.
			Launch scripts:
					Does not run on reboot
					Runs only during launch (i.e. regular Power On)
			t-family:
				When you go beyond the point credits,
					t2 is throttled
					t3, t4: burstable beyond, but must pay
				1 credit: 1 full vCPU at full speed for 1 minute burns up 1 credit
			macOS types are available
			Auto-scaling group:
				can mix On-demand and spot instances
			Savings plan:
				I promise to spend $10000/mo. in compute, irrespective of whether it's managed vs unmanaged
			Traffic-related cost (general rule):
				Cross-Region traffic:
					Almost always incurs charges (exceptions: IAM, CloudFront traffic)
				Cross-AZ traffic:
					Incurs charges if the talking services are AZ-scoped service, but the traffic crosses AZ
					Does not incur charge if the talking services are region scoped services
			Scoping:
				RDS is scoped at AZ level
				DynamoDB is scoped at REGION level
			IAM Identity policies = Role-based access control
			IAM Resource policies = Rule-based access control
			You can only have one IAM ROLE hard-hat at a time (have you seen anyone in the construction site wearing multiple hard-hats?)
			Managed-permission v/s Role
				Managed permissions is cumulative, while Role assumption is very narrow
				Role:
				You ought to have permissions to assume the role 
				Once you assume a role, you get a new set of permissions and relinquish your existing permissions
				Once you leave the role, you assume all your older permissions 
			CloudTrail:
				Global service
				I can enable CloudTrail in a central service AWS account to collect logs from other AWS accounts
			EC2 Auto Scaling:
				Launches & terminates instances
				It DOES NOT start or stop instances
				Targeted to launch instances within each subnet
				Scoped at REGION level exactly like your ELB
			DynamoDB:
				Does Adaptive capacity i.e. delta allocation of WCUs, RCUs for the hot shard 
				CloudWatch can be used to detect a hot partition
			KMS, CloudHSM, ELB, ElasticBeanstalk = region scoped
			CloudHSM = cluster of upto 32 nodes
			KMS flexibility:
				Master Keys can come from CloudHSM, while Data Keys can come from KMS
			SNS while calling HTTP/HTTPS awaits a response and so it can retry if needed. SNS for HTTP/HTTPS is the only subscription type where retries are available
			Serverless:
				Billing on 10ms	
			Sequencing of components:
				Route53 -> AWS Shield -> AWS WAF -> ALB
				Route53 -> AWS Shield -> AWS WAF -> API Gateway
				Route53 -> AWS Shield -> AWS WAF -> CloudFront -> API Gateway
			AWS Lambda function that pulls list of CloudFront IP addresses and update SG to reflect the IP address
			Serverless:
				Compute: Lambda, Fargate
				Storage
				Network
			Global Accelerator:
				The two public IP addresses that the browser gets from DNS request points to the Global Accelerator service
				This service then forwards the request to one of the many ELBs or ALBs
			Each VGW is associated with each VPC
				As you associate a CGW with a VGW, a pair of IP addressess and private VIF is spun up
				When the second CGW is connected with a VGW, another pair of IP addresses and private VIF is spun up
			Trandit Gateway:
				Has two route tables - one for Intra, One for external
			AWS Pricing and Cost Management:
				AWS Pricing Calculator, AWS Compute Optimizer, AWS Trusted Advisor, CopperEgg, NewRelic
			Controlling AWS managed resources provisioning:
				Proactive:
					IAM with tags, SCP
				Reactive:
					AWS Config
			DNS Hybrid:
				DC from on-premise replicating the forest to DC from AWS
			DR:
				AMI
					AMIs are scoped at REGION level
					AMIs cannot be copied from one region to another via GUI
					AMIs can be copied from one region to another via CLI
					When you copy, id of the AMI changes. Therefore, the CloudFromation needs to be updated 
				Route 53:
					It is charged based on the number of DNS requests. So configure it in the DR and keep it ready (even before the disaster)
				VPCs:
					It is free. So configure it in the DR site and keep it ready
			DR site types:
				Backup & restore -> Cold
				Pilot Light -> Warm
				Low-capacity Standby -> Warmer
				Multi-site active-active -> Hot
				
			One tebibyte is equal to 2^40 = 	1,099,511,627,776
			One terabyte is equal to 10^12 = 	1,000,000,000,000
			m5(/d/g).large
				m is family
				5 is the generation
				d=amd
				g=gravitron CPU
				large=number of vCPUs
				nano/micro/small=1vCPU, medium/large=2vCPU, xlarge=4vCPUs
				Also, keep in mind the network bandwidth that instance comes with		

	===================================================
	Global Infra: (Feb 2021)
	===================================================
		24 Regions
		76 AZs (3 to 6 AZs per Region)
		N AWS Facilities (Facility is a data center, 3 facility within an AZ)
		3 upcoming Regions 
		9 upcoming AZs
		245 countries/territories
		216 points of presence
		AWS Local Zones:
			Extension of AWS region in geo proximity of users
			Can support outbound connectivity to internet and Direct Connect
			AWS services like compute, storage, VPCs are available in a local zone

	===================================================
	IAM:
	===================================================
			- IAM is a GLOBAL service
				- Available in all Regions
				- AWS Service is a set of Resources AND each Resource has a list of Actions associated to it
					like EC2 Service contains a resource called SecurityGroups which has a list of actions (like Describe, List. Create, Delete, etc)
				- Updates in IAM are eventually consistent across REGIONS in your account
				- IAM enables shared access to accounts / Supports granular permissions / supports Identity Federation and access via STS (i.e. external IdP) 
				- IAM enables info logs for audits / PCI compliant
				- IAM is natively integrated with all AWS services
				- Root user (i.e. the creator of the AWS account) must be protected by MFA
				- Users from different AWS account can be given access to resources in your AWS account via IAM 
			** IAM = Principal + Authentication + Request for Action + Authorization + Actions allowed (API, CLI, GUI) + Resources
			- IDENTITY or PRINCIPAL is 
				- IAM User (Native AWS IAM User)
				- Federated User (user from an external Identity Provider which is trusted by AWS IAM)
				- Application
				- AWS Service (like EC2, ECS, etc) that wants to access a AWS Resource
				- IAM Role (Like an impersonation account that the above principal would assume. Role is not same as Managed Permissions)
			- RESOURCES
				- AWS Service (like S3, IAM, VPC, EC2, etc)
			- POLICIES in the AWS IAM console is the AUTHORIZATION part i.e. the Resource policies that can be granted to an authenticated principal
			- Password policy is at the AWS Account level and is applicable to all IAM users within the AWS Account
			- Billing & Cost Management is at the AWS Account level
			- User can be managed as an :
				1. AWS IAM user 
				2. Federated users from a trusted source: Cognito, MSFT AD, Amazon, Facebook, Google, OpenID compatible source
				- User, Application, AWS Service can assume ROLEs. Role is nothing but a set of permissions
				- Managed policies can only be attached to IAM users, groups, or roles. You cannot use them as resource-based policies
				- IAM users used to represent applications are called Service Accounts
			- Long term Token v/s Short term Token
				- User, when authenticated, gets a long term token
				- User, when assuming a role, gets a short term token
			- Amazon Resource Name:
					arn:aws:iam:account-id:user/rnair
			- Group:
				Collection of IAM users
				Groups cannot be nested
				Assign ROLE, permissions or policies to a group
				Limit of 300 groups per account
				A user can be member of 10 groups
			- Permission:
				- Permissions can be granted to User, Group, Role
			** Permissions Assignment:
					Two flavors:
						** Long term assignment of permissions
						** Short term assignment of permissions
					Long term assignment:
						** Permissions can be packaged in a User-based Policy. This Policy is then assigned to a principal like User, AWS Service
						** Permissions for a resource can be assigned to principals (like IAM User) via Resource-based Policy
						** Permissions for a resource can be delegated to AWS accounts (for further assignment) via Resource-based Policy
					Short term assignment
						** Permissions can be temporarily assumed via a Role
			- Role:
				- Packaging of Permissions
				- Role does not have a user-id, password or any credentials associated to it
				- Roles can be:
					** IAM Role: controls what the native IAM user (same account and cross account) or a Federated User can access
					** Service Role: controls what the AWS Service can access (like EC2 app accessing DynamoDB table)
				- Assuming a Role:
					- "Assuming a Role" results in the principal taking on the associated permissions for a short time using a Short Term Token
					NOTE: 
					- EC2 instances can assume only one SERVICE role. 
							- EC2 SERVICE role can be assigned during Launch or RUNNING state
							- For applications in the EC2 instance to assume the role, Instance profile MUST be created. While using console, the Instance profile creation and assignment is transparent to the user. While using CLI or API, Instance profile becomes visible, must be created explicitly  and assigned the appropriate role
							- All apps on an EC2 instance share the same SERVICE role
							- Apps can access the short term token for the Service Role from http://169.254.169.254/latest/meta-data/. There is no explicit requesting for token needed
					- Service role for ECS and other services, the short term token associated to Service Role needs to be explicitly requested and is not automatically available at http://169.254.169.254/latest/meta-data
			- Cross-Account Access:
				** How can IAM user in one account gain CROSS-ACCOUNT permissions?
					Simple answer using is to establish a TRUST RELATION USING Role Delegation
						[POLICY]Resource-based policy in the TRUSTING account
								The TRUSTING ACCOUNT grants permissions to execute actions to TRUSTED ENTITY. 
								The TRUSTED ENTITY is the ACCOUNT and is mentioned in the Principal element of the policy.
								This TRUSTED account can delegate the role to it's users
							+
						[POLICY]Identity-based policy in the TRUSTED account 
								The TRUSTED ACCOUNT must grant permissions to the user to assume the role
								The TRUSTED ACCOUNT is mentioned in the Principal element of the Resource-based policy in the TRUSTING account
				** Assuming ROLE cross-account:
					1. Interactively via AWS console 
							(during which the user temporarily looses his TRUSTED account permissions and assumes the role in the TRUSTING account)
					2. Using Resource-based policies 
							(in which case the user does not loose his account permissions; therefore the user has permissions from his native TRUSTED account plus permissions from the TRUSTING account)

			MFA:
					Used with AWS Mgmt console user
					Used with AWS API user
			User Credentials:
				Types:
					- User name + Password: example console access
					- User name + Password + MFA key: example console access
					- Access key - 2 components (generally used for CLI or programs / MAX two access keyes)
						* Access Key
						* Secret Key
					- Access key + MFA Key
					- X.509 certificate (say, for a 3rd party contractor to authenticate to a LB)
					- SSH key (used to access CodeCommit)
					- API key (used to access API)	
				Temporary Security Credentials
					Authentication Flow:
							Let's consider a federated Google user and Cognito providing the federation service
							Google user logs into Google and gets a Google specific ID token
							The Google specific ID token is passed onto Cognito and because of the trust relation, Cognito returns a Cognito Token
							The user takes the Cognito Token and shares it with the global STS service located at https://sts.amazonaws.com (or regional STS service)
							A JWT STS token is generated by the STS service and returned to the user
							STS Token has few components in the json format:
								- Access key id
								- Secret access key
								- Session (security) token
								- Expiration or Duration of validity
								- User or application info
					STS Token:
							Token embodies the permissions granted to the user
							Functionality similar to long term credentials (like the access key), except that it is issued for a short duration
							A STS token CANNOT be revoked prior to its expiration
							The token lifetime ranges from 15-minutes to 36 hours
							When (or even before) the temporary STS security credentials expire, the user can request new token, as long as the user requesting them still has permissions
							Can be generated via API calls, SDK calls, CLI, Tools for MSFT Powershell
							Cannot be generated via AWS Console
					STS API Actions
							GetSessionToken:
								- who can call: an IAM user or AWS Account Root User who can pass MFA tokens
								- MFA token authentication is supported by AWS STS service
								- this is the typical scenatio for AWS Console access
								- Token expiration default is 12 hours (long life tokens)
							GetFederationToken:
								- who can call: a federated user 
								- this is the typical scenatio for AWS Console access
								- Token expiration default is 12 hours (long life)
								- Ideally, MFA authentication happens at the external identity provider
							AssumeRole:
								- who can call: an IAM User or A user who can pass an STS Token / MFA tokens can also be passed in the call
								- MFA token authentication is supported by AWS STS service							
								- Token expiration default is 1 hour
							AssumeRoleWithSAML:
								- who can call: A user who can pass a SAML Assertions that can be validated against a trusted identity provider
								- Token expiration default is 1 hour
								- Ideally, MFA authentication happens at the external identity provider
								- Special case: To login to the AWS console, the SAML assertion can be posted at https://signin.aws.amazon.com/saml. 
												The URL essentially triggers the AssumeRoleWithSAML call to STS service to obtain the STS token
							AssumeRoleWithWebIdentity:
								- who can call: A user who can pass a Web Identity Token that can be validated against a trusted identity provider
								- Token expiration default is 1 hour
								- Ideally, MFA authentication happens at the external identity provider
								- Great for mobile applications

					STS Service:
							STS supports MFA devices (except U2F security keys)
							STS tokens can be generated by regional STS service (example: https://sts.us-west-1.amazonaws.com)
							STS tokens generated by a regional STS service work across all regions (exception is AWS GovCloud - US and AWS China Beijing)
							IAM users can request temporary STS security credentials for their own use by calling the AWS STS GetSessionToken API
					Used for:
						Cross-account access: Users from other AWS account gain temnporary access
						EC2 applications accessing a resource
						Enterprise Identity Federation (Custom broker, SAML2.0 based)
						Web identity federation (Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider)
					If you're making direct HTTPS API requests to AWS, you can sign those requests with the temporary security credentials that you get from AWS Security Token Service (AWS STS). 
						To do this, do the following:
							-Use the access key ID and secret access key that are provided with the temporary security credentials the same way you would use long-term credentials to sign a request. 
								For more information about signing HTTPS API requests, see Signing AWS API Requests in the AWS General Reference.
							-Use the session token that is provided with the temporary security credentials. Include the session token in the "x-amz-security-token" header. 
							-For Amazon S3, via the "x-amz-security-token" HTTP header 
			Identity Providers for Authentication:
				- Identity provider providing SAML assertions
				- AWS Directory <-> On-premise directory trust relation
				- Non-SAML Identity Broker based provider (when SAML 2.0 assertion is not supported by Identity Provider)
				- Amazon Identities
				- Facebook Identities 
				- Google Identities 
				- OpenID Connect Identity Provider
				** AWS recommends using Cognito to establish trust with external identity providers like Amazon, Facebook, Google, OpenID Connect
			Authorization Mechanism:
				Before a request is allowed by IAM:
					- IAM authenticates the Principal
					- IAM gathers request context (IP address, user agent, SSL enabled status, time of the day, geo info, etc)
					- IAM aggregate all relevant permissions
						- IAM Policy
						- Resource-based Policy
					- IAM evaluates permissions 
						- Explicit DENY has the highest priority
						- If there are no EXPLICIT DENY, then check for EXPLICIT ALLOW 
						- If no explicit allow, then fallback on default deny
				Policy:
					Components:
						SID
						EFFECT (mandatory)
						PRINCIPAL / NOTPRINCIPAL
						ACTION / NOTACTION
						RESOURCE / NOTRESOURCE
						CONDITION
					Policy Association:
						* Identity-based Policies: Provide permissions to a Principal within the same account. CANNOT provide CROSS-ACCOUNT permissions
						* Resource-based Policies: For CROSS-ACCOUNT access, the permissions for the resource must come from that account 
													The account granting permissions is the TRUSTING account
													The account granted permissions is the TRUSTED account
													CROSS-ACCOUNT is accomplished by Resource based policies only 
													CROSS-ACCOUNT cannot be accomplished by Identity-based policies, because the TRUSTED identity is not registered in the account
						Permissions boundaries: 
												Defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. 
												Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity.
						Organizations SCPs:
												Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). 
												SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.
						ACLs:
												ACLs are cross-account permissions policies that grant permissions to the specified principal. 
												ACLs cannot grant permissions to entities within the same account.
												XML representation - the rest are JSON based
						Session Policies:
												Session policies limit permissions for a created CLI or API session, but do not grant permissions
												Session policy is passed as parameter to AssumeRole, AssumeRoleWithSAML, or AssumeRoleWithWebIdentity
												If there is an ALLOW in Session policies that is explicitly DENIED in Identity-based policies, then it is a DENY
												Effective Session Permissions = Identity-based policy + Session Policy
					Management of Policies:
						- AWS Managed Policies
						- Custom User Policies
						- Inline Policy (attached to a specific Principal)

	===================================================
	DIRECTORY SERVICES:
	===================================================
		- Approaches:
			0. Create a Read-Only Domain Controller in AWS EC2 which is joined to the On-premise AD setup / Reduces Latency
			1. AD Service for MSFT AD (AWS Microsoft AD Service)
			2. Simple AD
			3. AD Connector
			4. AWS Cloud Directory
			5. AWS Cognito
		AD SERVICE FOR MSFT AD (AWS Microsoft AD Service):
				- MSFT AD Service hosted in AWS 
				- Full fledged service providing all features and functionality of a typical on-premise AD
				- Fully managed service: AWS takes care of patching, security fix, upgrade, replication, automated backups, replace failed controllers, monitoring
				- Comes in two editions:
					- Standard 	: 5000 users, 30,000 objects
					- Enterprise: >5000users, 500,000 objects
				- Supports LDAP(S) protocol
				- Supports RADIUS MFA infrastructure
				- Users can take snapshot for backup and restoration
				- Supports TRUST relationship with On-premise AD
					- Need to setup Direct Connect or VPN
					- TRUST relation essentially replicates in-scope objects between the two directories
					- Enables SSO i.e. a user from Cloud AD can seamlessly authenticate against an application relying on On-premise AD AND vice-versa
				- Supports Schema extensions to support your applications
				- Supports 
						logging into AWS management console (without needing to setup SAML assertions)
						Sharepoint
						SQL Server Always On Availability Group
						AWS Workspaces
						AWS WorkDocs
						AWS QuickSight
						AWS Chime
						AWS Connect 
						RDS for SQL Server
						Linux hosted applications (since AWS MSFT AD supports LDAP(S))
				- Default Configuration: Two DCs deployed in HA configuration across two AZs in same region
		SIMPLE AD:
				- MSFT AD Service hosted in AWS (Feature set is trimmed down)
				- Under the hood: SAMBA 4, AD Compatible Server with limited features
					** DOES NOT Support
						MFA, Schema Extensions, Powershell for AD, FSMO Role Transfer, LDAP(S), RDS for SQL Server, 
						TRUST relationship, AD Admin Center, AD Recycle Bin, Group-based managed service accounts
				- Comes in two editions:
					- Small	: 500 users, 2,000 objects
					- Large	: 5000 users, 20,000 objects
				- Can take snapshot for backup and restoration
				- Supports Kerberos-based SSO
				- Supports 
						logging into AWS management console (without needing to setup SAML assertions)
						AWS Workspaces
						AWS WorkDocs
						AWS QuickSight
						AWS WorkMail
						Linux hosted applications (since AWS MSFT AD supports LDAP)
				- Default Configuration: Two DCs deployed two AZs in same region
		AD CONNECTOR:
				- No directory is actually hosted in AWS
				- On-premise AD is the only directory
				- Connects AWS services to on-premise AD, byt acting as a proxy service to the on-premise AD
				- Need to setup Direct Connect or VPN
				- Supports RADIUS MFA infrastructure
				** DOES NOT Support
						RDS for SQL Server
				- It DOES NOT CACHE any information in the cloud
				- Comes in two editions:
					- Small	: 500 users 
					- Large	: 5000 users 
				- Supports 
						logging into AWS management console (without needing to setup SAML assertions)
						AWS Workspaces
						AWS WorkDocs
						AWS Chime
						AWS Connect 
						AWS QuickSight
						AWS WorkMail
						Domain join for EC2 for Windows
		AWS CLOUD DIRECTORY:
		COGNITO:
			User pool: 
				User sit in the Cognito database
				Users may sit in Facebook, Amazon, Google, etc
			Identity pools:
		Service Control Policies:
			Set organization controls 
			They override local IAM permissions including permissions of Root account

						
	===================================================
	Organizations:
	===================================================
		Organizations is a global service that helps in centralization of Budgetary, Security and Compliance needs
		Organization is eventually consistent across the regions
		Structure = Tree of OUs with a Root at the apex
		An OU can have only one parent
		An OU can have mutiple children
		Root is a special OU placed at the very apex of the OU tree
		Accounts are placed within an OU (including the Root)
		A member account can be in only one OU
		An account can be in only 1 organization
		Account attached to Root = Master Account 
		Account attached to OU = Member Account
		Master Account is the one that creates the Root
		Master Account is the payer account; pays usage bills of all member accounts
		Master Account invites accounts to become member of the organization
		Master Account joins and removes other accounts to the organization
		Master Account places member accounts under an OU
		OU tree = 5-level deep
		An Organization policy applied to the Root flows to all OUs and accounts within the OU
		An Organization policy applied to an OU flows to all sub-OUs and accounts under the OU hierarchy
		An Organization policy applied to the account applies to that account only
		Types of Organization Policies:
			- Service Control Policies: Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines
			- Tag Policies: You can use tag policies to define tag keys (including how they should be capitalized) and their allowed values. You can also specify that the tagging rules in the tag policy be enforced on certain resource types.
			- AI Services Opt-out Policies: Artificial Intelligence (AI) services opt-out policies enable you to control whether AWS AI services can store and use your content.
			- Backup Policies:Backup policies enable you to deploy organization-wide backup plans to help ensure compliance across your organization's accounts. 
		Service Control Policies at the organization/OU level and IAM Policies at the account level work in conjunction
			- User, Roles, Permissions, Policies (6 types) and Cross account access are managed at the account-level IAM Services
			- SCPs can restrict ability of the root IAM user of an account
			- SCPs DO NOT grant permissions. But it can RESTRICT usage of an IAM granted permission
			- SCPs specifies the maximum permissions that can be used by users and roles in your organization's accounts 
			- SCP 	allows whitelisting of actions,in which case rest of actions is blacklisted Or 
					allows blacklisting of actions, in which case rest of actions is whitelisted
			- SCP default at the root level is full whitelisting called 'FullAWSAccess'
			- Administrator in Master Account (and hence the Organization) has higher authority over administrators of the individual accounts in the AWS organization
			- Administrator in Master Account (and hence the Organization) determine the AWS Services and/or individual API actions that are available to the accounts in the OU hierarchy
			- Administrator in Master Account (and hence the Organization) can override IAM policies that administrators of the individual accounts may have provided
			- When Administrator in Master Account restricts access of a member account to a specific AWS Service or API action, this will override any permissions the administrator of the member account may have granted, via IAM policies, to its IAM User or Role
			- Administrator in Master Account (and hence the Organization) CAN prevent a member account from leaving the organization 
			- In short, Organization Permissions override Member Account Permissions
			- The SCPs do not effect the master account, regardless of whether the SCP was applied at the Root
		
		
	===================================================
	Billing & Cost Management:
	===================================================
		Two strategies for consolidation:
			- Multiple accounts, but Consolidated billing under an Organization
			- One account, multiple VPCs
		Consolidated Biling Model:
				Root of the ORGANIZATION is hinged at the Master account i.e. the account that created the Organization 
				And the administrator of the Master account can CREATE or INVITE multiple LINKED accounts into the organization
				AWS best practices recommends that the Master account be not used to run any services, except those required to run the org.
					Example: Master account will have S3 buckets where the consolidated AWS cost and usage reports can be saved
				The Master account is the PAYER account i.e. 
					- With CONSOLIDATED BILLING for an organization, resource usage, unit price-tier application, costs of all linked accounts are aggregated under the Master i.e. Payer account
					- The RI and Savings Plan sharing can be managed under the 'Billings Preferences' page of the Billing service of the account
					- If "RI and Savings Plan sharing" is ENABLED in a participating member account, then the reserved instance discounts from that account is pooled and applied to other participating member accounts [Need the other member account to also have sharing ENABLED]
					- If "RI and Savings Plan sharing" is DISABLED in a participating member account, then the application of reserved instance discounts is restricted to the owning member account
				The monthly bill is generated for individual AWS account and summary for each is included in the Invoice of the Master account 
				The AWS service limits are applied at the individual AWS account level
				The AWS accounts get Amazon support at the individual AWS account level
				Best Practices:
					-IAM user of MASTER account can CRUD budget and usage for the member account as long as CROSS ACCOUNT access has been granted
					-IAM user of member account can CRUD budget and usage for the Master account as long as CROSS ACCOUNT access has been granted
					-IAM user of one member account CANNOT CRUD budget and usage of another member account
		* Budget:
			Upto 20,000 Budgets can be configured per account
			First two budgets are free AND Each addn'l budget is charged at the rate of $0.02/day/budget
			Types of Budgets:
				- Cost budget: Monitor your costs against a specified amount and receive alerts when your user-defined thresholds are met 
					- Could also be expiring or recurring
					- Actual vs Forecasted
				- Usage budget: Monitor your usage of one or more specified usage types or usage type groups and receive alerts when your user-defined thresholds are met
					- Could also be expiring or recurring
					- Actual vs Forecasted
				- Reservation budget: Track the RI Utilization or RI Coverage associated with your reservations. These budgets support Amazon EC2, RDS, Redshift, ElastiCache and Elasticsearch reservation models
				- Savings Plan budget: Track the utilization and coverage associated with your Savings Plans
			Billing Alerts:
				- Alerts is via SNS notification or Email
				- Can be created on Actual usage against the budget or Forecasted usage against the budget or if Free Tier thresholds are crossed
				- Is triggered when the actual or forecasted usage exceeds a threshold Cost budget or Usage budget
				- Is triggered when the actual or forecasted usage falls below the RI budget or Savings Plan budget
		* Tags:
			- Either AWS generated or User-defined tags
			- Only Master account in an ORG. and standalone accounts have access to the Cost Allocation Tags Manager in the billing console
			- Tags cannot be deleted or merged - You deactivate them
			- AWS Resource Group is a grouping of AWS resources that share one or more tags
			- Upto 50 tags per resource (Hard limit)
			



	===================================================
	VPC:
	===================================================
		Isolated cloud in a region
			- A VPC can span multiple Availability Zones
			- A VPC is collection of one or more subnets
			- A subnet must reside in a single availability zone
			- Subnets within a VPC communicate via the IMPLIED ROUTER (You can configure the IMPLIED ROUTER on the AWS console)
			- By default, 5 VPCs in a region in an account
		VPC / subnet size contraints = IPv4 /28 to /16 || A subnet can cover the entire address range of a VPC i.e. a A VPC with only one subnet
			Default VPCs are assigned a CIDR range of 172.31.0.0/16
			You can delete a default VPC. Once deleted, you can create a new default VPC directly from the VPC Console or by using the CLI. This will create a new default VPC in the region
			One Default subnet per AZ in VPC
			Default subnets within a default VPC are assigned /20 netblocks within the VPC CIDR range
		Permitted CIDR Blocks fOr VPC:
			10.0.0.0/8		: RFC1918
			172.16.0.0/12	: RFC1918
			192.168.0.0/16	: RFC1918
			198.19.0.0/16	: 
			Publicly routable CIDR block (non-RFC 1918) OR CIDR block from the 100.64.0.0/10 range.
		5 Reserved IPs in every subnet	
			10.0.0.0: Network address.
			10.0.0.1: Reserved by AWS for the VPC router.
			10.0.0.2: Reserved by AWS. The IP address of the DNS server. For VPCs with multiple CIDR blocks, the IP address of the DNS server is located in the primary CIDR. We also reserve the base of each subnet range plus two for all CIDR blocks in the VPC.
			10.0.0.3: Reserved by AWS for future use.
			10.0.0.255: Network broadcast address. We do not support broadcast in a VPC, therefore we reserve this address
		For IPv6, the subnet size is fixed to be a /64. Only one IPv6 CIDR block can be allocated to a subnet.
		You can have one Primary CIDR block and multiple secondary CIDR blocks attached to VPC:
			- After you create a CIDR block, you can't resize it
			- If your depleted CIDR block is the primary CIDR block, you can’t disassociate it, but can associate a new secondary block
			- If your depleted CIDR block is a secondary CIDR block, you can associate a new secondary block
		DescribeInstances() will return all running Amazon EC2 instances. 
			- You can differentiate EC2-Classic instances from EC2-VPC instances by an entry in the subnet field. If there is a subnet ID listed, the instance is within a VPC. 
			- DescribeVolumes() will return all your EBS volumes.
		An Amazon EC2 instance can be booted from an Amazon EBS volume from within the same AZ and VPC
		Public IP addresses:
			Public IP addresses are Public IP v4 addresses
			Assigned to EC2 instance from a pool of public IP addresses (the pool is managed by AWS)
			The pool of public IP addresses is not allocated to a AWS account
			The pool of public IP addresses is shared by multiple AWS accounts
			A reboot or stop of EC2 instance might result in loss of the public IP v4 address
			A relaunch of stopped EC2 instance might result in allocation of a NEW public IP v4 address from the pool
		EIP addresses:
			Public IP v4 addresses that are allocated to your AWS account
			You can attach the EIP to one ENI or keep it detached
			EIP addr will only be reachable from the Internet (not over the VPN connection)
			Each EIP address must be associated with a unique private IP address on the instance
			EIP addresses should only be used on instances in subnets configured to route their traffic directly to the Internet gateway
			EIPs cannot be used on instances in subnets configured to use a NAT gateway or a NAT instance to access the Internet
			This is applicable only for IPv4. Amazon VPCs do not support EIPs for IPv6 at this time.
		Limit of 5 VPC Elastic IP addresses per AWS account per region
		Limit of 5 VPCs in a region = 5 (Soft limit) = 1 Primary + 4 Secondary
		Limit of 200 subnets (& route tables) per VPC
		Limit of 50 route entries per route tables
		Default SG: Allow inbound from that SG, Allow outbound to all IPs + ports
		Default NACL: Allow from all (100) & Deny from all (32767), Allow to all (100) & Deny to all (32767)
		Limit of 50 active peerings per VPC
		Limit of 25 pending peering requests per VPC
		Limit of One internet gateway per VPC
		Limit of Five virtual private gateways per AWS account per AWS Region
		Limit of Fifty customer gateways per AWS account per AWS Region
		Limit of Ten IPsec VPN Connections per virtual private gateway
		Instances in one region can communicate with each other using Inter-Region VPC Peering, public IP addresses, NAT gateway, NAT instances, VPN Connections or Direct Connect connections.
		Longest Prefix Match OR Longest Subnet Mask Match:
			- If a particular IP to which the traffic is to be routed matches multiple entries in the Route Table, then the entry with longest prefix (i.e. the longest subnet mask) is selected from the Route Table
		There are multiple options for your resources within a VPC to communicate with AWS services like Amazon S3. 
			- You can use VPC Endpoint for S3, which makes sure all traffic remains within Amazon's network and enables you to apply additional access policies to your Amazon S3 traffic. 
			- You can use an Internet gateway to enable Internet access from your VPC and instances in the VPC can communicate with Amazon S3. 
			- You can also make all traffic to Amazon S3 traverse the Direct Connect or VPN connection, egress from your datacenter, and then re-enter the public AWS network
		ENI:
			Scoped at a subnet level
			Always have a private IP address of the subnet
			Security Groups are applied at the ENI level
		Network ACLs:
			Scoped at a subnet level
			Security Groups are applied at the ENI level
			Implicit ALLOW due to default allow / You must assign explicit DENYs
		Internet Gateway:
			Internet Gateway is a region-wide service. Hence, is accessible from all private subnets within a VPC. Internet gateway for a VPC is a highly redundant
		NAT Gateway:
			NAT Gateway is a region-wide service. Hence, is accessible from all private subnets within a VPC. The best practice is to have a NAT Gateway per AZ, but you will pay for cross-AZ traffic
		Egress-only Internet gateway:
			This is for IPv6. You make your private subnets point to the Egress-only Internet Gateway in the public subnet. And, in the route table o public subnet, make the 00:00:00 traffic point to the Internet Gateway
		NAT Gateway V/S NAT Instance:
			- Gateway is managed AWS service / Redundancy within an AZ / Scales upto 45Gbps / Attach Elastic IP addr / Private IP is auto assigned / Security groups do not work with a Gateway / NACLs are useful / Cannot be used as BASTION HOST / Port forwarding is not supported
			- Instance is installed by the user from market place / No automatic redundancy / Throughput depends on instance selected / Attach Elastic IP or Public IP addr / Attach Private IP / Security groups and NACL work with it / Can be used as a BASTION HOST / Port forwarding is supported
		Bastion Host Redundancy:
			- AWS recommends an auto-scaling group of desired 2 and elastic IP address assigned to each instance
		Virtual Private Gateway:
			- Managed VPN gateway provided by AWS with built-in failover
			- You can have only one VGW attached to one VPC
			- VGW has two public IP addresses for High Availability
			- Traffic routed over the internet on an encrypted channel
			- Can have multiple public IP addresses associated to it for peering purposes
			- Supports dynamic route propagation via eBGP and has ASN (AWS and Customer assigned)
			- Virtual Private Gateway exposes two IP addresses just like the Transit Gateway
		BGP:
			- Main internet routing protocol
			- Connects different autonomous systems
			- Autonomous system is a collection of infrastructure
			- Each autonomous system is assigned an ASN
			- An autonomous system is PEERED with another autonomous system
			- BGP traffic between systems is carried on TCP port 179
			- Supports Interior-BGP (routing within an Autonomous system) and Exterior-BGP (rputing between different Autonomous Systems)
			- An Autonomous System can be configured to be transitive and thus allowing routing between autonomous systems that are not peered
				i.e. A is peered with B, A is peered with C, B is not peered with C
					If A is configured to be transitive, then B and C can route traffic
		Site-to-Site VPN Connection: 
			- You can establish VPN connection from your Private Subnet to HQ via:
			- IPSec tunnel from the Customer Gateway in the HQ to the Virtual Private Gateway (VGW) in the private AWS subnet
			- Customer Gateway MUST have an internet routable IP address
			- Quickest way and cheaper to establish secure connection over the internet
			- For redundancy, establish two IPSec tunnels between the Customer Gateway and the Virtual Private Gateway
			- You cannot use NAT Gateway to access the internet through a Site-to-Site VPN connection
			- In order for communication to happen, the two gateways must exchange route information
			- On the AWS side, you know about routes at the other end by: 
				** manually configuring the route table with specific IP prefixes OR 
				** by having the virtual private gateway dynamically update the route table based on the information received via BGP 
			- NOT ALL IP PREFIXES ARE EXCHANGED. ONLY THOSE PREFIXES THAT ARE CONFIGURED TO BE EXCHANGED ARE KNOWN AT THE OTHER END
		VPN Cloud Hub
			- This is essentially a Virtual Private Gateway that can allow multiple locations to make IPSec connections to it
			- Route exchange happens dynamically via eBGP
			- Even the branch locations can talk to each other as they know about the exchanged routes (and thefore, this could be a potential backup connection between the branches)
			- The gateway can host 10 upto IPSec tunnels (soft limit)
			- Looks the VGW is configured as a transitive autonomous system and hence allows for the traffic to go between the branches
		DIRECT CONNECT: DX Links
				- Private link
				- Number of Direct Connect dedicated connections per region per account is 10
				- Private or Public virtual interfaces per Direct Connect dedicated connection is 50
				- Routes exchanged per BGP session on a private virtual interface is 100
				- Routes exchanged per BGP session on a public virtual interface is 1000			
				- Traffic is not routed over public internet; INSTEAD routed over a PRIVATE DEDICATED connection between customer and AWS brokered via Partner in various Direct Connect locations. 
						- Time intensive and expensive. But the price drops as more traffic is pumped through.
						- The Direct Connect is not encrypted. Therefore, it is recommended to establish a IPSec tunnel on top of the Direct Connect
						- The connection from customer router to the partner router is a FIBER connection
						- The partner router is connected via a cross-over cable to the the AWS Direct Connect endpoint
				- The end-to-end connection from customer to AWS supports two important technologies:
					** 802.1q VLANs
					** BGP
					- 802.1q VLAN channel over the physical ethernet allows differentiation of the traffic for the Public VIFs from the Private VIFs
					- 802.1q can support upto 4096 channels
					- The two ends exchange route via BGP 
					- BGP is the ONLY ROUTING OPTION available in Direct Connect. No static routing is available. 
					- Therefore, the customer router must support BGP and 802.1q
				** VLAN 1: Provides access to Public Virtual Interface (Public VIF) i.e. AWS Services in any REGION
					AWS Services(Public VIF)<==>AWS Direct Connect Endpoint(AWS Cage)<==>Partner Router(Partner Cage)<==>Customer Router(HQ/Branch)
					AWS services in RFC 6598 address space
				** VLAN 2: Provides access to a VPC in the region where the Direct Connect Location falls
					AWS VGW (Private VIF)<==>AWS Direct Connect Endpoint(AWS Cage)<==>Partner Router(Partner Cage)<==>Customer Router(HQ/Branch)
				- Private VIF allows you to access only 1 VPC in 1 Region
					** Therefore, If you want to access multiple VPCs in a region, then you will need to configure multiple Private VIFs
					** This is where Direct Connect Gateway can come in
				- All access to the resources in the PRIVATE VIFs and PUBLIC VIFs remain on the AWS backbone and does not cross the internet
				- 10 Direct Connect per Region per Account
				- A Direct Connect can connect upto 50 virtual interfaces (Private & Public)
				- A Direct Connect can connect upto 1 transit virtual interface (i.e. Transit gateway)
				- Link Aggregation Group
					- Here we are NOT talking about VLANs; INSTEAD talking about the physical fiber/ethernet links between Customer and AWS-provided
					- If there are multiple existing links, then they can be aggregated in a group effectively adding up their bandwidths
					- All links in the group are ACTIVE
					- To aggregate, the links MUST have the same bandwidth
					- Max number of links in a group is 4
					- Max number of LAGS per region is 10
					- All links participating in a LAG terminate at the same AWS Direct Connect Endpoint router
					- A LAG carries an attribute that determines the minimum number of ACTIVE links for it to be operational / Default = 0 
				- DX Link - Inbound Policies applied at the AWS end:
					- Policies are validations or checks that are done on routes published from On-premise to AWS
					- Customer must OWN the On-premise public prefixes
					- Customer advertised public prefixes must be registered in appropriate regional internet registry
					- Traffic must go to Amazon public prefixes
					- Transitive connections are not supported i.e. On-premise cannot access cnn.com
					- Direct Connect checks that the source of the traffic is from the advertised public prefix
				- DX Link - Outbound Policies :
					- Policies are validations or checks that are done on routes published from AWS to On-premise
					- AWS advertises all public prefixes with the NO_EXPORT BGP community
					- AWS advertises all local-region service prefixes
					- AWS advertises all remote-region service prefixes
					- AWS advertises on-net prefixes for non-region points of presence like CloudFront and Route 53
					- AWS advertises prefixes with a minimum path length of 3
					- Attribute AS_PATH is used to determine the routing path AND AWS Direct Connect is the preferred path for traffic sourced from Amazon
				- DX Link - BGP Community Tags:
					- These tags are attached to routes while the ASs are exchanging route information
					- Tags can be attached by AWS to the routes it propagates
					- Tags can be attached by On-premise to the routes it propagates
					- Tags can help with:
						- prioritizing routes
						- controlling further propagation
				- DX Link - AWS ADVERTISED ROUTES: BGP Community Tags attached to the routes advertised by AWS for Private VIFs:
					- Local Preference BGP Community Tag: This enables on-premise to load balance the traffic between links (use same tag value) OR provides preference to one link over the other (use high preference7224:7300 over medium preference7224:7200 over low preference7224:7100). AWS, by default, 
					- NO_EXPORT tag stops customers from propagating the routes to the private VIFs
				- DX Link - AWS ADVERTISED ROUTES: BGP Community Tags attached to the routes advertised by AWS used in Public VIFs:
					- SCOPE BGP COMMUNITY: AWS can advertise routes with following tags to control further propagation:
						** 7224:8100—Routes that originate from the same AWS Region in which the AWS Direct Connect point of presence is associated.
						** 7224:8200—Routes that originate from the same continent with which the AWS Direct Connect point of presence is associated.
						** No tag—Global (all public AWS Regions)
							Example: If you have a public VIF in the us-east-1 region, AWS advertises the routes associated for public resources in us-east-1 region with a community tag of 7224:8100. For routes for public resources in North America, AWS advertises the routes with a community tag of 7224:8200. For all other prefixes, there is no tag.
					- NO_EXPORT tag stops customers from republishing the public VIF routes			
				- DX Link - CUSTOMER ADVERTISED ROUTES: BGP Community Tags attached to the routes advertised by Customer:
					- SCOPE BGP COMMUNITY: Customer can advertise routes with following tags to control further propagation:
						** 7224:9100—Restrict route propagation to Local AWS Region
						** 7224:9200—Restrict route propagation to All AWS Regions for a continent
						** 7224:9300—Unrestricted propagation Global (all public AWS Regions)
							EXAMPLE: If you have a public VIF in the us-east-1 region, you can limit the scope of the routes you advertise to us-east-1 region with the community tag of 7224:9100. If you tag your routes with the community tag of 7224:9200, your prefixes are advertised to all US regions (North America continent). If you tag your routes with the community tag of 7224:9300, or if you do not tag your prefixes with a community tag, your prefixes will be advertised to all AWS Regions.
				- Multiple Physical Links between On-premise and AWS:
					** 	The key questions are: which DX Link to use? Can traffic be routed on both links? Can one link be prioritized over other? What happens if the prioritized link fails?
					Ans: 	AWS->On-Premise Default Behavior for Route Tables and Route Priority: 
									1. Scenario 1 - DX link from CGW to a VPC AND S2S VPN from CGW to same VPC (i.e.VGW)
										- For traffic from AWS VPC -> On-premise network, a route must exist in the route table assigning the On-premise prefix to a VGW
										- When the traffic arrives at the VGW, it now has two links - one is the DX Link and the other is the S2S VPN
										- The VGW makes the Routing decision as follows:
											** BGP propagated routes on the DX link - MOST PREFERRED
											** Manually configured static routes to S2S VPN - SECOND PREFERRED
											** BGP propagated routes from S2S VPN connection - LEAST PREFERRED
									2. Scenario 2 - DX link from a CGW to a VPC (VGW) AND another DX link from another CGW to the same VGW
										- For traffic from AWS VPC -> On-premise network, a route must exist in the route table assigning the On-premise prefix to a VGW
										- When the traffic arrives at the VGW, it now has two links - one is the DX Link 1 and the other is the DX Link 2
										- The VGW makes the Routing decision as follows:
											** Load-sharing the traffic between the two links
							AWS->On-Premise Default Behavior for Traffic Forwarding 
									- This is not routing, instead it is traffic forwarding from AWS VPC to On-premise
									1. Scenario 1 - DX link from CGW to a VPC AND S2S VPN from CGW to same VPC (i.e.VGW)
										Forwarding decisions is made as follows:
											- Apply Longest Prefix Match
											- If all Prefixes are of equal length, then proceed to the next check 
											- If there are 'Local Route', they take priority. If not, proceed to the next check. 
											- If there are Static routes, they take priority
									2. Scenario 2 - DX link from a CGW to a VPC (VGW) AND another DX link from another CGW to the same VGW
							On-Premise->AWS Default Behavior for Route Tables and Route Priority:
									- There is no default behavior per-se
									- The On-premise routers uses STATIC routes that may have manually configured as the top priority
									- If no static routes, then the On-premise routers uses the 'Local Preference BGP Community' tag attached to the routes advertised by AWS. And AWS by default prefers the DX link over S2S VPN
		Direct Connect Gateway
			- In a Direct Connect setup (above), a Private VIF allows a customer to access one VPC within a region; usually, the region corresponding to the Direct Connect Partner Location. If the customer wants to access another VPC in the same region of the Direct Connect Partner, then a new Private VIF is to be configured. If the customer wants to access a VPC in a completely different region, it is not possible with Direct Connect links
			- SCENARIO: If customer wants to access a VPC in region different from that of the Direct Connect Partner Location, then there is no easy way. 
				SOLUTION:
					- A new Direct Connection will have to be setup in the new region with a new Partner, over which the new Private VIFs will need to be configured. It is time-consuming and expensive to setup a new Direct Connect Link in each region
						OR
					- The customer can establish S2S VPN to the VPCs in the other region over internet
						OR
					- The customer can establish S2S VPN to the VPCs in the other region over Public VIF
						OR 
					- The customer can establish a Direct Connect Gateway which enables access to all VPC-Virtual Private Gateways in all AWS regions and the public VIF
			- Direct Connect Gateway exposes 
				** PER ON-PREMISE LOCATION, ONE PRIVATE VIF for all VPCs in all regions
				** ONE and ONLY ONE PUBLIC VIF
			- Direct Connect Gateway can connect: 
				** multiple Virtual Private Gateways 
				** multiple Transit Gateways and 
				** multiple On-premise data centers
			- A Direct Connect Gateway ALLOWS TRAFFIC FROM a GIVEN ON-PREMISE SITE TO THE VPCs in the various regions VIA ONE PRIVATE VIF
				- The same Direct Connect Gateway ALLOWS TRAFFIC FROM ANOTHER ON-PREMISE SITE TO THE VPCs in the various regions VIA ANOTHER PRIVATE VIF
			- Direct Connect Gateway is NOT TRANSITIVE
				- The VPCs cannot talk to each other
				- The virtual interfaces in On-premise cannot talk to each other
				- The VPC cannot talk to a VPN connection of another VPC
				- A VPC-VGW can be connected to only one Direct Connect Gateway
				- The VPCs connected to a Direct Connect Gateway MUST have non-overlapping CIDR blocks
			- Direct Connect Gateway is a GLOBAL RESOURCE i.e. accessible from all regions
			- The connected VPC-VGWs and Transit Gateways can belong to account(s) different from the account that created the Direct Connect gtwy
			- 200 Direct Connect Gateways per Account
			- A Direct Connect Gateway can connect upto 10 Virtual Private Gateways (i.e. 10 VPCs)
			- A Direct Connect Gateway can connect upto 30 Virtual Interfaces (i.e. 30 corporate data centers)
			- A Direct Connect Gateway can connect upto 3 Transit Gateways
		VPC traffic mirroring
			Supports network packet captures at the Elastic Network Interface (ENI) level for EC2 instances on Nitro infra.
			SENDS the captured traffic to another EC2 instance or to an NLB with a UDP listener.
			The EC2 instance or the EC2 fleet behind the NLBs hosts security & monitoring tools which do out-of-band security, monitoring appliances for threats, content inspection, 
		VPC flow logs:
			Created at the VPC level, Subnet level or ENI level
			Logs are at Layer 3 
			If created at the VPC level, it applies to all ENIs within all subnets within the VPC
			If created at the subnet level, it applies to all ENIs within the subnet
			FLOW LOG RECORD = 
				What types to capture: Allowed OR Denied or ALL traffic
				What to capture: Source and destination IP addresses, ports, protocol number, packet and byte counts, and an action (ACCEPT or REJECT)
			You can use this feature to troubleshoot connectivity and security issues and to make sure that the network access rules are working as expected
			Logs can be published to CloudWatch logs or S3
		VPC DHCP Options:
			- DHCP passes configuration to hosts in a VPC. Configuration includes Domain name, Domain name server, NetBIOS-Node-Type
			- DHCP Option Set gets attached to a VPC during VPC creation
			- An attached DHCP set cannot be modified. Instead, create a new set and replace it in the VPC
			- When a new DHCP set is attached to VPC, all net new instances pick the new config and the existing instances pick up the new configs within a few hours (depending on how frequently the instances renew their DHCP lease)
			- Route 53 provides DNS to the AWS environment
			- On-premise DNS infra can also be used to provide DNS names to the AWS environment
			- Route 53 cannot provide DNS to the On-premise environment
		VPC Wizard:
			- VPC with a Single Public Subnet
			- VPC with Public and Private Subnets
				A /16 network with two /24 subnets
				Public /24 subnet associated with a custom route table, IGW, NAT Gateway  
				Private /24 subnet associated with main route table; uses the NAT Gateway to access Internet
			- VPC with Private Subnet Only and Hardware VPN access
			- VPC with Public and Private Subnets with Hardware VPN access 
				A /16 network with two /24 subnets
				Public /24 subnet associated with a custom route table, IGW
				No NAT Gateway or NAT Instance is created
				Private /24 subnet associated with main route table, uses VGW to establish a VPN connection
		VPC Peering:
			- Peering resource is identified by short name PCX
			- Do not forget to configure the route tables, NACLs, SGs
			- And do not forget the general cost calculation rules
			- Peering is 1:1 relation between the peered VPCs in same account or different accounts, in same region or different regions, intra-AWS account and inter-AWS accounts
			- Peering is not transitive i.e. edge-to-edge routing is not supported
			- For n VPCs to be peered, the number of VPC peerings to be configured is n*(n-1)/2
		Transit Gateway:
			- For n VPCs to be peered, the number of VPC peerings to be configured is n*(n-1)/2
				- For a company with 40 VPCs needing to be peered, the number of VPC peerings full mesh to be configured is 780 = 40*(40-1)/2
			- Transit Gateways solves the very problem of having to manage this full mesh
			- Transit Gateway is a REGION-level resource
			- Transit Gateway exposes two IP addresses for peering
			- Transit Gateway can have various ATTACHMENTS:
				An ATTACHMENT could be 
				- a VPC or
				- a S2S VPN Connection or
				- a Direct Connect Gateway or
				- Another Transit Gateway
			Routing Infrastructure:
				- When a VPC is attached to a transit gtwy, you specify ONE subnet from each AZ to be used by the transit gateway to route traffic
				- When a VPC is attached to a transit gtwy, you must add a route in each subnet route table for the appropriate traffic to route through the transit gateway
				- Transit gateway routes IPv4 and IPv6 packets between attachments using it's own Transit Gateway Route Table(s)
				- Your transit gateway automatically comes with a default route table. By default, this route table is the default association route table and the default propagation route table
				- Buth default Transit Gateway Route table can be abandoned AND For each attachment, the Transit Gateway can use a SEPARATE Route Table for ASSOCIATION i.e. ROUTING and ROUTE PROPAGATION
				- When a packet comes from one attachment, it is routed to another attachment using the route association in the Transit Gateway Route table that belongs to that attachment
				- You can create additional route tables for your transit gateway. This enables you to isolate subnets of attachments. Each attachment can be associated with one route table. An attachment can propagate their routes to one or more route tables
				- You can create a blackhole route in your transit gateway route table that drops traffic that matches the route
				- You can also add static routes to the transit gateway route tables
				- For transit gateway peering attachments, only static routes are supported 
			Route Propagation:
				- Each attachment comes with routes that can be installed to one or more transit gateway route tables
				- When an attachment is propagated to a transit gateway route table, these routes are installed in the transit gateway route table
				- For a VPC attachment, the CIDR blocks of the VPC are dynamically BGP propagated to the transit gateway route table
				- For a VPN connection attachment, routes in the transit gateway route table propagate to and from the transit gateway and your on-premises router using Border Gateway Protocol (BGP). The prefixes that are advertised over the BGP session are propagated to the transit gateway route table
				- For a Direct Connect attachment, routes are propagated dynamically via BGP
				- For Transit Gateway Peering attachment, routes have to be installed statically
				- Routes can also be statically installed as well
			Route Selection Logic:
				- Static routes have a higher precedence than propagated routes
				- Among propagated routes, VPC CIDRs have a higher precedence than Direct Connect gateways than Site-to-Site VPN
		Direct Connect:
			Private network connection from user data center to AWS VPC
			Even if your data center is not at a direct connect location, the partners can help extend your network to a direct connect location
			Direct connect supports Gigabit ethernet connections over singlemode fiber - 1000BASE-LX or 1000BASE-LR
			Direct connect ports are either 1Gbps or 10Gbps
			Each Direct Connect can be configured with virtual interfaces and a virtual interface can access
				- Resources in VPCs
				- 
			VLAN on the direct connect pipe keeps the two traffics separate (i.e. Private VPC separate from Public AWS)
			A max of 100 routes can be propagated over BGP / If > 100 routes are published, network traffic travelling over the interfaces is stopped
			Redundancy strategies:
				Have a second-backup connection between your router and Amazon router at the original location
				Have a second-backup connection between your router and Amazon router at a new direct connect location
			Link Aggregation Group
				Increase bandwidth to-from AWS by aggregating upto 4 links
				Each link in the group must be of same size (either 1Gbps or 10Gbps)
				LAGs support Jumbo frames
			Supports IPv4 single stack and IPv4+IPv6 dual stack
			Q. When creating a virtual interface to work with AWS services using public IP space, what IP prefixes will I receive via BGP?
				 Prefixes necessary to reach AWS services in all supported regions, and may include prefixes for other Amazon affiliates, including those of www.amazon.com
				 It also includes AWS non-Region points of presence (PoP) like CloudFront and Route53
		

	===================================================
	EFS:
	===================================================
		EFS:
			Shared across AZs, Regions, VPCs, ACCOUNT
			Network I/O within AZ is not charged
			Network I/O across AZs, across Regions is always charged
		Elastic scalable file-system grows to PetaByte scale
			NFS4 and 4.1 protocols accessible at Port 2049 of EFS
			Read-after write consistency
			POSIX compliant file system MOUNTABLE on Linux EC2 or on-premise Linux systems via Direct Connect (AWS VPN is not recommended as it goes over internet)
			Windows is not POSIX compliant, hence does not support EFS
			You pay only for what you use (unlike EBS where you pay for the storage you provision and not the utilized)
			Can be accessed by 1000s of EC2 instances in a region, while EBS volume can be mounted on a single EC2 instance within an AZ
		Use-cases:
			Content management and web serving
			Big data and Analytics
			Media processing workflows
			Department file systems
		EFS file systems is accessible at region level i.e. accessible from all AZs in a region and VPCs
			EFS file system is not accessible between VPCs in a given region i.e. ONLY 1 VPC at a time
			EFS can be mounted across VPC peering connections
			Mount targets are nothing but HIGHLY AVAILABLE ENIs made available in a subnet to be connected to EC2 instances
			AWS recommends creating one mount target for each AZ that the EFS is available in. All subnets in that AZ can access that ENI (aka mount target)
			Mount Target (aka ENI) and EC2s must have security group attached to them to allow traffic
			Private DNS and IP address assigned to mount targets are static and not dynamic
		Storage Classes:
			- Standard			Frequent Access
			- Infrequent Access (For example, EFS LifeCycle policy can automatically move files into EFS IA fourteen days of not being accessed)
								File meta-data is always stored in the Standard tier even if the actual content is stored in IA tier
			EFS Life Cycle Management:
				Move file between Standard and IA tiers
			Files smaller than 128 KiB are not eligible for Lifecycle Management and will always be stored on EFS Standard (Similar to S3)
		Performance Mode:
			General Purpose		Most file systems ans use-cases
			Max I/O				1000s of EC2 instances connect (high throughput and ops per seconds, slightly higher latency)
		Throughput Mode:
			Linear: 		throughtput scales as the file system grows
			Provisioned
		To access EFS file systems from on-premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC.
		AWS DataSync provides a fast and simple way to securely sync existing file systems into Amazon EFS, and works over any network, including AWS Direct Connect
			AWS DataSync can be used to transfer files between two EFS file systems, including ones in different AWS Regions or ones belonging to different AWS accounts
		Locking: applications can apply locks on the whole file OR byte range
		Data and Meta-data are encrypted at rest
		Backup:
			EFS->EFS backup / CloudFormation can be used
			AWS Backup / Creat-Migrate-Restore-Delete backups / Auditable / Monitorable / Retention Policies
		
	===================================================
	FSX:
	===================================================
		FSX:
			Shared across AZs 
		Two types: 
			FSX for Windows File Server 
			FSX for Lustre (Linux Cluster)
		Use-cases: similar to EFS
		FSX for Windows:
			Fully Managed Windows File Server requiring AD integration (AD can be self managed or AWS managed)
			SCALABLE from 32GiB to 64TiB
			Deployed in VPC and the corresponding AZs get ENIs attached to them in default subnet (similar to EFS)
			Mountable on Windows servers (using SMB, NTFS) / Exposed as an ENI in the subnet
			Mountable on Linux servers (using CIFS) / CIFS Utilities need to be installed / Exposed as an ENI in the subnet
			Uses SSD for Storage, High throuhgput, High IOPS
			Clients that support FSx for Windows: 
				Windows EC2, Linux EC2, AppStream, Workspaces, VMWare Cloud on AWS
			Multi-AZ FSx for Windows:
				Primary FSx file system
				Standby FSx file system
				SYNCHRONOUS replication from Primary FSx -> Standby FSx
				Only Windows client can failover / Failover can happen in 30 seconds / Failback happens to Primary when it comes up
				Linux clients do not failover automatically
			Backups (Automated, Manual) stored in S3 / Backups are incremental AND encrypted with the same key as the source key (AES 256)
			Data-At-Rest Encryption:
				Can use AWS managed keyes or Customer CMK
			Data in transit encryption:
				Supported by SMB 3.0 or higher client
				Requires SMB Kerberos session keys
		FSX for Lustre:
			Lustre = Linux Open Source HPC Cluster  POSIX compliant file system
			1.2TiB to 2.4TiB
			Horizontally Scalable 10s of Petabytes 
			100s of Gbps of Throughput / Millions of IOPS
			Lustre software is built as loadable kernel modules
			Mountable on Linux servers ONLY / Exposed as an ENI in the subnet
			Clients:
				Linux machines requiring 'FSx for Lustre' client software installed on them
			No replication service
			Backups (Automated, Manual) stored in S3 / Backups are incremental AND encrypted with the same key as the source key (AES 256)
			FSx for Lustre <-> S3 Integration
				- Lustre copies data from S3 into Lustre file system and
				- Lustre does the processing and
				- Lustre copies back into S3
			Data-At-Rest Encryption:
				Natively use AWS managed keyes; however currently with S3 integration only the S3-Standard methodology is supported
	 
	===================================================
	EC2:
	===================================================
				EC2 is a virtual machine using either HVM or Para-Virtualization
				EC2 is scoped at Availability Zone level
				99.99% availability in a Region / Service Level Credit provided if uptime falls below 99.95% SLA in a monthly billing cycle
				EC2 OS: Amazon Linux, Ubuntu, Windows, RHEL, SUSE Enterprise, openSUSE Leap, Fedora, Fedora CoreOS, Debian, CentOS, Gentoo Linux, Oracle Linux, and FreeBSD
				Soft Limits: 20 RIs/ On-Demand Family-based Regional vCPU Limit(like 1152 vCPUs for A,CD,H,I,M,R,T,Z family, 128 vCPU for F,G,P,X individually)  
				EC2 uses Error Correcting Code (ECC) RAM
				EC2 has one root volume (EBS-backed or Instance store) and multiple data volumes (EBS-backed or Instance store)
				IAM Key Pair:
						- Key Pair = Private Key + Public Key
						- Key Pair is used to identify a user
							- User keeps the private key to themselves
							- User stores the public key in EC2 instance
							- Linux EC2 saves the public key in ~/.ssh/authorized_keys
							- When you connect to your Linux instance using SSH, to log in you must specify the private key that corresponds to the public key content
						- Specification:
							- 2048-bit SSH-2 RSA keys
							- Up to 5,000 key pairs per Region
							- Verify your key pair's fingerprint:
									On your local machine, run the following to generate fingerprint:
										$ openssl pkcs8 -in path_to_private_key -inform PEM -outform DER -topk8 -nocrypt | openssl sha1 -c
									Compare the value generated by the above command with the fingerprint displayed in the AWS EC2 Console
						- To disallow a particular user or key:
							- Edit the ~/.ssh/authorized_keys and remove the public key for the user

				Amazon Machine Image: provides information required to launch an EC2 Instance
						AMI = 	Launch permission (i.e. RBAC for AWS accounts) 
										+
								Block Device Mapping specifies the volumes (EBS and Instance-store) to attach to the EC2 instance
										+ 
								EBS Snapshots of disks to attach
										+
								Template for the root volume of the Instance-stored backed EC2 Instance
						
						
				Block Device Mapping:
						- The mapping defines the block devices (instance store volumes and EBS volumes) to attach to an instance
						- Note that all NVMe instance store volumes supported by an instance type are automatically enumerated and assigned a device name on instance launch
						- Entries within the mapping:
								- Volume or device name used within EC2 instance (like /dev/sd01, /dev/sda, .......)
								- Instance store volumes: <ephemeral[0-23]> 
								- NVMe volumes are automatically enumerated
								- EBS volumes:
									- ID of the EBS snapshot used to create the block device (like snap-xxxxx)
									- Size of the volume in GiB
									- Delete the volume on instance termination = true / false 
									- Volume type = gp2, gp3, io1, io2, st1, sc1, standard 
									- Number of IOPS for io1, io2
						- Using Block Device Mapping:
								- You can specify a block device mapping as part of creating an AMI (Instance store volumes are not supported in AMI specs)
								- You can modify the block device mapping of the AMI when launching an instance from the AMI (like attaching additional volumes at launch time)
								- You can attach block devices launching an Instance
								
										
				ENI
					1 MAC address
					1 primary private IPv4 address
					1 or more secondary private IPv4 address
					1 auto-assigned public IPv4 address
					1 or more elastic public IPv4 attached to primary address
					Limit of 5 security groups
					Source - destination check flag
					NIC teaming is not available in EC2 instances
					Elastic IP: Soft Limit of 5 per region
				ENI is scoped at Availability Zone level
					- The total number of ENIs that can be attached to an EC2 instance depends on the instance type
					- In a multi-ENI situation, the max bandwidth supported by EC2 instance is divided amongst the ENIs. The bandwidth capability of individual ENIs are not summed up
					- ENI can only be attached to instances residing in the same Availability Zone
					- ENI CANNOT be attached to instances residing in different Availability Zone or different VPC
					- You can attach and detach secondary interfaces (eth1-ethn) on an EC2 instance, but you can’t detach the eth0 interface
						- You can attach the detached interfaces to another EC2 instances in the same AZ 
						- Primary IPv4, Secondary IPV4, IPv6, MAC addresses, and Elastic IP addresses continue to be with the same network interface when it is detached and re-attached to another instance
					- In addition to eth0, only one more eth1 can be COLD attached during launch. Remember eth1 COLD creation stops auto-assignment of public IPv4 to eth0 and therefore, elastic IP must be attached
					- You can HOT attach eth1, eth2, eth3 ... ethn when instance is running 
					- You can WARM attach eth1, eth2, eth3 ... ethn when instance is stopped
					- EFA ENIs can only be attached at launch time (COLD) or to stopped instances (WARM), but not when EC2 instances are running (HOT)
		
				EBS-OPTIMIZED EC2 Instances:
					- The EC2 instance minimizes contention between Amazon EBS I/O and other traffic from your instance
					- The EC2 instance delivers dedicated bandwidth to Amazon EBS
					- Designed to work with all EBS Volume Types (except standard-magentic) i.e. io2, io1, gp2, gp3, st1, sc1 
					- List of instances that are EBS-optimized
							https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html
					- How to optimize?
							~ Monitor two metrics EBSIOBalance% and EBSByteBalance% for the instance to be optimized
							~ Instances with a consistently low balance percentage are candidates to size up
							~ Instances where the balance percentage never drops below 100% are candidates for downsizing
							~ SAP HANA: The high memory instances are designed to run large in-memory databases, including production deployments of the SAP HANA in-memory database, in the cloud. To maximize EBS performance, use high memory instances with an even number of io1 or io2 volumes with identical provisioned performance
								
				EC2 Enhanced Networking (SR IOV or Single Root IO Virtualization):
					- Goal is to attain very high packets per second (PPS) throughout by bypassing the virtualization layers
					- Amazon has worked with the hypervisor firms to allow VMs direct access to the NIC cards
					- There is no additional fee for Enhanced Networking
					- Can be enabled on EBS-backed instances and Instance store-backed instances
					- Provides 10 to 25Gbps based on the instance
					- Requirement: 
						** Instance must support SRIOV using appropriate drivers AND
						** AMI should be HVM (Hardware Virtual Machine) virtualization AND not Para-virtualization
						** the instance must be in an Amazon VPC (and not classic VPCs)
					- Supported version:
						- C5, C5d, F1, G3, H1, I3, I3en, m4.16xlarge, M5, M5a, M5ad, M5d, P2, P3, R4, R5, R5a, R5ad, R5d, T3, T3a, X1, X1e, and z1d instances use the Elastic Network Adapter (which uses the “ena” Linux driver) for Enhanced Networking. C4, D2, I2, M4 (excluding m4.16xlarge), and R3 instances use Intel® 82599g Virtual Function Interface (which uses the “ixgbevf” Linux driver). 
						- Amazon Linux AMI includes both of these drivers by default
						- For AMIs that do not contain these drivers, you will need to download and install the appropriate drivers based on the instance types you plan to use. 
						- You can use Linux or Windows instructions to enable Enhanced Networking in AMIs that do not include the SR-IOV driver by default
				EC2 Placement Groups:
					- Cluster: Instances clustered in a single AZ, High throughput requirement, Most of traffic is between EC2 instances, Span Peered VPCs in same region, 10Gbps TCP/IP traffic between instances, Goes hand-in-hand with EC2 Enhanced Networking Instances (i.e. SRIOV)
					- Partition: Fault tolerance requirement, Span Multiple AZs, Span Multiple AZs in same region,  Max 7 partitions in one AZ,Other AZs can have similar, A partition can have many EC2 instances
					- Spread: Few critical EC2 instances requiring separate distinct hardware, Span Multiple AZs in same region, Max 7 instances in one AZ, Other AZs can have similar
				Elastic Fabric Adapter:
					EFA ENIs can only be attached at launch time or to stopped instances
					EFA ENIs provide all of the ENA ENI (traditional) functionalities plus a new OS bypass hardware interface that allows user-space applications to 
						communicate directly with the hardware-provided reliable transport functionality. 
					Most applications will use existing middleware, such as the Message Passing Interface (MPI), to interface with EFA. 
					AWS has worked with a number of middleware providers to ensure support for the OS bypass functionality of EFA. 
					Please note that communication using the OS bypass functionality is limited to instances within a single subnet of a Virtual Private Cloud (VPC)
				Instance meta-data: http://169.254.169.254/latest/meta-data/* (this URL is throttled)
				User data: 16KB limit/ Supplied during launch / To modify, stop the instance / http://169.254.169.254/latest/user-data/* (this URL is throttled)
				EC2 State change: 
					Launch an instance(s): Pending->Running (RunInstances is the corresponding API call)
					Stop an instance: Running->Stopping->Stopped
					Start an instance: Stopped->Pending->Running
					Stop-Hibernate an instance: 
					Terminate an instance: Running->Shutting down->Terminated (This takes into account Termination protection flag/ Termination Protection Flag can be overriden by OS shutdown)
					Stopped instances retain Instance ID, private IPv4/IPv6 address, public IPv4 address is released to the pool, Elastic IP address is retained
					Reboot is considered to be a Running state and is launched on the same host. Therefore, a reboot does not retrigger scripts in user data
					A Stopped instance, upon start may get launched on a new host
					Unexpected immediate termination reasons while trying to launch an EC2 instance:
						EBS volume account limit reached / Corruption of the snapshot on which EC2 is dependent / Instance-store missing a required part(an image.part.xx file) / No permission to access KMS keys
					You can detach/attach new EBS volumes (including the root volume)
				EC2 Service->CloudWatch: 
					EC2 service monitors EC2 instance, Basic 5-minutes default monitoring; Detailed 1-minute monitoring/paid; 15-months storage
					You can retrieve metrics data for any Amazon EC2 instance up to 2 weeks from the time you started to monitor it. After 2 weeks, metrics data for an Amazon EC2 instance will not be available if monitoring was disabled for that Amazon EC2 instance.
					Amazon CloudWatch stores metrics for terminated Amazon EC2 instances or deleted Elastic Load Balancers for 2 weeks
				EC2 Billing: Linux & Ubuntu On-Demand, Spot, Reserved: Per second billing / 60-seconds minimum / All regions - all AZs
				Accelerated Computing instance (3 categories) use hw accelerators, or co-processors, to perform some functions, such as floating-point number calc and graphics processing, more efficiently than is possible in software running on CPUs. 
					P instance family: GPU compute instances for general-purpose computing
					P instance family: FGPA programmable hardware compute instances for advanced scientific workloads
					G instance family: GPU graphics instances for graphics intensive applications
				VPC Peering:
					Supports IPv4 and IPv6
					Intra-region traffic is not encrypted, but is isolated / Inter-region traffic is encrypted [Authenticated Encryption with Associated Data; key mgmt by AWS]
					DNS in peered VPCs: 
						A query for a public hostname of an instance in a peered VPC in a different region will resolve to a public IP address. 
						Route 53 private DNS can be used to resolve to a private IP address with Inter-Region VPC Peering.
				VPC Endpoints:
					- Allows EC2 instances in a VPC to connect to AWS services over the AWS backbone. The EC2 traffic does not egress to the internet and hence does not require NAT devices, VPN connection, Direct Connection or IGW
					- Three types of VPC Endpoints:
						** Gateway Endpoint
						** Interface Endpoint (PrivateLink)
						** Endpoint Service
					- Endpoint Service:
						- Your own application in your VPC. Other AWS principals can create a connection from their VPC to your endpoint service
					- Gateway Endpoint:
						- Gateway endpoints support AWS S3 and DynamoDB endpoints only
						- The Gateway Endpoint becomes TARGET IN THE ROUTE TABLE in which IP addresses of S3 and DynamoDB will point to the VPC Endpoint id (vpce-id)
						- Endpoint policy can control who can access the service and their permissions. The Endpoint policy is essentially an IAM Resource Policy that is attached to an Endpoint. The default endpoint resource policy allows full access to the service. 
						- Endpoint once created can be accessed by VPCs in a given region. An endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies). It is a separate policy for controlling access from the endpoint to the specified service.
						- S3:
							- Your endpoint has a policy that controls the use of the endpoint to access Amazon S3 resources
							- The default policy allows access by any user or service within the VPC, using credentials from any AWS account, to any Amazon S3 resource; including Amazon S3 resources for an AWS account other than the account with which the VPC is associated
							- Endpoints currently do not support cross-Region requests—ensure that you create your endpoint in the same Region as your bucket
						- DynamoDB:
							- Your endpoint has a policy that controls the use of the endpoint to access DynamoDB resources
							- The default policy allows access by any user or service within the VPC, using credentials from any AWS account, to any DynamoDB resource
							- You cannot access Amazon DynamoDB Streams through a VPC endpoint
							- Endpoints currently do not support cross-region requests—ensure that you create your endpoint in the same Region as your DynamoDB tables
					- Interface Endpoint (PrivateLink):
						- Supported by AWS Services except S3 and DynamoDB
							- Can support other AWS accounts
							- AWS services like Kinesis, Redshift, etc
							- Services hosted by AWS partners or customers in their own VPC (i.e. 3rd party Endpoint services)
							- Certain AWS Marketplace Partners services
						- Interface endpoint essentially manifests as an "Endpoint ENI" that is an entry point for traffic destined to supported service
							- AWS services and AWS Marketplace services generally accept all endpoint requests from consumers automatically
							- For 3rd party partner services, the provider must configure the service to accept requests automatically or manually. 
							- Once available, the Interface endpoint is created at an AZ level. And an ENI is created in each selected subnet
							- Since the interface endpoint manifests as an ENI, it supports security groups
							- Since the interface endpoint manifests as an ENI, no changes to the ROUTE TABLE; the default local route accomodates the ENI
						- Private DNS assignment to an endpoint
							- If the endpoint service is for an AWS service, or a service available in the AWS Marketplace, there is a default DNS name.
							- If the endpoint service is hosted by a Service provider, they can specify a endpoint specific DNS name; although the service provider must prove that they own the private DNS name domain before consumers can use the private DNS name
							- If the EC2 instance accessing the Service is in a private subnet AND support Private DNS is not enabled. In that case, the default service host names will resolve to public IP addresses. And since the EC2 instance is in a Private subnet, it won't be able to access the service. Hence, for the service to be accessible from a private subnet, you must "Enable Private DNS" on the endpoint 
							- During Endpoint Creation Process, you can enable Private DNS for the endpoint to enable you to make requests to the service using its default DNS hostname
							- Example of Endpoint-specific Private DNS Hostname 
								example: vpce-123-ab.kinesis.us-east-1.vpce.amazonaws.com
							- Example of Default DNS Hostname
								example: kinesis.us-east-1.amazonaws.com
							- The service is accessible from the endpoint speicifc DNS hostname as well as the default DNS hostname (if available)
						- Traffic Flow:
							Services cannot initiate requests to resources in your VPC through the endpoint. An endpoint only returns responses to traffic that is initiated from resources in your VPC
						- Service Provider - Service Consumer scenario:
							Service Owner deploys an NLB on top of the EC2 instances hosting it's service. The Owner then registers the Network Load Balancer with the PrivateLink services, thus exposing an available Service. Service Consumer creates an Interface type VPC endpoints for the exposed Service. The Owner can specify that acceptance is required for connection requests; in which case the Interface Endpoint Connection request from the potential consumer is to manually REJECTED or ACCEPTED. After an interface endpoint is accepted, it becomes available. Once available, these service endpoints will appear as Elastic Network Interfaces (ENIs) with private IPs in consumer's subnets. Once these endpoints are created, any traffic destined to these IPs will get privately routed to the corresponding AWS services. The application in your on-premises can connect to the service endpoints in Amazon VPC over AWS Direct Connect. The service endpoints will automatically direct the traffic to AWS services powered by AWS PrivateLink.
				BYOIP
					Bring Your Own IP (BYOIP) enables customers to move upto 5 IP ranges of their existing publicly routable IPv4 or IPv6 address space to AWS
					You can use ARIN, RIPE, and APNIC registered prefixes and customers will continue to own the IP range 
					Customers will continue to have access to Amazon-supplied IPs and can choose to use BYOIP Elastic IPs, Amazon-supplied IPs, or both
					Once configured, your BYOIP prefix will show as an IP pool in your account. 
					You can create Elastic IPs (EIPs) from the IPv4 pool and use them like regular Elastic IPs (EIPs) with any AWS resource that supports EIPs (EC2, Nat Gatewat and NLB) 
					You can associate CIDRs from your IPv6 pool to your VPC. The IPv6 addresses brought over via BYOIP work exactly the same as Amazon-provided IPv6 addresses. For example, you can associate these IPv6 addresses to subnets, Elastic Network Interfaces (ENI) and EC2 instances within your VPC.

				EC2 Purchase Options:
					- On-Demand Instances: 
								~ EC2 Instances purchased at market price
								~ Pay by the second
					- Spot Instances: 
								~ Unused EC2 Instances launched when bid price is higher than market price
								~ Pay by the second
										** supported by auto-scaling groups, EMR (but NOT Redshift)
										** If the Spot price exceeds your maximum price for a given instance or if capacity is no longer available, your instance will automatically be terminated (or be stopped/hibernated, if you opt for this behavior on persistent request). 2-minutes grace period.
					- Reserved Instances: 
								~ 1 or 3 years commitment to an EC2 Instance Family & Region
								~ Two types 
									- Standard		: 
											- Standard can be sold in the RI Marketplace. 
											- Standard can be bought in the RI Marketplace.
											- Standard CANNOT be exchanged
											- Instance Family CANNOT be modified
											- Tenancy CANNOT be modified
											- Instance Size, Scope (=Region, Zonal), Network, Platform/OS can be modified
									- Convertible	: 
											- Convertible cannot be sold in the RI Marketplace. 
											- Convertible cannot be bought in the RI Marketplace.
											- Convertible can be exchanged. 
											- Convertible allows for exchange with NEW Instance Family, NEW Tenancy, Instance Size, Platform/OS, Scope (=Region, Zonal)
											- Exchange can happen as long as the exchange results in the creation of Reserved Instances with equal or greater value. 
											COnvertible enables you to merge 2 or more convertible instances, the term of the new instance is the greater of the original instances
								~ Two scopes:
									- You pay the same price irrespective of the Regional or Zonal scope
									- Regional Reservation 
											- Reserving an instance in a region
											- This reservation does not reserve capacity
											- The Reserved Instance discount applies to all Instance Sizes within an specific Instance Family in all AZs in the specified Region
											- Region reservation = 20 per region 
									- Zonal Reservation 
											- Reservaing an instance in a specific availability zone
											- This reservation does reserve capacity
											- The Reserved Instance discount applies to specified Instance Family, Instance Size in the specified Availability Zone only
											- Zonal reservation = 20 per AZ
								~ Billing:
									- Pay for the entire term irrespective of the actual usage
									- Example: 
											For example, if you purchase one m4.xlarge Reserved Instance and run four m4.xlarge instances concurrently for one hour, one instance is charged at one hour of Reserved Instance usage and the other three instances are charged at three hours of On-Demand usage. However, if you purchase one m4.xlarge Reserved Instance and run four m4.xlarge instances for 15 minutes (900 seconds) each within the same hour, the total running time for the instances is one hour, which results in one hour of Reserved Instance usage and 0 hours of On-Demand usage.
									- Payment Options:
											- Upfront
											- Partially Upfront 
											- Monthly
					
					- On-Demand Capacity reservation:
								~ By creating Capacity Reservations, always have access to EC2 capacity in a specific AZ
								~ No 1-year or 3-year commitment
								~ EC2 Instance(s) are reserved within an AZ
								~ EC2 Instance(s) are reserved within a Local Zone
								~ Billing starts as soon as the capacity is provisioned
								~ You are charged the equivalent On-Demand rate whether you run instances in the reserved capacity or not
								~ If you do not run instances that match the reservation, you still pay and this shows up as "UNUSED RESERVATION" on the bill. 
								~ When you run an instance that matches the attributes of a reservation, you just pay for the instance and nothing for the reservation 
								~ Discounts from Savings Plan or Regional Reserved Instances can be applied
								~ Discounts from Zonal Reserved instances cannot be applied
								~ Number of instances, instance type, tenancy, and platform/OS are specified to make reservation
					- Savings Plan: 
								~ 1 or 3 years of commitment to a consistent amount of usage, in USD per hour.
								~ This does not reserve capacity
								~ This can be applied against On-Demand, Reserved Instances and On-Demand Capacity Reservation
					- Scheduled Instances: 
								~ EC2 Instance Family + Size is scheduled at recurring time-slots for a period of 1 year
					- Dedicated Instances: 
								~ Pay, by the hour, for instances that run on single-tenant hardware
					- Dedicated Host: 
								~ Pay for a physical host that is fully dedicated to running your instances
								~ Use your existing per-socket, per-core, or per-VM software licenses, including Windows Server, Microsoft SQL Server, SUSE, and Linux Enterprise Server
				
				Tenancy attribute:
					- Can be defined at EC2 level or VPC level
					- Tenancy defined at EC2 level overrides tenancy defined at VPC level
					- Three values:
						~ default 	: instance launched into shared hardware
						~ dedicated : instance launched as dedicated instance into a single tenant hardware
						~ Host  	: instance launched on a particular host
					- Changing tenancy from one to the other:
						x default to dedicated is NOT acceptable
						x default to host is NOT acceptable 
						x dedicated to default is NOT acceptable
						x host to default is NOT acceptable
						Y dedicated to host and host to dedicated is acceptable
					
 				Fleet:
					- A configuration that launches multiple instance types across multiple purchasing options across multiple AZ within a region with a cap on the max $/hour
					- Two types:
							~ EC2 Fleet
							~ Spot Fleet 
					
				EC2 Fleet:
					- EC2 Fleet Purchasing options = 
									On-Demand only OR 
									
									Spot Instances only OR 
									
									On-Demand + Spot
					- The fleet tries to meet the target capacity until it reaches the maximum amount that you’re willing to pay
					- The On-Demand instances could be furnished by Reserved Instances or On-Demand Capacity Reservation
					- Additionally savings plan could be applied to On-demand instances
					- The Spot instances are provisioned only if your purchase price > Spot price
							- Spot instances are fulfilled from Spot capacity pools (or in short Spot pools)
							- A Spot capacity pool is a set of unused EC2 instances with the same instance type and Availability Zone
							- Allocation Strategy: The fleet selects the Spot capacity pool or pools that are used to fulfill the Spot request using allocation strategy:
									~ lowest-price			 : Default strategy. Spot Instances come from Spot capacity pool with lowest price
									~ diversified			 : Spot Instances is distributed across Spot capacity pool
									~ capacity-optimized	 : Spot instances come from Spot capacity pool with optimal capacity for # of instances
									~ InstancePoolsToUseCount: Used in combination with lowest-price. Instances are distributed across multiple Spot capacity pools that you specify
					- Example:
							Example launch configuration = C1.medium (25 On-Demand) + C2.large (10 On-Demand) + T1.Medium (50 Spot) + T2.small (100 Spot)
											AND 
									a total max $/hour = 500
					- Three request types:
							~ Instant  : Request to API is one-time synchronous. The response contains instance-ids that were launched and error for instance types that could not be 
							~ Request  : Request to API is one-time asynchronous. Fleet does not try to maintain capacity by replenishing interrupted Spot instances.
							~ Maintain : Request to API is one-time asynchronous. Fleet tries to maintain capacity by replenishing interrupted Spot instances.
				
				Spot Fleet:
					- Fleet with Purchasing options = Spot Instances + optional On-Demand
					- The fleet tries to meet the target capacity until it reaches the maximum amount that you’re willing to pay
					- Rest of the options are similar to EC2 Fleet
						like Spot capacity pools, Allocation strategy
						
						
				EC2 Instance Recovery:
					~ Different from host recovery that dedicated hosts can undergo
					~ If failures detected on underlying host, then Cloudwatch alarm for "metric=StatusCheckFailed:System and statistic=Average" is triggered 
						~ Failure examples are Loss of network connectivity, Loss of system power, Hardware failure, Software failure
					~ Recovered instances retain same Instance ID, Private IPs, Elastic IPs, EBS volume attachments, Instance metadata
					~ Since instance was migrated, all data in memory is lost
					~ Instances with attached instance-store storage cannot be recovered
					~ EC2 requirement:	
							- Uses one of the following instance types: A1, C3, C4, C5, C5a, C5n, C6g, C6gn, Inf1, M3, M4, M5, M5a, M5n, M5zn, M6g, P3, P4, R3, R4, R5, R5a, R5b, R5n, R6g, T2, T3, T3a, T4g, high memory (virtualized only), X1, X1e
							- Runs in a virtual private cloud (VPC)
							- Uses default or dedicated instance tenancy
					NOTE: Your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure.

	===================================================
	Dedicated Hosts and Dedicated Instances:
	===================================================

			Dedicated Host v/s Dedicated Instance:
				~ Per-host billing V/S Per-instance billing
				~ Provides visibility of the number of sockets and physical cores v/s No visibility
				~ Allows you to consistently deploy your instances to the same physical server over time v/s Not supported 
				~ Provides additional visibility and control over how instances are placed on a physical server v/s Not supported
				~ Automatic instance recovery is supported in both cases
				~ BYOL is supported v/s not supported
				
			Dedicated Hosts and Dedicated Instances - AWS Config Recording:
				~ AWS Config recognizes Dedicated Host and the instances on top of them as RELATIONSHIPS
				~ AWS Config records "Configuration Items" for the Dedicated Hosts and the instances on them
		
			Dedicated Host and Dedicated Instances - License Reporting:
				~ Leverage AWS Config recorder for license reporting
				~ Turn ON recording at 3 levels - AWS Config, Dedicated Host and Instance
				~ After AWS Config starts recording configuration changes to your hosts and instances, you can get the configuration history of any host that you have allocated or released and any instance that you have launched, stopped, or terminated. 
				For example, at any point in the configuration history of a Dedicated Host, you can look up how many instances are launched on that host, along with the number of sockets and cores on the host. For any of those instances, you can also look up the ID of its Amazon Machine Image (AMI). You can use this information to report on licensing for your own server-bound software that is licensed per-socket or per-core.

			[Dedicated Host] Host Recovery:
				~ If failure detected on a Dedicated Host, then instances automatically restarted on new replacement hosts
						~ Failure examples are Loss of network connectivity, Loss of system power, Hardware failure, Software failure
				~ Replacement host gets new Host ID, but inherits AZ, Instance Type, Tags, Auto placement settings
				~ Recovered instances retain same Instance ID, Private IPs, Elastic IPs, EBS volume attachments, Instance metadata
				~ When all of the instances have been recovered on to the replacement Dedicated Host, the impaired Dedicated Host is released
				~ Instance store-backed EC2 instances are not automatically recovered. You need to manually copy the data to the new instance
				~ Host recovery integrated with and updates AWS License Manager as part of Host Recovery 
				
			[Dedicated Instance] 
				~ Placed by EC2 Service on hosts dedicated to customer
				~ User does not get to place instances
				~ When you launch an Amazon EBS-backed Dedicated Instance, the EBS volume doesn't run on single-tenant hardware
				~ ASGs can be used with Dedicated Instances
				~ dedicated instances can be recovered 
				~ User can launch dedicated instances in a VPC with tenancy of default, dedicated but not host 
					EC2 tenancy overrides VPC tenancy, but host VPC tenancy cannot be over-written as the network config is different
			

				
	===================================================
	EBS:
	===================================================
		- EBS has 5 9s availability
		
		- EBS is scoped to an Availability Zone:
				- EBS is automatically replicated within the AZ to prevent data loss due to failure of any single hardware component
				- EBS attaches to only one EC2 instance in the AZ where it was created
						- EXCEPTION: 
								EBS multi-attach = io1, io2 volumes can attach to upto 16 Nitro instances in the same AZ. Each instance to which the volume is attached has full read and write permission to the shared volume
				- 99.999% guaranteed availability
				- Service Level Credit provided if uptime falls below 99.95% in a monthly billing cycle
				- If you attach multiple EBS volumes to an EC2 instance, you can stripe data across the volumes for increased I/O and throughput performance
		
		- DeleteOnTermination flag:
				- 
					By default, the root EBS volume that is created and attached to an instance at launch is deleted when that instance is terminated. You can modify this behavior by changing the value of the flag DeleteOnTermination to false when you launch the instance. This modified value causes the volume to persist even after the instance is terminated, and enables you to attach the volume to another instance.
				- 
					By default, additional EBS volumes that are created and attached to an instance at launch are not deleted when that instance is terminated. You can modify this behavior by changing the value of the flag DeleteOnTermination to true when you launch the instance. This modified value causes the volumes to be deleted when the instance is terminated.
		
		- EBS Volume Types:
				Solid State Drives:
					gp3
					gp2
					io2 block express
					io2
					io1
				Hard Disk Drives:
					st1
					sc1
					standard (Magnetic)
				
		- Overall of the below five instances: Max IOPS per instance = 80000, Max throughout per instance = 2375MB/s
				* gp3: 
					General Purpose IOPS based, Boot volumes, SSD, 1GiB - 16TiB, 500IOPS/1GiB,  IOPS=3000/Baseline->16000IOPS, Throughput=125/Baseline->1000MB/s
				* GP2: 
					General Purpose IOPS based, Boot volumes, SSD, 1GiB - 16TiB, 3 IOPS/1 GiB (IOPS follows the size), Min IOPS/volume = 100, Max IOPS/volume = 16000(i.e.5334GiB), Burst upto 3000IOPS for 30mins for <1000 GiB, Max Throughput=250MiB/s
				* io2 block express: 
					4GiB - 64 TiB, IOPS/volume=256000, Max Throughput/volume=4000MiB/s
				* io2, io1: 
					Provisioned IOPS, Ideal for IO intensive Tx database (High IOPS, High throughput, Low latency ), Can be a boot volume, SSD, 4 GiB - 16 TiB, 50IOPS/ 1 GiB, IOPS & Trhoughput can be scaled independent of the Size, Max IOPS = 32000(Most), Max IOPS = 64,000 (Nitro), Max Throughput=1000MiB/s 
				* ST1: 
					Throughput Optimized MiB/s based, Ideal for dist file systems & streaming & big data & log processing & dw ucs, Frequent accessed data, Cannot be a boot volume, HDD, Max Throughput=500MiB/s, Size = 125 GiB to 16TiB, Credit bucket fill rate =40MiB/s, Total bucket size=1TiBs
				* SC1: 
					Cold HDD MiB/s based, Normal latency workloads, less frequently access, Cannot be a boot volume, HDD, Max Throughput=250MiB/s, Size = 125 GiB to 16TiB, Credit bucket fill rate =12MiB/s, Total bucket size=1TiBs
				* Magnetic EBS: 
					HDD, High latency workloads, Infrequent access, Max 100IOPS, Size = 1GiB to 1TiB
		Volume:
				** EBS-backed EC2 instance can be stopped, terminated, restarted, rebooted
				** Instance-store backed EC2 cannot be stopped, they can be rebooted & terminated, data on the store is retained after reboot / it CANNOT BE STOPPED or RESTARTED
				** 5000 EBS Volumes per Account
				** EBS Volume cannot be down-sized (for example: you cannot downsize from a GP2 to Magnetic-Standard or try to reduce size from say 16GB to 2GB)

		EBS Snapshots:
				- First snapshot is full copy. Subsequent snapshots is incremental delta
				
				- How to take a snapshot?
						- Non-Root EBS volume: Best way to ensure consistency is to detach the EBS volume, then take a snapshot and re-attach the volume. You DO NOT have to wait for the snapshot to be completed to reattach the volume. The idea is to stop all I/Os so that a 'consistent' point-in-time copy can be taken.
						- Root EBS volume: Best way to ensure consistency of a snapshot is to shutdown the EC2 instance, then take a snapshot and then restart EC2 instance
						- Automatic backups using Amazon Data Lifecycle Manager or AWS Backup
						- Multi-volume snapshots allow you to take exact point-in-time, data coordinated, & crash-consistent snapshots across multiple EBS volumes attached to instance
						- You cannot create snapshots from instances for which hibernation is enabled
						- You cannot create snapshots from hibernated instances
						- Having multiple pending snapshots of a volume can result in reduced volume performance until the snapshots complete.
						- When a snapshot is created from a volume with an AWS Marketplace product code, the product code is propagated to the snapshot
						- Snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached in memory by any applications or the operating system

				- Relations among incremental snapshots of different volumes
						~ Vol 1 has 10 GiB of data. Because Snap A is the first snapshot taken of the volume, the entire 10 GiB of data is copied and stored.
						~ Vol 2 is created from Snap A, so it is an exact replica of Vol 1 at the time the snapshot was taken.
						~ Over time, 4 GiB of data is added to Vol 2 and its total size becomes 14 GiB.
						~ Snap B is taken from Vol 2. 
						~ *********For Snap B, only the 4 GiB of data that was added after the volume was created from Snap A is copied and stored. The other 10 GiB of unchanged data, which is already stored in Snap A, is referenced by Snap B instead of being copied and stored again. **************
								##########################################################################################################################################
							****<>*<>*<><><> Snap B is an incremental snapshot of Snap A, even though it was created from a different volume ****<>*<>*<><><>
								##########################################################################################################################################
				
				- Quotas: 
						- 10000 EBS Snapshots per Account
						
				- Storage:
						- EBS Snapshots are stored in S3 (system S3 and not user's S3), but available from EC2 APIs and EC2 console, but not from S3 console
						- EBS Volume created from a snapshot must be of same size or larger than the snapshot size
				
				- Encryption:
						- If source EBS volume is encrypted, snapshot is encrypted using same keyes 
						- If source EBS volume is not encrypted, snapshot is not encrypted
						- Volumes that you create from an unencrypted snapshot that you own or have access to can be encrypted on-the-fly.
						- When you copy an unencrypted snapshot that you own, you can encrypt it during the copy process.
						- When you copy an encrypted snapshot that you own or have access to, you can reencrypt it with a different key during the copy process.

						
				- Visibility & Sharing: 
						- EBS Volume is visible at the AZ level, while Snapshots created from the EBS volume are visible at the REGION level
						- Snapshots that are unencrypted can be shared with all AWS accounts / PUBLIC SHARING
						- Snapshots that are encrypted using AWS managed key CANNOT be shared
						- Snapshots that are encrypted using customer managed key can be shared from 1 account to another account / PRIVATE SHARING
								- When you share an encrypted snapshot, you must also share the customer managed key used to encrypt the snapshot
								- Following perms to be granted to the recipient: 
															kms:DescribeKey
															kms:CreateGrant
															kms:GenerateDataKey
															kms:ReEncrypt
															kms:Decrypt
						- The recipient of the snapshot share may create a new volume from the share
									CloudTrail event = SharedSnapshotVolumeCreated  
						- The recipient of the snapshot share may copy to a new snapshot
									CloudTrail event = SharedSnapshotCopyInitiated 
						
				
				- Copying a Snapshot:
						- Use Cases:
								~ Geographic expansion: Launch your applications in a new AWS Region.
								~ Migration: Move an application to a new Region, to enable better availability and to minimize cost.
								~ Disaster recovery: Back up your data and logs across different geographical locations at regular intervals.
								~ Encryption: Encrypt a previously unencrypted snapshot, change the key with which the snapshot is encrypted 
								~ Data retention and auditing requirements: Copy your encrypted EBS snapshots to another AWS account for preservation, auditing or data retention 
						- Amazon S3 server-side encryption (256-bit AES) protects a snapshot's data in transit during a copy operation
						- The snapshot copy receives an ID that is different from the ID of the original snapshot
						- To share a snapshot with another REGION, you need to copy the snapshot from one region to another and then share the copy
						- Whether a snapshot copy is incremental is determined by the most recently completed snapshot copy:
								When you copy a snapshot across Regions or accounts, the copy is an incremental copy if the following conditions are met:
									The snapshot was copied to the destination Region or account previously.
									The most recent snapshot copy still exists in the destination Region or account.
									All copies of the snapshot in the destination Region or account are either unencrypted or were encrypted using the same KMS key.
								If the most recent snapshot copy was deleted, the next copy is a full copy, not an incremental copy. If a copy is still pending when you start a another copy, the second copy starts only after the first copy finishes.
				
				- Remember AMIs store the image in a SNAPSHOT and not an active volume
		
		EBS Encryption:
				- All EBS Volume Types support EBS Volume Encryption
				- AES-256 bit encryption
				- EBS Encryption/Decryption happens at the EC2 to which the volume is attached. 
				- All EC2 instance families support EBS Volume Encryption 
						**** But remember there might be some instances within the instance family that does not support EBS volume encryption
				- If the EBS Volume is not configured to be encrypted, then the traffic from EC2 to EBS volume is not encrypted
				- If not using an Encrypted EBS Volume, you can use 3rd party encryption tools or Guest-OS level encryption or Application can encrypt or Encrypted File system
				- NVMe (NonVolatileMemoryExpress) instance storage encryption is always ON(XT-AES-256 block cipher), and cannot be disabled

		AMI
				- AMI is region-specific 
				- AMI can be copied from one region to another (similar to snapshot) 
				- AMI can be shared from 1 account to another - private-public
				- AMI can be created for Instance Store-backed EC2 as well as EBS-backed EC2
				- An AMI created from an EC2 instance takes snapshots of every EBS volume and Instance store attached to the instances and saves the same in S3
				- If any volumes attached to the origin EC2 instance is encrypted, then the new AMI will launch successfully on those instance that support EBS encryption
				
		AMI Block Device mapping:
				- The mapping defines the block devices (instance store volumes and EBS volumes) to attach to an instance
				- AMI's block device mapping will show all storage associated to the config. For root volume, you can increase the volume size, volume type, and "Delete on Termination" flag. You cannot decrease the volume size.
				- EC2 Instance's block device mapping will show only the actual storage attached to the EC2 instance. The EC2 instance's block device mapping is accessible at 169.254.169.254/latest/meta-data/block-device-mapping
				- Note that all NVMe instance store volumes supported by an instance type are automatically enumerated and assigned a device name on instance launch
				- Entries within the mapping:
						- Device name used within EC2 instance (like /dev/sd01, /dev/sda, .......)
						- Instance store volumes: <ephemeral[0-23]> 
						- NVMe volumes are automatically enumerated
						- EBS volumes:
							- ID of the snapshot used to create the block device (like snap-xxxxx)
							- Size of the volume in GiB
							- Delete the volume on instance termination = true / false 
							- Volume type = gp2, gp3, io1, io2, st1, sc1, standard 
							- Number of IOPS for io1, io2
				- Using Block Device Mapping:
						- You can specify a block device mapping as part of creating an AMI (Instance store volumes are not supported in AMI specs)
						- You can attach block devices while launching an Instance
						- You can modify the block device mapping of the AMI when launching an instance from the AMI

		EBS Elastic Volumes:
				- With Amazon EBS Elastic Volumes, you can increase the volume size, change the volume type, or adjust the performance of your EBS volumes
				- After you increase size of a volume, run commands to extend the file system to the larger size using resize2fs (ext4) or xfs_growfs (XFS)	
				
				
		Amazon Data Lifecycle Manager
				- Used to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs
				- Uses resource tags to identify the resources to back up
				- applies the following tags to all snapshots and AMIs created by a policy
								aws:dlm:lifecycle-policy-id
								aws:dlm:lifecycle-schedule-name
								aws:dlm:expirationTime
								dlm:managed
				- DLM Policy:
						~ Snapshot lifecycle policy:		Used to automate the lifecycle of EBS snapshots. These policies can target EBS volumes and instances.
						~ EBS-backed AMI lifecycle policy: 	Used to automate the lifecycle of EBS-backed AMIs. These policies can target instances only.
						~ Policies can have up to four schedules—one mandatory schedule, and up to three optional schedules
		
				
	===================================================
	Load Balancers:
	===================================================
		General:
			99.99% availability
			ELB is scoped at a REGION level i.e. can span all AZs within a region
			To do inter-region load balancing, you need Route 53
			ELB Service can scale the load balancer layer by provisioning multiple ELB Node (not visible to user or administrator)
			ELB Front End: User facing side of the ELB
			ELB Back End: EC2 facing side of the ELB
			ELB placement:
				- Internet Facing LB: Internet resolvable DNS is assigned. 
					example: name-12344.region.elb.amazonaws.com
				- All ELBs support: IPv4, IPv6, Dual Stack (IPv4+v6) DNS names
				- ELB is targeted towards 1 or more AZs
				- Only 1 subnet needs to be identified in each AZ where you want the ALB to be functional in
				- If the ELB is internet-facing, the subnet selected must be PUBLIC subnet
				- If the ELB is intranet-facing, the subnet selected must be PRIVATE subnet
				- If you select another subnet in the same AZ, it replaces the former one
				- Internet-facing LB can load-balance on web instances that reside in a PRIVATE subnet i.e. the instances themselves NEED NOT be in a PUBLIC subnet
				***** If you do not select subnet in an AZ, then the instances in that AZ DOES NOT GET TRAFFIC
			Hybrid load balancing: [Definition] If an application runs on targets distributed between a VPC and an on-premises location, you can add them to the same target group using their IP addresses
			Cross-zone load balancing DISABLED:
				Each AZ will get an equal share of the traffic irrespective of the actual number of targets in each AZ
			Cross-zone load balancing DISABLED:
				Load-balancing will be done equally across all targets (like EC2) irrespective of the zone
			Health Checks:
				- AWS console uses HTTP:80 for health checks. Registered instances MUST RETURN "HTTP 200 OK" in response within the timeout duration to be marked as HEALTHY 
				- Sometimes, the web server may not have a standard implementation of HTTP (like Legacy applications). In this case, switch the health check to TCP:80 for health checks. In this case, the ELB sends a SYN packet to the instance. If the instance responds with SYNACK within timeout duration, the instance is healthy. If ELB does not received SYNACK, then the instances in UNHEALTHY
				 - If an instance is UNHEALTHY, then it must pass consecutive number of HEALTHY THRESHOLD to 
		** CLB:
			Layer 4+7 LB / Supports HTTP, HTTPS, TCP, TLS
			Supports HTTP, HTTPS, TCP (but not HTTP/2)
			Command: aws elb describe-load-balancers
			Upto 100 listeners can be configured per CLB
			1:1 mapping between Front End Listener and Back End Listener
			CLB: Internet facing AND Internal facing
			LIMITATION: A CLB can host only 1 TARGET GROUP
				A Target could ONLY be a:
					- EC2 instance
			Cross-zone load balancing is enabled by default in Classic Load Balancer
			CLB supports Backend-authentication i.e. SSL auth on connection #2
			CLB supports EC2-in-VPC and EC2-Classic
			Connection draining is supported = default 300 seconds
			Listeners:
				- Front End Listener- Port + Protocol / listens for incoming connection requests from user to ELB at a port and protocol
				- Backend Listener	- Port + Protocol / configured for traffic from ELB to backend instances at a port and protocol
				- Front-End PORT + PROTOCL <=> BACK-end PORT + PROTOCOL
				- EXAMPLE: Suppose you have two DISTINCT apps: app1.rrd.com and app2.rrd.com. app1 is hosted on a group of EC2 instances. app2 is hosted on another group of EC2 instances. There is only CLB that front-ends these two groups. Now CLB does not possess the ability to recognize distinct apps, and EC2 instance groups; therefore, it does not have the ability to route traffic appropriately based on the website name. Therefore, traffic for app1 may be routed to an instance hosting app2 and vice-versa, which will result in HTTP response 404. Therefore, app1 and app2 cannot be hosted on the same CLB. Two separate CLBs will need to be provisioned for the individual apps.
			HTTPS and PROXY mode:
				- If front end is TCP then backend is restricted to TCP. Packets are forwarded and the request is not manipulated
					- This is known as the ** PROXY MODE ** and ELB does not teminate the connection
				- If front end is TLS then backend can be TCP or TLS.	 Packets are forwarded and the request is not manipulated
					- This is known as the ** PROXY MODE ** and ELB does not teminate the connection
					- ELB is not aware of encrypted traffic at Layer 4. ELB does not host TLS certificate for decryption. SSL certificate is hosted by EC2 instance and it is the EC2 instance that terminates the connection and decrypts the request
				- If front end is HTTP, then backend is restricted to HTTP. Connection#1 is terminated at the ELB stage & a fresh HTTP connection#2 is initiated from ELB to the EC2 stage and headers can be manipulated at each stage
				- If front end is HTTPS, then the backend can be HTTP or HTTPS. Connection#1 is terminated at the ELB stage & a fresh HTTP connection#2 is initiated from ELB to the EC2 stage and headers can be manipulated at each stage. ELB is aware of the encrypted traffic. ELB hosts the SSL certificate and terminates connection#1. If the backend traffic is HTTPS, then the backend EC2 instance hosts another certificate and connection #2 is also encrypted
				- TCP Listeners support source client / source details by forwarding the details in a PROXY mode
				- HTTP Listeners supports client / source details in the following request headers:
					X-Forwarded-For		: IP address of the source
					X-Forwarded-Proto	: Protocol of the source
					X-Forwarded-Port	: Port of the source
			SSL - X.509 Certificate:
				- ELB is great for hosting server certificates
					** For this put the host 
				- CLIENT AUTHENTICATION:
					** ELB does not support client authentication using client certificates. 
					** To authenticate a client, you must switch the ELB to TCP mode and configure the PROXY protocol such that the SSL termination happens at the EC2 instances. 
					** And the client authenticates the server using server certificates and 
					EC2 instances authenticate the client using client cert. 
					** ELB in HTTPS mode DOES NOT support client authentication, ELB MUST BE IN THE PROXY MODE
				- Session stickiness IS NOT OPTION IN THE TCP PROXY mode
				- CLB does not support SNI. Therefore, each distinct domain needing a SSL certificate will need it's own CLB OR alterntaively put the CLB into TCP mode and proxy the traffic all the way to an EC2 instance which then reads the SNI header in the certificate and does downstream processing
			LOAD TESTING:
				- Load distribution could be a problem if DNS caching is enabled at the test client side
				- While using Route 53, ensure your load testing software is not caching the DNS results and sending traffic to same ALB
				- Instead ensure that the load test software polls the Route 53 for IP addresses of the next ALB in round robin
			Monitoring:
				- AWS Cloudwatch receives ELB metrics every 60 seconds
				- CLB logs are DISABLED by default. Contains Source IP address, proto and port, request time, etc. Logs stored in S3
				- CloudTrail receives all API calls from ELB. Logs stored in S3
			LIMITATION: 
				A CLB can host only 1 TARGET GROUP
				WAF Integration is not available with Classic Load Balancer
				CLB does not have delete protection feature
				CLB does not support SNI in TLS certificates
				CLB does not support Path-based routing or Host-based routing
				CLB does not support dynamic host port mapping that is required for ECS container services
				CLB does not preserve source IP of the client by default
				SOURCE IP ADDRESS PRESERVATION:
					- Not supported in CLB or ALB
					- ONLY SUPPORTED IN NLB
	===================================================
	ALB:
	===================================================
			ALB is scoped at a REGION level
			Layer 7 LB / Supports HTTP, HTTP/2, HTTPS, Websocket
			Does not support OSI Layer 4; use NLB instead
			Command: aws elbv2 describe-load-balancers
			ALB: Internet facing AND Internal facing
			An ALB can ROUTE to multiple TARGET GROUPS based on RULESs 
				- Upto 50 listeners per ALB
				- Upto 100 rules can be defined per ALB (i.e. route to TG1 or TG2 or TG3 based on rules)
					- Conditions are driven by Host Header (i.e. Domains or FQDN !!), URL PATH, Query String, Http Header, Source IP
			- EXAMPLE: Suppose you have two DISTINCT apps: app1.rrd.com and app2.rrd.com. app1 is hosted on a group of EC2 instances (TG#1). app2 is hosted on another group of EC2 instances (TG#2). There is only one ALB that front-ends these two groups. Now ALB possesses the ability to recognize distinct target groups. And therefore it does have the ability to route traffic appropriately based on the website name. Therefore, traffic for app1 will get routed to instances in TG1 and app2 to TG2. Therefore, app1 and app2 cannot be hosted on the same ALB. There is not need to host multiple ALBs.
			- CLB Frontend Listener (Port + PROTOCOL) points to ONE CLB Backend Listener. And that CLB Backend Listener is a IP address + Port + Protocol 
			** ALB Components:
				- Frontend Listener
				- Rule [=Priority + Condition + Action]
				- Target Group
				- Target
				- Health Check
			- ALB Frontend Listener (PORT + PROTOCOL) points to a Target Group based on the rules that you define
					- PORT + PROTOCOL
					- There is NO concept of a Backend Listener
					- Upto 50 listeners per ALB
			- ALB Rules = Priority + Conditions Matched + Actions
					- Upto 100 rules per ALB
					- An ALB needs to have a default rule 
					- Default Rule is MANDATORY, cannot have conditions and cannot be deleted
					- Priority 
						- Lower number has HIGHER priority and evaluated earlier
						- Default Rule gets evaluated LAST
					- Conditions:
						Two types:
							Host based: Domain name
							Path based: URL Path (like /images), Query String
						You can combine Host based and Path based rules
			- Target Group contains multiple Targets of the same type 
					- Upto 1000 targets per ALB
					- A Target Group is collection of 1 or more Target(s) of the SAME TYPE
					- A Target = PORT + PROTOCOL + Backend Services
					- A Target Backend Service could be:
							- EC2 instances by Instance ID - If Instance ID used for selection, then traffic sent to Primary Private IP address on eth0
							- Private IP address assigned in a VPC
							- Private IP address assigned in a PEERED VPC 
							- Private IP address assigned in On-premise connected to VPC via AWS VPN Connection
							- Private IP address assigned in On-premise connected to VPC via AWS Direct Connect
							- ECS container
							- One Type of Lambda Function
							- RFC 6598 IP addresses assigned to EC2 Instance (AWS = 100.64.0.0/10)
						A Target cannot be an internet routable public IP address
						If a Target is specified by an Instance ID, the Primary Private Address assigned to eth0 is used for routing
						A Target could be a member of multiple Target Groups
						A Target can be added to the same group multiple times using different port numbers				
						A target cannot be publically routable IP address
						ALB selects target from the target group based on ROUND ROBIN ALGORITHM, unless sticky session is enabled
					- A Target Group cannot mix types from the above. 
						A Target Group could be attached to only one Protocol and Port of incoming listener in an ALB
						Auto Scaling Groups can scale a EC2 Target Group
						Health Check can be assigned to a Target Group
			TLS Termination & Certificates:
				Upto 25 certificates per ALB
				SNI:
					- ALB supports multiple TLS/X.509 certificate using SNI
					- SNI: Extension to TLS that enables an ELB to host multiple SSL certificates at the same IP and Port
					- Based on the host name in the SNL header, the ELB selects the correct certificate to present to the client. SNI is automatically enabled when you associate more than one TLS certificate with the SAME SECURE LISTENER on a load balancer. Similarly, SNI mode for a secure listener is automatically disabled when you have only one certificate associated to a secure listener
					You can associate multiple certificates for the same domain to a secure listener. 
						For example, you can associate:
						ECDSA and RSA certificates
						Certificates with different key sizes (e.g. 2K and 4K) for SSL/TLS certificates
						Single-Domain, Multi-Domain (SAN) and Wildcard certificates
			WAF Integration:
				Conditions checked: IP address, HTTP header, HTTP body, URI string, SQL injection, XSS
				Action: ALLOW, BLOCK, MONITOR
			Cross-zone load balancing is ENABLED by default in Application Load Balancer
			Hybrid load balancing: 
				- If an application runs on targets distributed between a VPC and an on-premises location, you can add them to the same target group using their IP addresses
			Lambda Invocation:
				The content of the request (including headers and body) is passed on to the Lambda function in JSON format. 
				The response from the Lambda function should be in JSON format. The response from the Lambda function is transformed into an HTTP response and sent to the client
			ALB DOES NOT support Backend-authentication
			ALB does not support EC2-Classic. The EC2 must be in a VPC
			ALB has delete protection feature
			Deregistration delay = 300 seconds (default) upto 3600 seconds (akin to Connection draining in CLB)
			Sticky Session:
				ALB generates a cookie named "AWSALB" that has information about the EC2 instance that was selected for serving 
				Cookie is encrypted by ALB using rotating key
				Session stickiness is set at the Target Group level
				You can set the duration of the stickiness
				With each client request that contains the cookie, the expiration duration extends
			Websockets: 
				Starts off as a HTTP protocol
				You need to send HTTP Upgrade header to upgrade to Websockets API
			Health Check
				- Health Check is configured at the Target Group level
				- If none of the health checks pass, then the ALB routes the traffic to all instances in the Target Group
			Monitoring:
				Via CloudWatch metrics reported every 60 seconds
				Via X-amzn-Trace-Id header which is added by the ALB to trace the request from the ALB to the target
				Via CloudTrail for API calls
				Via ALB logs stored in compressed format in S3 buckets
			ALB does not preserve Source IP of the client by default
			Supports client / source details
				X-Forwarded-For
				X-Forwarded-Proto
				X-Forwarded-Port
			ALB can keep backend connections to the targets alive by setting the TCP KeepAlive header
			SOURCE IP ADDRESS PRESERVATION:
				- Not supported in CLB or ALB
				- ONLY SUPPORTED IN NLB
		** Network Load Balancer:
			- Layer 4 LB / Supports TCP, UDP, TCP+UDP on same port, TLS
			- Handles millions of requests/sec, sudden volatile traffic patterns and provides extremely low latencies
			- In addition Network Load Balancer also supports TLS termination, preserves the source IP of the clients, and provides stable IP support and Zonal isolation. 
			- It also supports long-running connections that are very useful for WebSocket type applications
			** NLB Components:
				- Frontend Listener
				- Rule [=Priority + Condition + Action]
				- Target Group
				- Target
				- Health Check
			- NLB Frontend Listener (PORT + PROTOCOL) points to a Target Group based on the rules that you define
					- PORT + PROTOCOL
					- There is NO concept of a Backend Listener
			- NLB creates an ENI in each AZ where it is configured to load-balance
			- A Target could be:
					- EC2 instances by Instance ID - If Instance ID used for selection, then traffic sent to Primary Private IP address on eth0
					- Private IP address assigned in a VPC
					- Private IP address assigned in a PEERED VPC 
					- Private IP address assigned in On-premise connected to VPC via AWS VPN Connection
					- Private IP address assigned in On-premise connected to VPC via AWS Direct Connect
					- RFC 6598 IP addresses assigned to EC2 Instance (AWS = 100.64.0.0/10)
					A target cannot be publically routable IP address
					In NLB, a Target cannot be Lambda function or ECS container
					If a Target is specified by an Instance ID, the Primary Private Address assigned to eth0 is used for routing
					A Target could be a member of multiple Target Groups
					A Target can be added to the same group multiple times using different port numbers				
					A target cannot be publically routable IP address
					ALB selects target from the target group based on ROUND ROBIN ALGORITHM, unless sticky session is enabled
				- A Target Group cannot mix types from the above. 
					A Target Group could be attached to only one Protocol and Port of incoming listener in an ALB
					Auto Scaling Groups can scale a EC2 Target Group
					Health Check can be assigned to a Target Group
			NLB: Internet facing AND Internal facing
			NLB exposes End-point using Static IP address:
				- NLB is exposed as a static private address which can be coupled with an elastic IP address
				- NLB setup ends up generating an ENI per AZ with a STATIC IP address for the NLB in each AZ where NLB is configured to load-balance
				- You can attach a Public Elastic IP to that STATIC IP addresses
			Cross-zone load balancing needs to be explicitly enabled in Network Load Balancer
			PrivateLink: Network Load Balancers with TCP and TLS Listeners can be used to setup PrivateLink. You cannot setup PrivateLink with UDP listeners on Network Load Balancers.
			UDP Flow: 
				While UDP is connectionless, the load balancer maintains UDP flow state based on 5-tuple hash, making sure that packets sent in the same context are consistently forwarded to the same target. 
					The flow is considered active as long as traffic is flowing and until the idle timeout is reached. Once the timeout threshold is reached, the load balancer will forget the affinity, 
					and incoming UDP packet will be considered as a new flow and load-balanced to a new target. 
			TLS Termination & Certificates:
				Supported by NLB
				SNI:
					SNI is automatically enabled when you associate more than one TLS certificate with the same secure listener on a load balancer. 
					Similarly, SNI mode for a secure listener is automatically disabled when you have only one certificate associated to a secure listener
			SOURCE IP ADDRESS PRESERVATION:
				- Not supported in CLB or ALB
				- NLB preserves Source IP of the client in the TCP frame, if the target is an EC2 instance identified by an Instance ID
					- In ALB and CLB, with each hop the Source IP is replaced by the IP of the sender in the hop. While the original client details is passed on via the X-Forwarded headers
					- But Client Source IP preservation is helpful so that NLB can forward traffic to WAFs, NGFW, Filters and other security products which may decide to drop the traffic all together based on the SOURCE IP address in the packet
					- The traffic that passes the security layer then goes to the application layer
					- But remember that since the Source IP now happens to be the public IP of the client, we must provide a path for the response using NAT Gateway or NAT Instance or Internet Gateway
					- Remember that NACL of the EC2 instances must ALLOW traffic from source client IP and not the IP of the NLB
					- Look at NLB Proxy as well
			NLB Proxy - 
				NLB, in the PROXY mode, will inject the Source i.e. Client IP address to the TCP body
				The Target must have ability to read Source IP address from TCP body
			NLB do not support security groups
			Health Check:
				- NLB uses active and passive health checks
				- Active health check: NLB sends periodic health checks to Targets in Target Group
				- If there are no Healthy Targets in any of the Target Groups, then request gets targeted to all enabled AZs
			Monitoring:
				Use VPC Flow Logs at the NLB ENI level / Logs stored in S3 bucket
				Via CloudWatch metrics reported every 60 seconds
				Via CloudTrail for API calls
				Via NLB logs - DISABLED BY DEFAULT -  stored in compressed format in S3 buckets

	===================================================
	Auto-scaling:
	===================================================
		- Similar to ELB, Auto-scaling is a REGIONAL service that can span multiple AZs within a VPC within a REGION
		- Auto-scaling cannot scale over the boundaries of a REGION
		- 	** An Instance can be part of multiple Target Groups of an ELB
			** BUT An Instance CANNOT be part of multiple Auto-Scaling Groups
		- Composed of 3 components:
			** Launch Configuration + 
			** Auto-Scaling Group + 
			** Auto-Scaling Policy
		- Launch Configuration:
			- Used to define the template of the EC2 Instance
			- Parameters are AMI, Instance family (A, C, M, R, T, etc) - Instance Type (C4, C5 T2, T3, etc), On-Demand/Spot/RI, Key-pair, EBS Volumes, Security Groups, Tags, IAM Role, CloudWatch Monitoring (BASIC, DETAILED)
			- MIXING PURCHASE OPTIONS:
				You can mix On-Demand, Spot and RI instances:
				- Specify On-Demand base 
				- Specify On-Deman percentage above the base 
				- Specify Spot percentage
				- Specify Spot allocation strategy per Availability Zone (i.e. Cost optimized, Capacity optimized)
				- Prioritize instance types that can benefit from Reserved Instance or Savings Plan discount pricing.
			- [OPTIONALLY] MIXING Instance Types within a Family
				- Auto-Scaling group can be configured to launch multiple instances types of a family (like C5.2xlarge, c5.4xlarge, ...)
				- By default, a C5.2xlarge is treated as having same weight as  C5.4xlarge
				- But, you can provide Instance Type Weight wherein C5.4xlarge can be weighed at 4 while C5.2xlarge is weighed at 2 
			- Launch Configuration, once defined, is not editable
			- But a new Launch Configuration can be defined and attached to the Auto-Scaling Group
			- If the launch configuration was created from the CONSOLE, the EC2 metrics values are collected every 5-minutes (BASIC)
			- If the launch configuration was created from the CLI, the EC2 metrics values are collected every 1-minute (DETAILED)
		- Auto-Scaling Group:
			- Min, Desired, Max size of the group
			- Define the VPC, Subnets in AZs, ELB to use, Health Check Grace Period, Instance Protection, Scaling Policies
			- Collection of EC2 instances to serve a workload
			- It can span multiple AZs within a region
			- Auto-Scaling tries to distribute EC2 instances evenly across AZs as long as the AZ has the capacity
			- [PROVISIONING] Auto-Scaling group tries to launch instances in all registered AZs EVENLY
			- [IMBALANCE] Reasons for imbalance in EC2 distribution across AZs:
				- AZ went down, and therefore instances get provisioned in another AZ(s). The failed AZ comes back up again
				- AZ did not have enough capacity and now has enough capacity to host EC2s
				- User terminated specific EC2 instances, thus creating imblanace
				- Bid price fell below market spot rate, and EC2 instances got terminated
			- [AUTO-REBALANCING] If Auto-Scaling detects imbalance in distribution, it tries to rebalance in the following sequence: 
				1. provisioning EC2 instances in under-utilized AZ and 
				2. then terminating EC2 instances from over-used AZ
				During rebalancing, Auto-Scaling allows you to temporarily breach the Max size constraint by 10% or 1EC2 (whichever is greater). This avoids negative user impact
			-[MANUAL-REBALANCING] Admin can attach an EC2 instance in an AZ to the ASG, provided the instances us RUNNING, AMI is available, and Instance not part of another ASG
		- Auto-Scaling Policy
			- Three types of Scaling:
				1. Manual Scaling: Min, Max, Desired is manually adjusted by user constantly
				2. Schedule-based or Cyclic Scaling
				3. Event-based Scaling
					- Event Based Scaling in turn has 3 Policy Types:
						3.A On-Demand Simple Scaling
						3.B On-Demand Step Scaling
						3.C Target Tracking Scaling
			2. Scheduled or Cyclic Scaling:
				- Scaling scheduled once or recurring at a set date-time
				- Cannot configure two scheduled actions for the same ASG at the same date-time
			3.A On-Demand Simple Scaling:
				- Policy gets triggered in response to an ALARM ON A METRIC 
				- (example: scale out when CPU Util>70%, scale in when CPU Util < 60%)
				- Cool Down Period: (default = 300 seconds) to let the ASG provision new EC2 instances and newly spawned EC2 instances to contrbiute to the metric normalization
			3.B On-Demand Step Scaling:
				- Policy gets triggered in response to the TIER OF THE METRIC VALUE
				- (example: if CPU Util >80% add 5 instances, between 70 & 80% add 5 instances, less than 70% remove 2 instances, less than 50% remove 5 instances)
				- (example: if CPU Util >80% add 10% instances, between 70 & 80% add 20% of DESIRED capacity, less than 70% remove 10% of DESIRED capacity, less than 50% remove 30% of DESIRED capacity)
				- Warm Up Period: Time for the newly spawned instances to be up and running and ready to contribute to the metric normalization
			3.C Target Tracking Scaling Policy:
				- Policy gets triggered to maintain the GROUP's AVERAGE VALUE OF A METRIC to the desired value specified in the configuration
				- (example: maintain the average CPU utilization of the ASG at 75% || ASG will scale out, scale in automatically to maintain the target 75% util)
				- Average values: 
					ALB Request Count per Target
					Average CPU Utilization
					Average Network In (Bytes)
					Average Network Out (Bytes)
		- Putting EC2 Instance of an ASG into STANDBY mode:
			- If you want to troubleshoot and/or reboot an EC2 instance within an ASG, you can temporarily detach it from the ASG
			- When you  detach it from the ASG, the instance goes into a STANDBY mode
			- A STANDBY instance is not considered in the count of ASG and is not considered in the health check probe
			- A STANDBY instance is still charged as In-service instances
		- Auto-Scaling Monitoring:
			Two settings:
			A. Rate at which metrics are sent by EC2 Services to CloudWatch (i.e. BASIC 5-minutes or DETAILED 1-minute frequency)
			B. Rate at which metrics are calculated/aggregated by CloudWatch for Scaling Policy purposes
			- Ideally A and B rates match
			- What would not make sense is if frequency of B is greater than frequency of A, in which case you are running empty cycles
				(example: EC2 sends metrics to CloudWatch every 5 minutes; while the Average CPU util is calculated every 1-minute)
		- ELB->Auto-Scaling Group:
			- When a ELB is targeted on an ASG as its targets, all EC2 instances within the ASG get AUTOMATICALLY registered with the ALB
			- The sysops admin need not do anything for this automatic registration to happen
			- If connection draining was defined for an ELB, ASG will honor the same while detaching or terminating EC2 instances
			- STANDBY Instances are deregistered from the ALB automatically
			- Terminated Instances are deregistered from the ALB automatically		
		- Auto-Scaling Health Check:
			- ASG has its own health check on EC2 targets (HEALTHY, UNHEALTHY || Health Grace Period 300 seconds <default> after reboot to check the EC2 Instance health)
			- If ASG is front ended by an ELB, ASG has access to the Instance health check provided by the ELB as well
			- ELB Health Checks MUST BE EXPLICITLY CONFIGURED TO BE FED INTO ASG HEALTH CHECKS
			- Once ELB Health Checks are fed into ASG, ASG can get alerted either by its own health check or the ELB health checks
			- And, if any of the health checks (ASG or ELB) indicates unhealthy instance, ASG will TERMINATE the instance
				- There is a short period to save the EC2 instance from termination by running CLI command:: 
						as-set-instance-health <InstanceId> --status Healthy
		- Auto-Scaling Group Merge:
			- Two ASGs in the same AZ can be merged into either one of them
			- ASG1 in AZ1 and ASG2 in AZ2 can be merged into ASG1 provided ASG1 is made Multi-AZ
			- NO NEW GROUP is created during the merge process
		?? - Allows you to select your applications based on resource tags or AWS CloudFormation stacks. 
		Predictive Scaling 
			- EC2 service looks at historic traffic patterns and forecasts them into the future to schedule changes in the number of EC2 instances
			- Predictive Scaling works with in conjunction with target tracking 
			- Predictive Scaling sets up the minimum capacity for your application based on forecasted traffic, target tracking changes the actual capacity based on the actual traffic at the moment
		Auto-Scaling Group Operations SUSPEND - RESUME:
			- Processes within Auto-Scaling group can be temporarily SUSPENDED AND then RESUMED
			- Primary Processes are "Launch" and "Terminate"
			- Secondary processes are AddToLoadBalancer, AlarmNotification, AZRebalance, HealthCheck, ReplaceUnhealthy, ScheduledActions
			- Each of the above processes can be suspended and resumed idependently

	===================================================
	Elastic Container Service:
	===================================================
				REGIONAL service 
				Virtual Machines 
						Virtualize the underlying hardware
						There is Metal, Host OS, Hypervisor, Guest OSs and the App that runs on the Guest OS
						Multiple different types of Guest OSs could be run on the Host OS
						Apps run on the Guest OS and are isolated from the Host OS and peripheral beneath. Hence, considered to be relatively more secure
						VM = Guest OS + Application 
						VM size is usually in GBs, hence boot speed is lower than containers
				Containers 
						Virtualize the Host OS
						There is Metal, Host OS, Container Runtime and Containers on top of Host OS
						There is no traditional Hypervisor or a Guest OS
						There is a Host OS and Multiple Containers encapsulating applications (and dependencies like libraries, code, runtime, tools) running on top of the Host OS
						Containers encapsulate the entire application, runtime, libraries, config files -> therefore Containers are portable
						The Containers share the kernel of the Host OS and is usually running as isolated processes in the User Space
						Apps running on containers are not isolated from each other and the Containers may be visible to each other as they share host OS 
						Container size is usually in MBs. They are relatively light weight, hence boot fast
				Docker
						Packages apps, libraries, runtime, tools, code, etc. into a design-time Docker IMAGE
						A run-time instance of the Docker IMAGE is called a DOCKER CONTAINER
						Docker CONTAINER runs on Docker ENGINE during runtime
						Docker ENGINE runs on Host OS both Windows and Linux 
				Kubernetes
						Orchestrate the creation of containers, scaling of containers, etc.
				Amazon ECS Terminology:
						ECS is an AWS service that works as an orchestrator
						It uses Docker IMAGE to define the components that get packaged within a container
						Docker IMAGE is essentially a design-time snapshot of your container
							Docker IMAGE components and packaging instructions are defined in a Dockerfile
							Docker IMAGE build process pulls the elements defined in the Dockerfile and builds the image
							Docker IMAGE, once built using the instructions in Dockerfile, can be stored in a container registry (AWS ECR OR registry.hub.docker.com)
							Containers are run-time instances of the DOCKER Image
							Containers are known as Tasks
							Docker IMAGE gets rolled up in to an entity called Task Definition
						EC2 Container Instance could either be 
							** EC2 instances (user managed) OR 
							** Fargate (serverless)
							The Container Instance runs the Amazon ECS Container Agent
							Container Agent: 
								- Container Agent is the connector between the EC2 Container Instance and the Tasks 
								- The ECS Container Agent is running on the Container Instance and is contantly interacting with the ECS service
								- The ECS Container Agent registers the EC2 Instance into the ECS cluster
								- Amazon ECS Optimized Linux AMI comes pre-installed with ECS Agent. The agent is pre-configured and tested by AWS Engineers
								- Non Amazon Linux EC2 instances must have the ECS Container Agent installed MANUALLY
								- It sends information about the Container Instance's current running tasks and resource utilization to Amazon ECS
								- It starts and stops tasks whenever it receives a request from Amazon ECS
								- Can be assigned an IAM role with appropriate policies
								- The Container Instance pulls the container image from the container registry
								- The Container Image could be stored on AWS ECR OR registry.hub.docker.com OR a Private Registry
						ECS Cluster:
							- Cluster of Container Instances providing Compute, HA and scaling
							- A Container Instance could be managed by users as EC2 instances **OR** it could be AWS managed (Fargate)
							- A Cluster can mix Fargate and EC2 Instances
							- A Cluster can mix different Container Instance types
						Task (Runtime representationof a Container):
							A Task is essentially a runtime Container (or a group of cohesively tight containers)
							A Task runs on an EC2 instance known as the Container Instance
							A Task is instantiated from a Task Definition
						A Task Definition (Designtime representation)
							A Task Definition defines the following: 
								(This is similar to the Launch configurations of Auto-scaling groups)
								- Task Definition roughly = Docker IMAGE + Launch configurations
								- A Task Definition can define upto 10 Containers in JSON file
									- These 10 containers are for different components of the application
								- Attributes of Container:
									- The Docker image to use with each container in your task and the repository to fetch it from 
									- How much CPU and memory to use with each task or each container within a task
									- The launch type to use, which determines the infrastructure on which your tasks are hosted
									- The command the container should run when it is started
									- The Container Instance (i.e. Container EC2 Host) port that the containers will map to
									- The environment variables that should be used within the containers in the task
									- Any data volumes that should be used with the containers in the task
									- The IAM role that your tasks should use
									- The Docker networking mode to use for the containers in your task
									- The logging configuration to use for your tasks
									- Whether the task should continue to run if the container finishes or fails
								A Task Defintion does not have scaling related information
								Scaling related information is in the Service definition
						Task is instantiation of a Task Definition in an ECS cluster
								- (i.e.) Task is essentially a set of container that have some cohesion 
								- Task utilizes the settings defined in the Task Definition to identify the containers
								- Tasks are run on Docker Enterprise Edition Engine
								- Task Role: Tasks can be assigned IAM role with appropriate IAM policies
								- ECS Cluster is a cluster of EC2 Instances
						Task Scheduler:
							- While Container Instance is responsible for pulling the Docker IMAGE from the registry, the Service Scheduler is responsible for placing Task instance into the cluster
						Service 
							- A Service or the MICRO-SERVICE enables you to run, scale and maintain a specified number of instances of a Task simultaneously in an ECS cluster
							- A Service essentially runs the Task Definitions in an ECS Cluster
							- you can optionally run your Service behind a load balancer (CLB/ALB/NLB). The load balancer distributes traffic across the tasks that are associated with the service
						Docker Engine runs on a cluster of Container Instances
							- Cluster is collection of Container Instances
							- Container Instances are deployed in your VPC, thus providing isolation from other customer containers
						ROLES at various levels:
							- ECS Container Agent makes calls to the ECS services to register Container Instances to cluster. Therefore, Container Instances hosting the agent needs to be assigned IAM Roles. Role needs to be assigned to the ECS Container agent
							- Roles assigned to Tasks (i.e. Containers within Tasks) are called IAM roles for Tasks. In order for ECS Service to be able to assign IAM role to tasks, ECS Service must possess the "ECS Container Service Task Role" (When two Tasks require different roles, they must be defined in separate task definitions)
						
						Dynamic Container-to-Host Port Mapping with ELB:
							- PROBLEM STATEMENT: Multiple instances of the same Task cannot run on the same Container Instance (i.e. Host) as they use the same port which is defined in the Task Definition
							- SOLUTION:
								1. [IMPRACTICAL] Run only one instance of a given Task on the first Container Instance (i.e. Host). 
												 Run the second Task instance on a second Container Instance .... and so on
								2. [PRACTICAL]Run multiple instances of the Task on the same Container Host, but dynamically map their listening port to a different dynamic port on the Container Instance. The ELB (ALB/CLB/NLB) gets further updated with the list of Container Instance ports
						Licensing
							ECS supports Docker Community Edition
							ECS supports Docker Enterprise Edition
			
	===================================================
	AWS Lambda
	===================================================
		AWS Lambda and AWS API Gateway are REGIONAL services
		AWS Lambda and AWS Lambda@Edge runs on top of Amazon Linux distribution
		Use-cases:
			Refer to Eissa Pro video on Lambda@Edge use-cases
		Supported Language for coding:
			- Node.js
			- C#
			- Java 
			- Python
			- Ruby 
			- Go
			- Powershell
		Layers:
			- Enables custom runtime in addition to the 7 languages/runtime listed above
			- It is deployed as a ZIP file containing libraries, runtime and dependencies
			- A function can use 5 layers at a time
			- Layer use permissions can be granted to another account, an organization, or all accounts
		Features:
			Handler method in the function is invoked 
			Arguments passed to the Handler method:
				- Event object: the actual message or messages. Multiple events may arrive in the batch in the poll-based invoke scenario
				- Context object: the env, info about the client, protocol, etc
			Time-out: 1 second to 900 seconds (Default = 3 seconds)
			Memory: 64MB increments from 128MB to 3GB
				- CPU power is proportional to memory allocated
				- Above 1536MB, multiple CPU threads ara allocated. Good to 
				- At 1792MB, a function has the equivalent of 1 full vCPU
			Env variables: 4KB (Environment variables can be passed into Lambda)
			Layers: 5
			Concurrent executions: 1000 per region (all lambda functions)
			Invocation payload: 6MB
			/tmp: 512MB (Use EFS for Lambda)
			Deployment Package Zip: Contains code, dependencies / 50MB-Zipped / 250MB-Unzipped
			Lambda function code can be versioned. A version is immutable. The latest is versioned $LATEST
			Lambda function Aliases can be created (like DEV, QA, STG) and configured to point to one version or two versions (CANARY deployment)
		Where does Lambda execute?
			Lambda functions, by default, executes outside of a VPC
		Lambda accessing VPC resources:
			- Lambda can access resources behind a single Amazon VPC (Security Group is also required for the accessed resource)
			- Lambda function cannot access resources in multiple VPCs
			- Lambda functions configured to access resources in a particular VPC will not have access to the internet as a default configuration
			- If you need access to external endpoints, you will need to create a NAT instance/gateway in your VPC to forward this traffic and configure your security group to allow this outbound traffic
		Three Types of Invocation:
			* Poll-based Invocation
			* Asynchronous Invocation
			* Synchronous Invocation
			* Scheduled Invocation (cron expression or frequency is terms of number of hours/days/weeks)
		* Poll-based invoke: "Event Source mapping" is maintained by the Lambda function
			- Amazon Kinesis
			- Amazon DynamoDB
			- Amazon Simple Queue Service
			Concurrency: Number of instances of a Lambda function running in parallel is equal to the number of shards in Kinesis or DynamoDB
		* [ASYNCHRONOUSLY] Services that invoke Lambda functions asynchronously. "Event Source mapping" is maintained on the invoking AWS service
			- Amazon Simple Storage Service
			- Amazon Simple Notification Service
			- Amazon Simple Email Service
			- AWS CloudFormation
			- Amazon CloudWatch Logs
			- Amazon CloudWatch Events
			- AWS CodeCommit
			- AWS Config
			- AWS IoT
			- AWS IoT Events
			- AWS CodePipeline
			Concurrency: Number of instances of a Lambda function running in parallel is equal to the number of generated by the event source
		* [SYNCHRONOUSLY] Services that invoke Lambda functions synchronously.  "Event Source mapping" is maintained on the invoking AWS service
			- Elastic Load Balancing (Application Load Balancer)
			- Amazon API Gateway
			- Amazon CloudFront (Lambda@Edge)
			- Amazon Kinesis Data Firehose
			- AWS Step Functions
			- Amazon Cognito
			- Amazon Lex
			- Amazon Alexa
			- Amazon Simple Storage Service Batch
			Concurrency: Number of instances of a Lambda function running in parallel is equal to the number of generated by the event source
		Lambda@Edge:
			Global service that 
			* Viewer Request - This event occurs when an end user or a device on the Internet makes an HTTP(S) request to CloudFront, and the request arrives at the edge location closest to that user.
			* Viewer Response - This event occurs when the CloudFront server at the edge is ready to respond to the end user or the device that made the request.
			* Origin Request - This event occurs when the CloudFront edge server does not already have the requested object in its cache, and the viewer request is ready to be sent to your backend origin webserver (e.g. Amazon EC2, or Application Load Balancer, or Amazon S3).
			* Origin Response - This event occurs when the CloudFront server at the edge receives a response from your backend origin webserver.
		API Gateway+Lambda VS Lambda@Edge:
			API Gateway + Lambda are regional services
			Using Lambda@Edge and Amazon CloudFront allows you to execute logic across multiple AWS locations based on where your end viewers are located.
		Lambda Exceptions Handling:
			Poll-based Invoke Kinesis and DynamoDB Streams:
				Lambda polls and pulls a batch of records from the shards in the stream
				When an error happens during execution of Lambda, Lambda continues to attempt processing the batch until the records expire
					(i.e.) the error handling is treated as BLOCKING 
				if the retries succeed or the data expires, the error handling UNBLOCKS  
				Amazon Kinesis and Amazon DynamoDB Streams retain data for minimum 24 hours and max expiry is 7 days
			Poll-based Invoke SQS:
				Lambda polls and pulls a batch of messages from SQS
				If the invocation fails or timesout, messages in the batch are put back in the queue and become avail after visibility timeout
				If the invocation succeeds, all messages in the batch are removed from the queue by Lambda function
			Asynchronous:
				When an error happens during execution of Lambda, the function invocation will retried two times before the event is discarded
				If no DLQ is configured, the event is discarded after two retries
				[Exceeding Throttle Limits] 
					Lambda functions can absorb reasonable bursts of traffic for approximately 15-30 minutes
					Retried two times, after which incoming events will be rejected as throttled.
				Event can be DLQed into a SQS queue or SNS Topic
			Synchronous:
				Lambda function shares error code back to the caller when an error happens
				Client is responsible for catching the exception from the Lambda function and for retrying
				[Exceeding Throttle Limits] AWS Lambda functions being invoked synchronously will return a throttling error (HTTP 429 error - Too Many Requests)

	
	===================================================
	API Gateway
	===================================================
		Features:
			- REGIONAL service providing HTTP, REST and WebSocket API services
			** Supports 
				1)REST API
					REST API : works with Lambda, HTTP Backend, AWS services
					REST API PRIVATE : works with Lambda, HTTP Backend, AWS services
				2)HTTP API : works with Lambda, HTTP Backend
				3)WEBSOCKET API : works with Lambda, HTTP Backend, AWS services
			- Resource is a logical entity represented by an Access Path (i.e. URL Path)
			- Each Resource have multiple HTTP call methods (like GET, HEAD, PUT, POST, PATCH, DELETE, OPTIONS)
			- An API Gateway API is a collection of Resources and HTTP Call Methods that integrate with a Backend 
			- Backend could be HTTP endpoints (EC2, ECS, Public Endpoint), Lambda functions, or other HTTP-exposing AWS services
			- Backend can be integrated in the following fashion:
				*1=REST, 2=HTTP, 3=WEBSOCKET
				MOCK (1)			Return response without sending request to the backend. Integration request, Integration response not setup
				AWS	(1)				Expose AWS service action. Integration request, Integration response and corresponding data mappings required
				HTTP (1)			Expose HTTP backend. Integration request, Integration response and corresponding data mappings required
				AWS_PROXY (1,2)		Proxy (pass-through to) Lambda function. Integration request, Integration response not setup
				HTTP_PROXY (1,2)	Proxy (pass-through to) HTTP backend. Integration request, Integration response not setup
			- REST API supports all integration types / HTTP API supports proxy integration types only
			- Client SDKs can be generated - JS, Ruby, Java, iOS Swift, iOS Objective-C, Android
			- API Gateway can expose APIs 
				* directly to client OR 
				* via CloudFront distribution when the clients are globally distributed
			- API keys can be distributed to developers
			- Use AWS Sig-v4 to authorize access to APIs
			- Supports scaling, caching and throttling
		Encrypted Sessions:
			*** Amazon API Gateway DOES NOT support unencrypted sessions between Client and API Gateway
				Amazon API service automatically assigns an internal domain to the API that uses Gateway certificate
				Amazon API Gateway supports hosting SSL certificate for your custom domains
			*** Amazon API Gateway can use a client-side SSL certificate to authenticate itself to the backend
				For backend to be able to verify the certificate provided by API Gateway, it needs to have access to the Public Key
		CORS:
			- Cross Origin Resource Sharing
			- When an API resource receives a request from a domain other than API's own domain, CORS must be enabled for the selected methods on the resource
			- When an API resource receives a request from a domain other than API's own domain AND CORS is not enabled, the request will be BLOCKED and the requestor receives an "Cross-Origin Request Blocked" response
			- CORS is enabled on the specific HTTP methods or calls made on the resource
		REST v/s HTTP
			Caching, API Keys, Usage Plans, Request Transformation, Response Transformation are supported by REST APIs only
			IAM, Client Certificates, AWS WAF, Resource Policies are supported by REST APIs only
			Edge-optimized, Private endpoints are supported by REST APIs only
			Cloud Watch logs, Execution logs, Cloud WatchMetrics, X-Ray are supported by REST APIs
			REST APIs 71% more expensive than HTTP APIs
			OIDC and OAuth 2 authorization not supported by REST APIs, but supported by HTTP APIs
		Typical data formats include JSON, XML, query string parameters, and request headers
		Usage Plan:
			request quota, throttle, API keys
		API lockdown:
			You can apply a Resource Policy to an API to restrict access to a specific Amazon VPC or VPC endpoint. You can also give an Amazon VPC or VPC endpoint from a different account access to the Private API using a Resource Policy.
		Backend lockdown:
			- You can validate that API gateway is calling the backend by having the API gateway use a client-side SSL certificate, whil you have the public key to validate the cert
		Access & Authorization:
			Access to an API can be controlled using the following ways:
				- IAM Roles and Policies: 
					-IAM User, IAM Group, IAM Role, Account, Cross-A/C can access all methods or specific methods of API
					- To control who can create, deploy and manage API, the 'API Developer' permissions are to be granted
					- To control who can HTTP call a deployed API, the 'API Caller' permissions are to granted
				- IAM Resource Policies: 
					- A typical IAM resource policy that allows/denies access to a Principal, IAM User, IAM Role, Service Role, Source IP addr range, Source IP CIDR blocks, VPCs, VPC endpoints
				- CORS
					- CORS is granular and sharing is to be enabled on specific HTTP methods of the API
				- Lambda Authorizers
					- Also known as custom authorizers
					- This is a Lambda function that gets invoked synchronously that allows or denies execution of the API call
					- TOKEN TYPE: Lambda authorizer can validate the OAuth Bearer Token sent by the client in the HTTP request header
						OR
					- REQUEST TYPE: Lambda authorizer can validate the URL Path, Query String, Headers, Stage variables or Context parameters
						Or
					- COGNITO_USER_POOLS: Lambda authorizer 
					- You can use a Lambda function from a different AWS account as your Lambda Authorizer
				- Amazon Cognito User Pool
					- API gateway can use client-side SSL certificates to authenticate itself with the backend
					- The backend must have access to the public key to validate the certificate being submitted 
				- Backend Lockdown
				- API keys & Usage Plans
			- AWS Sigv4 (i.e. use AWS credentials - access and secret keys - to sign requests to your service. The signing of an Amazon API Gateway request is managed by the custom API Gateway SDK generated for your service)
			- Lambda Authorizer is available for the REST APIs
		Caching:
			- API can cache results in the Gateway cache
			- Responses cached carry TTL
		Throttling:
			- Throttling can be configured for each HTTP Method underneath a resources
			- Throttling rates for standard and BURST rates can be set
			- Generates HTTP response 429 Too Many requests when number of requests exceed thresholds
			- Client SDKs retry calls automatically when met with this response
		WebSocket APIs:
			Connection between client and the integrated backend is persistent
			When a new client is connected to the WebSocket API, a unique URL, called the callback URL, is created for that client
			The backend service can use this callback URL to send messages to the client from the backend service.
			The backend service can use the callback URL GET method on the connection to identify if the client is connected to the WebSocket connection
			AWS Lambda authorizers can be used to authorize the request at the API gateway
			Billing factors:
				Connection Minutes AND Messages	
		Monitoring:
			- API gateway sends metrics to CloudWatch every 60-seconds
			- Examples of metrics; # of calls, cache hit, cache miss, latency, backend integration latency

	===================================================
	AWS Server Migration Service:
	===================================================
		Scenarios:
			On-premise VMWare to AMI
			On-premise Hyper-V to AMI
			Azure VMs to AMI
		Server Migration Service is a FreeBSD VM installed in your On-premise virtualized environment
			Connector available for VMWare
			Connector available for Hyper-V
			Connector available for Azure
		An already migrated server may change. SMS tracks these incremental changes and migrates to the cloud
		An application may be composed of multiple servers. Service can orchestrate migration of multiple servers	
		SNS notification when job has completed, failed or been deleted
		SMS requires permissions to connect to AWS EC2 service
		Limits:
			50 concurrent server migrations per account
			90 days of SMS usage per VM
			50 concurrent applications per account (1 app = 50 servers)
			22 volumes per migrated server
			Migrated VM gets one ENI even if source had multiple NICs attached
			Migrated VM receives private IP
			Migrated VM does not receive a public IP even if hosted in a public subnet 
				Assign an Elastic IP
			IPv6 addresses not supported
			AMI can be encrypted, but encrypted volumes are not supported 
		

	===================================================
	KMS/Encryption:
	===================================================
		AWS KMS: Global HSM service but the keys are REGIONAL / 99.999999999% durability / Supports multiple-AZ in region / Import of keys is supported, but export of keys is not supported
		AWS KMS CMK: CMK=Customer Master Key
			CMK could be a 256-bit Symmetric or an Asymmetric Public-Private Pair
			CMK could be AWS owned CMK (rotation varies) or AWS managed CMK (required 3-year rotation) or Customer managed CMK (optional 1-year rotation)
			AWS managed CMK: keyes managed by AWS Services on your behalf / Only the service that created the key can use it
			Customer managed CMK / Customer create-disable-deletes-rotates-keypolicies-IAMpolicies keyes / AWS Services can also use Customer mamaged CMKs
			CMK rotation rotates the material within. Old key materials are saved for future purposes until the whole CMK is deleted
			CMK can encrypt 4096bytes of data. Therefore, the general strategy is to use Envelope Encryption i.e. CMK encrypts a Data Key and these Data Keyes are much longer in size
			You can request KMS for Data Key in a plaintext format or encrypted format || But remember CMKs never leave the KMS-HMS in an unencrypted format
		DynamoDB: 
			Uses both  key-hierarchy and CMK i.e. Envelope encryption / CMK could be AWS owned or AWS managed or Customer managed
			Key hierarchy: Dynamo generates a Table Key from the CMK. This tables key encrypts the data keys that are used to encrypt structures within a single table. If the parent CMK is changed, the table key is changed and the data keyes are re-encrypted
			DynamoDB saves the encrypted Table Key and Data Keyes along with the tables. Decrypted Table Key is cached for 5-minutes for performance; if not used in 5-minutes, it approaches KMS to decrypt the Table Key
			DynamoDB encrypts the tables-LSI-GSI-streams-backups-global tables, stores encrypted data key with the volume / DynamoDB requests for data key decryption and does en/de-cryption during RW
		EBS: Uses both data key and CMK i.e. Envelope encryption / CMK could be AWS managed or Customer managed / During volume attachment: EC2 to which the volume is attached, requests the decrypted data key from KMS, caches the key in the Hypervisor memory, encrypts the volume and stores the encrypted data key along with the volume / EC2 instance is responsible for encryption/decryption during I/O
		Secrets Manager: Uses both data key and CMK i.e. Envelope encryption / CMK could be AWS managed or Customer managed 
		S3:
			CSE: Client side encryption i.e. Envelope encryption / Key could be a CMK+S3 Enc Client or a key stored locally in app / S3 has no role in encyption or decryption of data
			SSE-S3: Server side encryption using AWS managed CMK and AWS managed data key / Each object has its own AES-256 data key / CMK is regularly rotated and old key material revisioned / S3 clears data key from memory as soon as enc or dec is done for the object
			SSE-KMS (or SSE-CMK): Server side encryption using Customer managed CMK and AWS managed data key
			SSE-C (Customer Provided Key): Client generates and provides a CMK & data key to S3 to do AES-256 encrypt before storing. Client provides the same keyes to S3 for AES-256 decryption during retrieval. S3 clears the data key from memory as soon as enc or dec is done for the object

	===================================================
	Digital Certificates:
	===================================================
		- X.509 digital certificates issued by CA for Key Encipherment, Digital Signing, CRL Signing, CRL Revocation
		- Certificates can be issued to :
			* Personal		: Used by Individuals for digital signing of documents
			* Organizations	: Used by organizations for digital signing of documents, conduct electronic transactions, and secure emails
			* Server		: Used by servers to prove domain ownership, and establish SSL/TLS encrypted session with a client
			* Client		: Used by client software to prove identity and establish SSL/TLS encrypted session with a server
			* Developer		: Used by developer to prove code authorship and to retain integrity of the distributed software
		- Certificate Authority:
			* the requestor signs the request for certificate with it's private key
			* the requestor submits the request + Public key to the CA
			* the CA accepts request for certificate from the above entities
			* the CA verifies that the requests are legitimate by doing background checks, validating that the requestor + his claims are valid
			* the CA uses the requestor's public key to validate the signature on the CSR
			* once satisfied, the CA generates the certificate body by embedding all the relevant information that requestor had provided, along with requestor's public key, SAN, version, validity, issuing CA information and the usage of the public key (etc)
			* the CA generates a hash of this core body and signs the hash with it's own private key
			* the CA structures this core body and the signed hash in the X.509 format
			* the CA issues the X.509 certificates
			* the CA maintains details and status of issued certificates
			* CAs are trusted widely, so browsers use the public key of the CAs
			* Upon receipt of a certificate the browser splits the certificate into two parts : 
				- the core body of the certificate
				- the signed digital hash
			* the browser decrypts the signed hash using the Public key of the CA and then compares this decypted hash with the hash of the core body of the certificate
			* if the hashes match, then the certificate is OK
		- The CA may also employ intermediate CAs to sign the certificate in which case the entire CA certificate chain is required for trusted validation by the browser
		
		
	===================================================
	Route53:
	===================================================
		- Features: 
			- Domain registration
			- Domain name translation 
			- Route 53 Recursive DNS:
				Explanation: When Route 53 receives a query for DNS resolution, it may either be configured to automatically forward the query directly to a specific recursive DNS server, or it may recursively search beginning with the root of the domain and continuing until it finds the final answer. In either case, once an answer is found, the recursive DNS server may cache the answer for a period of time so it can answer subsequent queries. 
			- Health-check web services to route to healthy endpoints
			- Can route users to non-AWS infrastructure
			- Globally Available Service
			- Allows you to list multiple IP addresses for an A record and responds to DNS requests with the list of all configured IP addresses.
			- Propagates updates to authoritative DNS records to its world-wide network of authoritative DNS servers within 60 seconds
			- A change is successfully propagated world-wide when the API call returns an INSYNC status listing
			- DOES NOT support DNSSEC for DNS resolution at this time
			- SUPPORTS DNSSEC on domain registration
		- Record types:
			A (address record / maps to IPv4 address)
			AAAA (IPv6 address record / maps to IPv6 address)
			CNAME (canonical name record / maps a domain name to a hostname / it cannot map a Zone Apex or Domain Root to a hostname)
			CAA (certification authority authorization)
			MX (mail exchange record / defines where mail for a paticular domain should be delivered; maps to mail server hostnames)
			NS (name server record / identifies which name server is authoritative for domain name, apex of the domain or hosted zones within)
			Alias: 
				- Route 53-specific extension to DNS to map names to AWS resources like CLB/ALB/NLB, CloudFront distributions, Elastic Beanstalk environments, API Gateways, VPC interface endpoints, and Amazon S3 buckets that are configured as websites
				- Using an alias record, you can map your record name (example.com) to the DNS name for a resource(elb1234.elb.amazonaws.com)
				- During run time name translation, the Alias record behaves like A or AAAA, and returns the IP address of the AWS resource
				- This is known as INTRA AWS DNS QUERIES in the usage reports
			SOA (start of authority record)
			SPF (sender policy framework)
			NAPTR (name authority pointer record)
			SRV (service locator)
			TXT (text record)
			PTR (pointer record - supports IPv4 and IPv6)
		- Main difference between ALIAS and CNAME:
			- Similarity:
				- Both map domain names to hostnames
			- Difference:
				- CNAME record CANNOT translate Domain Root to a hostname [eg: rrdnair.com cannot mapped using a CNAME record]
				- ALIAS record CAN map Domain Root to a hostname [eg: rrdnair.com CAN be mapped using an ALIAS record]
				- Also CNAME is a standard record type visible in a response from a name server, while ALIAS is an Route53 extension to DNS visible only in the Route53 console
				- CNAME record is followed by recursive resolution publicly / ALIAS record is recursively resolved within Route 53
		- DNS query to a name server:
			- The closest Route 53 DNS server(s) can be reached by the client making the DNS query
			- This is accomplished via anycast (i.e. 1 IP address hosted by multi Route 53 servers)
			- Propagates updates to authoritative DNS records to its world-wide network of authoritative DNS servers within 60 seconds
		- Hosted zone:
			- A hosted zone is an Amazon Route 53 concept 
			- A hosted zone is analogous to a traditional DNS zone file
			- A hosted zone represents a collection of resource records that can be managed together, belonging to a single parent domain name 
			- All resource record sets within a hosted zone must have the hosted zone’s domain name as a suffix. 
				For example, the amazon.com hosted zone may contain records named www.amazon.com, and www.aws.amazon.com, but not a record named www.amazon.ca. The zone apex amazon.com can also be mapped to a AWS resource using an ALIAS record
			- A wildcard entry is a record in a DNS zone that will match requests for any domain name based on the configuration you set. 
				For example, a wildcard DNS record such as *.example.com will match queries for www.example.com and subdomain.example.com.
			- Limit of 500 hosted zones and 10,000 resource record sets per hosted zone
		Multiple hosted zones for the same domain:
			- Creating multiple hosted zones for the same domain allows you to verify your DNS setting in a “test” environment, and then replicate those settings on a “production” hosted zone. 
				For example, hosted zone Z1234 might be your test version of example.com, hosted on name servers ns-1, ns-2, ns-3, and ns-4. 
				Similarly, hosted zone Z5678 might be your production version of example.com, hosted on ns-5, ns-6, ns-7, and ns-8. 
				Route 53 will answer DNS queries for example.com differently depending on which name server you send the DNS query to
		Out-Of-Box DNS RESPONSE POLICIES:
			* Simple routing:
				The resource record maps a domain name to a single resource
				Example: test.rrdnair.com maps to alb3877347746.us-east-1.alb.amazonaws.com
			* Failover routing:
				An ACTIVE-STANDBY failover scenario where R53 continuously checks health status of a endpoint corresponding to the PRIMARY resource record and failsover to STANDBY resource record, if the PRIMARY becomes UNHEALTHY
			* Geolocation:
				Collection of multiple record sets configured based on the geographic locations. There is one default catch-all GLOBAL. The record returned depends on the geo-location of the IP address from where the DNS query originated. If geo-location could not be mapped, the catch-all GLOBAL mapping is returned. Geography could be CONTINENT, COUNTRY, states within USA or catch-all GLOBAL
			* Latency:
				Collection of multiple record sets corresponding to global distribution. The record returned is the one that provides the lowest latency to the end-user thus focusing on the user experience. Latency metrics are refreshed continuously by R53, therefore the hostname resolved may be different at different points in time
			* Weighted:
				Collection of multiple record sets wherein each record is given a weightage. The higher the weightage, the DNS resolves that record proportionely more. Weight assigned varies from 1 to 255. Weight of 0 implies STOP sending traffic. Great for A/B testing, etc
			* Multi-Value answer:
				Route 53 responds to a DNS query with up to eight healthy records. And the client selects one out of the returned records to make the connection.
		NESTED ROUTING POLICIES:
			* Example-1: 	
						You could do LATENCY routing for api.rrdnair.com on two ALIAS records 
						The ALIAS record#1 points to usa.api.rrdnair.com
						The ALIAS record#2 points to asia.api.rrdnair.com
						Now usa.api.rrdnair.com in turn could do WEIGHTED routing on 192.12.0.0 and 192.12.0.1
						And asia.api.rrdnair.com in turn could do WEIGHTED routing on 90.23.210.0 and 90.23.210.1
						Remember NEVER FORGET TO CONFIGURE HEALTH CHECK at the LATENCY level and WEIGHTED level
			* Example-2: 
						Suppose you are doing WEIGHTED routing policy on 30%-192.0.2.10, 20%-192.0.2.11, 50%-192.0.2.12
						And say HEALTH check has been established for the first two i.e. 192.0.2.10, 192.0.2.11
						And no health check has been established for the third i.e. 192.0.2.12
						And suppose now the third node goes down i.e. the one that bore 50% of the load
						Since health check is not configured, Route53 assumes node#3 is healthy and continues to resolve 50% to node#3
		Private DNS:
			- Authoritative DNS for within your VPCs across regions without exposing DNS records to the Internet
			- Private DNS uses VPC to manage visibility and provide DNS resolution for private DNS hosted zones
			- To take advantage of Route 53 Private DNS, you must configure a VPC and migrate your resources into it
			- You can associate VPCs belonging to different accounts with a single hosted zone
		Health Check:
			* Route 53 has globally distributed health checkers (US East, US West, EU, APAC, South America) that periodically pings given target
			* A Target host gets monitored by multiple health checkers. 
				If >18% of the Health Checkers say the endpoint is HEALTHY, then the endpoint is HEALTHY
				If =<18% of the Health Checkers say the endpoint is UNHEALTHY, then the endpoint is UNHEALTHY
			- Types of Health Checks with Route 53:
			
					* HTTP and HTTPS health checks – Route 53 must be able to establish a TCP connection with the endpoint within four seconds. In addition, the endpoint must respond with an HTTP status code of 2xx or 3xx within two seconds after connecting
					
					* TCP health checks – Route 53 must be able to establish a TCP connection with the endpoint within ten seconds
					
					* Other Health Checks - A health check can monitor the status of other health checks; this type of health check is known as a calculated health check. The health check that does the monitoring is the parent health check, and the health checks that are monitored are child health checks. One parent health check can monitor the health of up to 255 child health checks
						- Route 53 adds up the number of child health checks that are considered to be healthy.
						- Route 53 compares that number with the number of child health checks that must be healthy for the status of the parent health check to be considered healthy.
					
					* CloudWatch Alarms - When you create a health check that is based on a CloudWatch alarm, Route 53 monitors the data stream for the corresponding alarm instead of monitoring the alarm state. If the data stream indicates that the state of the alarm is OK, the health check is considered healthy. If the data stream indicates that the state is Alarm, the health check is considered unhealthy. If the data stream doesn't provide enough information to determine the state of the alarm, the health check status depends on the setting for Health check status: healthy, unhealthy, or last known status
			
			* If health check is NOT CONFIGURED on an endpoint, then that endpoint is assumed to be HEALTHY regardless of the actual status
			* Health Check can be configured on AWS resources as well as On-premise resources
			* For ALIAS records, you just need to check "Evaulate Target Health" checkbox
			* Default: when an endpoint has failed three consecutive observations, Route 53 will consider it failed 
				Overridable between 1 & 10 / 30 seconds interval
			* Amazon Route 53 health checks include an optional latency measurement feature which provides data on how long it takes your endpoint to respond to a request. When you enable the latency measurement feature, the Amazon Route 53 health check will generate additional Amazon CloudWatch metrics showing the time required for Amazon Route 53’s health checkers to establish a connection and to begin receiving data
		Domain registration:
			Supports gTLD (.com, .net) and country-specific TLDs (.fre, .de, etc)
			Each domain associated with four unique Route 53 name servers, known as a delegation set.
			You can also use the Route 53 API to create a “reusable delegation set”, which you can then apply to multiple hosted zones that you create
			And then you can instruct your domain name registrar to use the same delegation set for all your domains managed by Route 53
		Route 53 RESOLVERS:
			DNS DHCP Service:
				- Route 53 provides DNS to the AWS environment
				- On-premise DNS infra can also be used to provide DNS names to the AWS environment
				- Route 53 cannot provide DNS to the On-premise environment
			DNS Query Resolution:
				- Although Route 53 cannot provide DNS DHCP to on-premise infrastructure, it can still respond to DNS queries for on-premise infra
				- This is achieved by Route 53 Resolvers
				- Resolvers are REGIONAL service and not a Global service like Route 53
				- Route 53 Resolvers helps resolve DNS QUERIES *FOR* resources in 
					** VPC 
					** On-premise infra connected via DX Link
					** On-premise infra connected via S2S VPN
					** Internet
				- Route 53 Resolvers helps resolve DNS QUERIES *FROM* resources in 
					** VPC (Outbound queries)
					** On-premise infra connected via DX Link (Inbound queries)
					** On-premise infra connected via S2S VPN (Inbound queries)
					** Internet
				- Inbound DNS query:
					- When query for DNS resolution emanates from a resource in On-premise or another VPC and gets forwarded to your VPC 
						~ your On-prem Linux server trying to access EC2 instance
					- This forwarding is enabled by an Inbound Endpoint
						- The Inbound Endpoint is essentially an ENI configured within a subnet in your VPC (AWS recommends two ENIs for HA)
					- All inbound DNS queries from your On-premise pass through this VPC on the way to Resolver
					- NOTE: You need to ensure that DNS resolvers in your On-premise environment forwards DNS queries for domains in the AWS to the Inbound endpoint's IP address
				- Outbound DNS query:
					- When query for DNS resolution emanates from your VPC and gets forwarded to On-premise or another VPC 
						~ an EC2 instance trying to access cnn.com
						~ an EC2 instance trying to access On-premise Windows server
					- This is enabled by an Outbound Endpoint
						- The Outbound Endpoint is essentially an ENI configured within a subnet in your VPC (AWS recommends two ENIs for HA)
					- The Outbound Rule is either FORWARD or SYSTEM
						- FORWARD: means forward through the Outbound Endpoint to the IP address of the On-premise DNS resolvers
						- SYSTEM: means Route 53 Resolver will respond to DNS queries			
					- NOTE: You need to ensure that DNS resolvers in AWS forwards DNS queries for On-premise environment to the Outbound endpoint's IP address
							
				- Conditional forwarding rules allow Resolver to forward queries for specified domains to the target IP address of your choice, typically an on-premises DNS resolver. Rules are applied at the VPC level and can be managed from one account and shared across multiple accounts.
				A DNS endpoint includes one or more elastic network interfaces (ENI) that attach to your Virtual Private Cloud (VPC). 
				Each ENI is assigned an IP address from the subnet space of the VPC where it is located. 
				This IP address can then serve as a forwarding target for on-premises DNS servers to forward queries. 
				Endpoints are required both for DNS query traffic that you're forwarding from VPCs to your network and from your network to your VPCs over AWS Direct Connect and Managed VPN.

	===================================================
	CLOUDFRONT:
	===================================================
		- Features:
			- GLOBAL service providing near-user access to content
			- Requests for your content are automatically routed to the edge location that gives your users the lowest latency
			- Supports INGRESS traffic in addition to EGRESS traffic
			- CDN is essentially a Global network of Edge Locations
				* Edge location is essentially global network of data center that sits between the user and content origin servers
				* Edge cache sits between the edge location and the content origin servers
			- The ORIGIN could be 
				* AWS ORIGIN: Objects hosted in S3
				* CUSTOM ORIGIN: Content served by AWS resource (like EC2 hosting HTTP server, website hosted in S3 bucket) or from On-premise
		- Features:	
			- Great for caching frequently accessed static content like images, audio, videos, media streaming or software downloads
			- The maximum size of a single file that can be delivered through Amazon CloudFront is 20 GB
			- Supports GET, HEAD, OPTIONS, POST, PUT, PATCH, DELETE (same as API Gateway)
			- Caches GET, HEAD, OPTIONS only
			- DOES NOT cache POST, PUT, PATCH, DELETE
			- Cloudfront support Websockets
				* CloudFront establishes WebSocket connections only when the client includes the 'Upgrade: websocket' header in HTTP request and the server responds with the HTTP status code 101 confirming that it can switch to the WebSocket protocol
			- You can configure CloudFront to return custom error messages for 4xx, 5xx to the viewer
			- Time-To-Live: Controlled by cache control header. If none mentioned, CDN caches for 24 hours
			- Geo Restriction feature lets you specify a list of countries in which your users can access your content OR not
			- Supports backup ORIGINs for redundancy
		- 2 HOPS:
			- User<==>Edge Location<==>Content Origin Server
							||
					  Regional Edge Cache
		- Endpoints:
			- exposed as DNS name
			- exposed as IPv4 address
			- exposed as IPv6 address
		- Route53->CloudFront:
			- Example domain generated by CloudFront is 
				d74aade74rt48362.cloudfront.net
			- Custom Domains also known as Alternate Domains can also frontend CloudFront domain names
				- Route53 ALIAS (A) record pointing to CloudFront endpoint
				- Route53 ALIAS (AAAA) record pointing to CloudFront endpoint
				- Route53 CNAME record can be used AS LONG AS zone apex / domain root is not being mapped
		- Distribution Types:
			- Web also known as Progressive Download distribution
			- Streaming also known as RTMP or Adobe Real-Time Messaging Protocol
		- WEB Distribution:
			- Also known as Download Distribution or Progressive Download Distribution
			- Content like text, audio, video distributed over HTTP, HTTPS can be served
			- Supports static content like .html, .css, .jpg etc 
			- CDN service => ORIGIN over HTTP/1.1
			- Supports multimedia content on demand using progressive and Apple HTTP Live Streaming (Apple HLS)
			- DOES NOT SUPPORT Adobe Flash Multimedia content (for Adobe use the RTMP distribution)
			- Origin can be S3 bucket, Custom HTTP endpoint and Live events/Apple HLS
			- Static website in an S3 bucket is also supported
			- If you do not want to restrict access to S3 objects using CFN signed URLs, then make the objects public readable
			- Web distribution can forward Cookies to the origin
			- Web distribution can forward query strings to the origin
			- Web distribution can cache based on Cookies 
			- Web distribution can cache based on query strings
			- Web distribution can blacklist countries 
			- Web distribution can whitelist countries
			- Viewer Protocol Policy:
					- Policies that control the User to Edge Location Traffic is the Viewer Control Policy
					- Supported HTTP versions: HTTP/2 (Default), HTTP/1.1, HTTP/1.0
					- Supports "HTTP and HTTPS"
					- Supports "Redirect HTTP to HTTPS" [CloudFront responds with HTTP 301 - Moved Permanently  response header with the new HTTPS url]
					- Supports "HTTPS Only"
					- For HTTPS scenarios above, CloudFront terminates the connection and hence needs to have SSL certificate. If the CloudFront provided DNS name is being used, then AWS provides the SSL certificate. If custom domain name is used, the admin needs to provide the SSL certificate
			- ORIGIN Protocol Policy:
					- Policies that control the Edge Location to the ORIGIN Traffic is the Origin Control Policy
					- Supports "HTTP" [NOTE: Static S3 website does not support HTTPS and Match Viewer]
					- Supports "HTTPS" 
					- Supports "Match Viewer" [ Match whatever was used in the Viewer Protocol Policy]
					- For HTTPS scenarios above, ORIGIN terminates the connection and hence needs to have SSL certificate. And the SSL certificate on the ORIGIN server should a Subject Alternate Name matching the Domain Name in the SSL Certificate hosted by CloudFront. If the SAN names do not match, then HTTP 502 (Bad Gateway) is generated as response
			- How does the edge location identify the SSL/TLS certificate when it serves so many customers?
				- DESIGN TIME - CloudFront associates each CloudFront distribution with a unique IP address. Therefore, at a given edge location you will have multiple IP addresses uniquely mapped to varios distributions 
				- RUN TIME - Route 53 returns the IP address of Distribution in the nearest Edge location. Browser obviously has the IP in request which enables Distribution to serve correct SSL cert
				- Alternatively, the Browser can also send the SNI to help the Distribution identify the SSL certificate
		- RTMP Distribution:
			- Also known as RTMP or Adobe Real-Time Messaging Protocol
			- Stream media files using Adobe Media Server and Adobe Real-time Messaging Protocol (this is not HTTP protocol)
			- The client needs to support Adobe Flash
			- HTML5 only clients do not support RTMP
			- Origin can be S3 bucket Only
			- Static website in an S3 bucket is NOT supported (remember this is not serving on HTTP protocol)
			- If you do not want to restrict access to S3 objects using CFN signed URLs, then make the objects public readable
		- SECURITY:
			- Content Protection
				- Restrict access to objects in the CloudFront edge caches using CloudFront Signed URL
					OR
				- Restrict access to objects in the CloudFront edge caches using CloudFront Signed Cookie
					AND
				- Restrict public access to objects in the ORIGIN
					AND
				- Provide Origin Access Identity access to the ORIGIN
			- Who generates Signed URL or Signed Cookie?
				- It is the backend application that generates the Signed URL or the Signed Cookie once the user has successfully authenticated
				- An ending date-time after which the URL is no longer valid
				- OPTIONAL A starting date-time after which the URL becomes valid
				- OPTIONAL A range of IP allowed addresses 
			- Field-level encryption
				- Scenario: Browser POSTS sensitive data to CloudFront over SSL. Sensitive data like cc number, PII info. You could further encrypt these senisitve data before CloudFront forwards it further or caches it. CloudFront encrypts using a Public Key saved in KMS. Only those components in the backend that have access to Private Key decrypt the sensitive. For example, the payment microservices has access to the Private Key to decrypt the credit card number, while the Order microservices does not have access to the Private Key. 
			- AWS Certificate Manager
			- DDOS protection using AWS Shield
		- Caching:
			- CloudFront can add custom HTTP headers before forwarding to ORIGIN
			- CloudFront can override the value of existing headers before forwarding to ORIGIN
			- CloudFront can forward some or all of your cookies to your custom origin server. 
			- Cookies can be part of cache key such that request is fulfilled by the cache itself
			- Query parameters (up to 10) can be part of the cache key such that dynamic web pages (e.g. search results) may be cached at the edge for some amount of time
				- In case of highly dynamic content, the best approach is to set TTL to 0 (the caching yields zero benefits) and forward query string to the ORIGIN, so the correct results can be fetched
			- CloudFront can gzip the response as long as the client had added "Accept-Encoding: gzip in the request header 
			- Time-To-Live in the cache:
				- If Time-To-Live expires on an object in the cache, CloudFront forwards the next request for that object to the ORIGIN with a request for validation using IF-MODIFIED-SINCE header. Now, if the object has not changed, the ORIGIN return HTTP 304. However, If the object has changed, the ORIGIN returns HTTP 200 with the new version
				- Minimum is 0			: STILL caches the object associated to that cache key and uses the IF-MODIFIED-SINCE header to fetch net new content. If headers have to be forwarded to the ORIGIN, it behaves like 0 TTL
				- Default is 24 Hours 	: overridden by ORIGIN Cache-Control response headers like max-age, s-maxage, Expires
				- Maximum is One year
				- Cache Control headers can be used in conjunction with CloudFront TTL such that CloudFront TTL can be the base setting for all objects while Cache-Control headers fine-tunes the expiration of subset of objects
		- What is the use of TTL=0?
				** Variable Time-To-Live (TTL) - In many cases, dynamic content is either not cacheable or cacheable for a very short period of time, perhaps just a few seconds. In the past, CloudFront's minimum TTL was 60 minutes since all content was considered static. The new minimum TTL value is 0 seconds and it scraps that earlier assumotion of everything being static. 
				** If you set the TTL for a particular origin to 0, "CloudFront will still cache the content from that origin". It will then make a GET request with an If-Modified-Since header, thereby giving the origin a chance to signal that CloudFront can continue to use the cached content if it hasn't changed at the origin.
				** In other words, using a TTL of 0 primarily means, that CloudFront delegates the authority for cache control to the origin, i.e. the origin server decides whether or not, and if for how long CloudFront caches the objects; please note specifically, that a GET request with an If-Modified-Since header doesn't necessarily mean that the object itself is retrieved from the origin, rather the origin can (and should) return the HTTP status code 304 - Not Modified where applicable
				** HTTP 304 indicates the resource has not been modified since last requested. Using this saves bandwidth and reprocessing on both the server and client, as only the header data must be sent and received in comparison to the entirety of the page being re-processed by the server, then sent again using more bandwidth of the server and client.
		- Streaming:
			- Audio and Video streaming supported
			The protocols used for streaming include those that use HTTP for delivery such as 
				Apple’s HTTP Live Streaming (HLS)
				MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH)
				Adobe’s HTTP Dynamic Streaming (HDS) and 
				Microsoft’s Smooth Streaming
			Alternatively, you can also run a third party streaming server (e.g. Wowza Media Server available on AWS Marketplace) on Amazon EC2, which can convert a media file to the required HTTP streaming format. 
				This server can then be designated as the origin for an Amazon CloudFront web distribution.
		- Lambda@Edge:
			- Trigger Lambda function corresponding to four different events
			- Viewer Request - This event occurs when an end user or a device on the Internet makes an HTTP(S) request to CloudFront, and the request arrives at the edge location closest to that user.
			- Viewer Response - This event occurs when the CloudFront server at the edge is ready to respond to the end user or the device that made the request.
			- Origin Request - This event occurs when the CloudFront edge server does not already have the requested object in its cache, and the viewer request is ready to be sent to your backend origin webserver (e.g. Amazon EC2, or Application Load Balancer, or Amazon S3).
			- Origin Response - This event occurs when the CloudFront server at the edge receives a response from your backend origin webserver
		- Remove from CloudFront:
			Remove content from origin. It will be removed from CDN upon expiration in the cache
			Invalidate the content from edge cache for immediate removal by issuing "aws cloudfront create-invalidation" command
				Limit - Upto 3000 objects per distribution can happen concurrently
			Use file versioning to serve a different version of the file 
		- Are there any reasons why I would want to disable IPv6?
			In discussions with customers, the only common case we heard about was internal IP address processing. 
			When you enable IPv6 for your Amazon CloudFront distribution, in addition to getting an IPv6 address in your detailed access logs, 
			you will get IPv6 addresses in the ‘X-Forwarded-For’ header that is sent to your origins. If your origin systems are only able to process IPv4 addresses, you may need to verify that your origin systems continue to work for IPv6 addresses before you turn on IPv6 for your distributions.

	===================================================
	AWS Kinesis Video Stream:
	===================================================
		- Fully managed AWS service
		- Millions of PRODUCERS like Mobile phones, Webcam, Drones, Security Cam, CamCorders, BodyCam, Radar, IR camera, Thermal cameras can SEND & SAVE Audio, Image, Video, Thermal image, Radar image to AWS Kinesis Video Stream
		- Kinesis Video Streams API library can be used by Consumers (applications) to connect to these video stream to live display, real-time process or batch-process the stream
		- Kinesis can Encrypt, Store and further Index the video streams based on timestamps
		- Kinesis Video Stream support Matroska Container format for FARGMENTS
		- Fragment:
			- A fragment is a self-contained SEQUENCE of frames
			- A frame in one fragment has no dependency on a frame in another fragment	
		- PutMedia API:
			- Producer sends a stream of media fragments using PutMedia API
			- PutMedia can upload real-time or in batches
		- GetMedia API:	
			- Consumers must know the starting fragment from where the read should initiate and the first read will return corresponding fragment
			- Fragments are returned in the order in which they were added
		- Playback using Apple HTTP Live Streaming (HLS)

	===================================================
	AWS Rekognition:
	===================================================
		- Labels: 
			objects  (flower, tree, table)
			events   (birthday party)
			concepts (evening, nature, landscape)
			Labels can be detected in image and videos
				- Activities can be detected only in videos
			API = DetectLabels(in image), StartLabelDetection (in video)
		- Faces
			Faces can be detected in image and videos
			API = DetectFace (in image), StartFaceDetection(in video), CreateStreamProcessor (for streaming video)
		- People Paths
			- Track the path of people detected in stored video
			- API = StartPersonTracking(in video)
		- Celebrities
			- API = RecognizeCelebrity(in image), StartCelebrityRecognition(in video)
		- Text	
			- API = DetectText(in image)
		- Unsafe Text
			- API = DetectModerationLabels(in image), StartContentModeration(in video)
		- Image Operations:
			- Are synchronous
			- Inputs in .jpg, .png file
			- Output in JSON format
		- 
		- Rekognition API can curate, verify and organize millions of images and videos
		- Rekognition API can make the images and videos searchable
		- Rekognition API can extract context and scene from the images and videos
		- Rekognition API can identify objects, events, concepts, text, people, scene, and activities from an image or video
		- Rekognition API can detect face, analyze sentiment and compare faces
		- If images and videos are in an S3 bucket, the bucket should be in the same region as the Rekognition endpoint
		- If images and videos are in Kinesis Video Stream, Rekognition API can read the stream, process & output back into Kinesis Vid Stream
		- Scenario: Amazon S3 bucket wher videos and images are uploaded which asynch

	===================================================
	S3:
	===================================================
		Scoped at a REGION level / 99.999999999% durability per year / Object size upto 5TB / Single PUT request is 5GB / If size > 100MB, use Multi-part Upload
		Storage classes based on Access pattern: Lifecycle rules can transition between storage classes
			S3-Standard (frequently accessed / Replicated in 3AZs / 99.99% avail)
			S3-Infrequently Accessed (99.9% avail / Infrequent access, rapid retrievals / Replicated in 3AZs / Data deleted within 30 days charged full 30 days / 128KB minimum object size)
			S3-OneZone Infrequently Accessed (99% avail / Infrequent access / Saved in OneAZ / Data deleted within 30 days charged full 30 days / 128KB minimum object size)
			S3-Glacier: Archive (99.99% avail / Replicated in 3 AZs / Data deleted within 90 days charged full 90 days / Expedited(1-5mins)-Standard(3-5hrs)-Bulk(5-12hrs) retrieval modes)
			S3-Glacier-DeepArchive: Long Term Cold Archive (99.99% avail / Data deleted within 180 days charged full 180 days / Standard(12hrs)-Bulk(48hrs) retrieval modes) / AWS Tape Gateway, Snowball, Snowmobile -> DeepArchive
			S3-Intelligent-Tiering: Starts off in S3-Standard tier and based on access patterns, moves the objects between other tiers
		Bucket can have a mix of objects belonging to S3 Standard, S3-Standard IA, S3 One Zone-IA classes
		Retrieval from Glacier and Glacier DeepArchive:
			Because Amazon S3 maintains the mapping between your user-defined object name and Amazon S3 Glacier’s system-defined identifier, 
			Amazon S3 objects that are stored using the S3 Glacier & DeepArchive storage class are only accessible through the Amazon S3 APIs or the Amazon S3 Management Console.
		Lifecycle transition policy:
			Applied at bucket or prefix level
			Automatically migrate objects stored in the S3 Standard to the S3 Standard-IA, S3 One Zone-IA, and/or S3 Glacier storage classes based on the age of the object 
			Automatically expire/remove objects based on the age of the object
			You can set a policy for multipart upload expiration, which expires incomplete multipart uploads based on the age of the upload.
		S3 Provisioned Capacity guarantees that your retrieval capacity for Expedited retrievals will be available when you need it. 
			Each unit of capacity ensures that at least 3 expedited retrievals can be performed every 5 minutes and
			Provides up to 150MB/s of retrieval throughput
			1PCU = $100/month
		Billing: Monthly billing / Average storage + Data Transfer costs + Mgmt costs(like tags) + #of requests - data retrieval / Tiered pricing (like <50TB, 50TB-450TB, >500TB)
		Security: IAM policies, Bucket policies, ACL, Signed URLs / Tracking in CloudTrail at bucket level / Look at encryption keyes above
		GDPR-EU Regions: Frankfurt, London, Paris, Ireland
		Amazon Macie: Discovery, Classification, Loss Prevention tool over S3
		IAM Access Analyzer for S3: $0 cost / Alerts when bucket that is configured to allow access to anyone on the internet or that is shared with other AWS accounts. 
			You receive insights or ‘findings’ into the source and level of public or shared access. 
			For example, Access Analyzer for S3 will proactively inform you if read or write access were unintendedly provided through an access control list (ACL) or bucket policy. 
		S3 Replication:
			Same Region (SRR) / Cross Region (CRR) / within same account / to a different AWS account+Ownership Overwrite
			Applied at bucket level, prefix level or object tags AND Can be used to replicate the object into a specific storage class in the destination
			KMS-encrypted objects can be replicated by providing a destination KMS key in your replication configuration
			S3 Replication Time Control is designed to replicate most objects in seconds, 99% of objects within 5 minutes, and 99.99% of objects within 15 minutes
		Versioning: 
			Supported with PUT,COPY,POST,DELETE operation / 
			Special protection for DELETE using MFA capability: requires AWS credentials + 6-digit code + Serial number from an authentication device in your possession
		Access point: (1000 per region / $0 cost)
			A bucket is the logical storage container for your objects while an access point provides access to the bucket and its contents. 
			An access point is a separate resource created for a bucket with ARN,an bucket access control policy, a network origin control & hostname (in the format of https://[access_point_name]-[account ID].s3-accesspoint.[region].amazonaws.com), 
			Network origin control helps lock down access from a specific VPC / Block Public Access can be set at access point level
			Access points have their own IAM policy that controls request authorization and IAM policies cane be attached to users, groups, roles
			An access point can be used by a user or users or application or applications. One bucket has multiple access points. A access point is attached to only 1 bucket.
			Instead of making a complex bucket policy for all users & apps, access is managed at the access point level instead of bucket level
		Query in place: S3 is the Data Lake
			querying options: S3 Select (simple SQL queries, Integrate with Lambda,Hive,Spark,Presto) / Amazon Athena(Presto based complex SQL queries) / Redshift Spectrum (query exabytes without ETL)
		S3 Transfer Acceleration: 
			Service accessible via Accelerate Endpoint or dual-stack (i.e. IPv+IPv6) endpoint
			Use CloudFront Edge Locations to route data from client to S3 bucket within optimized AWS network / No data residue at the Edge locations
			For objects<1GB, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance and skip S3 Transfer Accelerate
			Food for thought: Perform initial heavy lift moves with an AWS Snow family and then transfer incremental ongoing changes with S3 Transfer Acceleration
		S3 Dual stack endpoint:
			Different from the accelerate endpoint / Supports IPv4 and v6
		S3 Object Tags:
			Retained during cross-region replication and changes in the object storage class
		S3 Inventory report
			Not free / CSV, ORC, or Parquet file output of your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix AND to verify encryption and replication status of your objects
		S3 Batch:
			Feature to automate the execution, management, and auditing of a specific S3 request or Lambda function across many objects stored in Amazon S3. 
			You can use S3 Batch Operations to automate replacing tag sets on S3 objects, updating access control lists (ACL) for S3 objects, copying storage between buckets, initiating a restore from Glacier to S3, or performing custom operations with Lambda functions. 
			Start by selecting an S3 Inventory report or providing your own custom list of objects for S3 Batch Operations to act upon. 
			You choose from a set of S3 operations supported by S3 Batch Operations, such as replacing tag sets, changing ACLs, copying storage from one bucket to another, or initiating a restore from Glacier to S3. You can then customize your S3 Batch Operations jobs with specific parameters such as tag values, ACL grantees, and restoration duration. 
			To further customize your storage actions, you can write your own Lambda function and invoke that code through S3 Batch Operations 
		S3 Object Lock:
			Feature that blocks object version deletion during a customer-defined retention period(data protection or regulatory needs)
			Configure S3 Object Lock at the object- and bucket-levels regardless of the storage class and transition between storage classes due to lifecycle events
			Once a Retain Until Date has been assigned to an object, that object cannot be modified or deleted until the Retain Until Date has passed. 
			S3 Object Lock can be configured in one of two Modes. When deployed in Governance Mode, AWS accounts with specific IAM permissions are able to remove WORM protection from an object. If you require stronger immutability in order to comply with regulations, you can use Compliance Mode. In Compliance Mode, WORM protection cannot be removed by any user, including the root account.
			 you can make an object immutable by applying a Legal Hold to that object. A Legal Hold places indefinite S3 Object Lock protection on an object, which will remain until it is explicitly removed
		S3 as File/Block Storage:
			S3 can be presented as a File/Block storage to On-premise servers, On-premise Backup tools
			Storage Gateway is a virtual appliance that enables this interfacing with S3
			Storage gateway essentially presents the S3 environment over an iSCSI interface
			Storage gateway is downloadable from the AWS site as a VMDK and can be hosted on EC2, Hyper-V, VMWare ESXI, Hardware Appliance
			Storage Gateway can present three On-premise storage interfaces over S3:
				- File Gateway		NFS protocol / File stored as S3 as S3 objects / Cache files locally on the gateway / supports VF v3-v4.1 and CIFS
				- Volume Gateway	iSCSI block device interface / S3 acts as block store volumes / supports iSCSI
									- Gateway Cached Volumes: Gateway proxies data to block-store in S3, while frequently accessed data is cached in the gateway
									- Gateway Stored Volumes: Backup data is stored on On-premises storage in SAN while scheduled asynchronous offsite snapshots of the volume are backed-up in S3 block store
									Note: EBS volumes can be provisioned from the snapshot that was backedup in the 'Gateway Stored' mode
				- Tape Gateway		Existing VTL and VTS (Virtual Tape Shelf) software can use backup to S3 and Glacier respectively / supports iSCSI
									using iSCSI interface. (VTL->S3, VTS->Glacier)
									
		Cross-Region Bucket Replication:
		Backups and Region failure:
			Cross-Region Bucket Replication
			
			

	===================================================
	RDS:
	===================================================
			- Managed relational db service supports Amazon Aurora, MySQL, MariaDB, Oracle, SQL Server, and PostgreSQL database engines
					~ Managed services include scale compute, scale storage, Replication
					~ Provides native db access to users, User still manage app-specific db settings
					~ You can logon to schema, but cannot log into the OS
					~ RDS uses Stripped EBS storage
					~ AWS responsibility: automated backup daily, patching of db + OS, security fixes of db + OS, db update, db upgrade
					~ Your responsibility: Manage DB settings, Manage schema, Performance Tuning
			
			- Two license model:
					~ BYOL
					~ License Included	
			
			- DB instance:
					~ DB instance has unique identifier and database instance name
					~ DNS name for the DB instance = 
								<db-instance-name>.<db-instance-id>.<region>.rds.amazonaws.com
								supply-chain.123456789012.us-east-1.rds.amazonaws.com
								and 
								the supply chain instance could host multiple databases like inventory, sales, customer, marketing
					~ DB instance can contain multiple user-created databases
					~ DB instance = Environment =  specifies the db product + env settings + compute + storage + VPC in which to create the DB
					~ A database parameter group (DB Parameter Group) acts as a “container” for engine configuration values / Default is optimized for the engine
					~ 40 DB instances per account:
						BYOL - 40 could be Aurora, MySQL, Maria, PostgreSQL, Oracle-BYOL
						Out of 40, 10 could be Oracle-License Included or SQLServer-License Included
					~ A DB Subnet Group:
						A collection of subnets. One subnet IS IDENTIFIED in each AZ. 
						The subnet group is assigned to DB instance during creation.
						Amazon RDS then uses that DB Subnet Group and your preferred Availability Zone to select a subnet and an IP address within that subnet. 
						Amazon RDS creates and associates an Elastic Network Interface to your DB Instance with that IP address

			- LIMITS: Databases or schemas per DB instance:
					~ No limit imposed by software:
							~ RDS for Amazon Aurora
							~ RDS for MySQL
							~ RDS for MariaDB
							~ RDS for PostgreSQL
					~ 1 Database per DB instance:
							~ RDS for Oracle: 1 database per instance; no limit on number of schemas per database imposed by software
					~ 100 Databases per DB instance
							~ RDS for SQL Server
			
			- Supported storage types:
					~ General purpose
								MariaDB, MySQL, Oracle, and PostgreSQL database instances: 20 GiB–64 TiB
								SQL Server for Enterprise, Standard, Web, and Express editions: 20 GiB–16 TiB
								
					~ Provisioned IOPs
								MariaDB, MySQL, Oracle, and PostgreSQL database instances: 20 GiB–64 TiB
								SQL Server for Enterprise, Standard, Web, and Express editions: 20 GiB–16 TiB
					
					~ Magnetic (standard and cold are not supported)
			
			- Multi-AZ setup:
					~ MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology
					~ SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs)
					~ Primary db for Reads & Write AND a Hot Standby in same region-but-different AZ which is SYNCHRONOUSLY kept in sync with primary
					~ Primary fronted by a DB Instance DNS name and the RDS Service updates the CNAME record of the DNS name to point to the standby instance automatically during failover. Therefore, the DB client does not need to be updated with DB endpoint info when failover has happens
					~ But the DB client can cache the IP address. Therefore, AWS recommends a TTL value of no more than 60 seconds
					~ AWS recommends Provisioned IOPS for Multi-AZ setup
					~ Reasons for failover: 
						Primary DB - Compute failure, Storage failure, AZ failure, Network failure
						Primary DB is changed (like Parameter Changes, DDL updates)
						Primary DB instance is being patched
						Exception situation: Amazon RDS provides an option to initiate a MANUAL failover when rebooting your instance
					~ Failover is AUTOMATIC:
							~ It takes few minutes (60-120secs) for failover to happen
							~ large transactions or a lengthy recovery process can increase failover time
							~  it can take additional time for the RDS console to reflect the new Availability Zone

			- Read replicas:
					~ Upto 5 read replicas for a given DB instance in the same region or different region
					~ Read replicas are synced ASYNCHRONOUSLY, hence lags the primary database
					~ MySQLDB and Maria DB support logical replication 
					~ Postgresql, Oracle and MS SQL supports physical replication 
					~ Each replica has it's own DNS name, so user needs to track replicas and application will need to be updated with the new replicas
					~ Scaling Read Replicas:
							~ You can SCALE UP the compute or storage for the READ REPLICA when it is being created (Again you CANNOT SCALE DOWN)
					~ READ REPLICA CHAINING or Second-tier read replica:
							~ Read replicas can have their own Read Replicas
							~ Supported by MariaDB, Aurora, MySQL 
							~ Upto 4 in the chain starting with the source database
							~ Not yet supported by Oracle, SQL Server, Postgresql
					~ CIRCULAR Replication is not supported
					
					~ The READ REPLICA inherits the following configuration from it's source primary DB instance
							The Backup window 		+ 
							The Maintenance window 	+
							The db parameter groups
					~ Multi-AZ failover:
						Any associated and available read replicas will automatically connect to the NEW PRIMARY and resume replication once failover has completed 
					~ Multi-AZ setup for READ REPLICA:
						Amazon RDS for MySQL, MariaDB, PostgreSQL, Aurora and Oracle allow you to enable Multi-AZ configuration on read replicas 
					~ Promoting Read-Replica to a Standalone database:
							~ Promoting a read replica to a standalone database disconnects it from the replication process and makes it a standalone database
							~ When a read replica in a replication-chain becomes standalone database, all replicas that were replicating from it continue to replicate from it
								example: SourceDB->R1->R2->R3 and now if you promote R1 to be a standalone database, R2 continues to source the replication traffic from R1 (and NOT SourceDB)
					~ Deleting the primary database essentially disconnects all read replicas from the replication process and makes them standalone databases
			
			
			- Pricing:
					~ RDS cost components include 
						compute: charged only when the DB is up and running
						storage: per GiB per month
						backup storage: per GiB per month 
						I/Os: per 1 million requests per month
						provisioned IOPS: per IOPS per month
						Data transfer: per GB
					~ Purchasing options:
						~ On-Demand Instances	: pay by the hour
						~ Reserved Instances	: one-year or three-year commitment
					~ Billed for each hour your DB instance is running in an available state. Stop or delete it to avoid being billed for additional instance hours
					~ Backup storage upto the size of EBS volume attached to the RDS instance is free	
					~ Partial DB instance hours consumed are billed as full hours.
					~ When your database is backed up, the backup data (including transactions logs) is geo-redundantly replicated across multiple Availability Zones to provide even greater levels of data durability.
					~ Multi-AZ: You are not charged for the data transfer incurred in replicating data between your primary and standby
					~ RDS reserved instances: 1-3 Year reservation / High discounts / Payment -- No Upfront, Partial Upfront, All Upfront
						Bought at Region level & not AZ & Cannot be transferred to other Region (Remember EC2 supports both RI and capacity reservation)
						Reserved Instances discounts can be applied against Read Replica and Multi-AZ setup
					~ Read Replicas are billed as standard DB instance
			
			- Parameter Groups:
					~ A DB parameter group acts as a container for engine configuration values that are applied to one or more DB instances
					~ When you change the DB parameter group associated with a DB instance, you must manually reboot the instance for the config to kick in
					~ Set character set or collation before creating the DB instance and databases within. This ensures that the default database and new databases in your DB instance use the character set and collation values that you specify. If you change character set or collation parameters for your DB instance, the parameter changes are not applied to existing databases
					~ When you change a static parameter and save the DB param group, the parameter change takes effect after you manually reboot the DB instance
					~ When you change a dynamic parameter and save the DB param group, the change is applied immediately regardless of the Apply Immediately setting

			- Upgrading a DB instance engine version
					~ Major version supported for 3 years from the date of initial support by RDS
					~ Minor version supported for 1 year from the date of initial support by RDS
					~ When Amazon RDS supports a new version of a database engine, you can choose how and when to upgrade your database DB instances
					~ There are two kinds of upgrades: 
							- major version upgrades 
							- minor version upgrades
					~ a minor version upgrade includes only changes that are backward-compatible with existing applications
					~ major engine version upgrade can introduce changes that are not compatible with existing applications
					~ DB Admin can enabled auto-upgrade minor versions
					~ DB Admin needs to manually modify the DB engine version from console, CLI and Api, when upgrading major versions 
					~ Single-AZ Aproach:
							##### Do it during maintenance window as this will require an outage
					~ Read Replicas Upgrade:
							##### If your DB instance is using read replicas, you must upgrade all of the read replicas before upgrading the source instance.  
					~ Multi-AZ upgrade:
							##### If your DB instance is in a Multi-AZ deployment, both the writer and standby replicas are upgraded at the same time. Your DB instance will not be available until the upgrade is complete, resulting in app outage
				~ Upgrade Procedure: 
						~ To manually upgrade a database instance to a supported engine version, use the Modify DB Instance command on the AWS Management Console or
							the ModifyDBInstance API and set the DB Engine Version parameter to the desired version. By default, the upgrade will be applied or during 
							your next maintenance window. You can also choose to upgrade immediately by selecting the Apply Immediately option in the console API.
						~ If AWS determines that a new engine MINOR VERSION contains significant bug fixes compared to a previously released minor version, 
							AWS will schedule automatic upgrades for DB instances which have the Auto Minor Version Upgrade setting to “Yes”. 
							These upgrades will be scheduled to occur during customer-specified maintenance windows.
						~ Testing: Create a DB snapshot of your existing DB instance, restoring from the DB snapshot to create a new DB instance, and 
							then initiating a version upgrade for the new DB instance. You can then experiment safely on the upgraded copy of your DB instance 
							before deciding whether or not to upgrade your original DB instance.
						~ Actual Upgrade: Amazon RDS takes two DB snapshots during the upgrade process. The first DB snapshot is of the DB instance before any 	
							upgrade changes have been made. If the upgrade doesn't work for your databases, you can restore this snapshot to create a DB instance running the old version. The second DB snapshot is taken when the upgrade completes.
						~ Backups and Snapshots:
							In Multi-AZ setup, backups are taken on the STANDBY database to avoid interrupting the PRIMARY database

			- CW Events for RDS:
					Four categories: DB Instance, DB Snapshot, DB Security Group, DB Parameter Group
			
			- Define Maintenance window: 
				~ YOU SHOULD GENERALLY EXPECT OUTAGE DURING RDS MAINTENANCE 
				~ Maintenance Windows are 30-minutes time-slot every week defined by the user 
				~ If the user does not specify, AWS randomly chooses a 30-min maintenance window
				~ Activities carried out during maintenance window are as follows:
					DB Instance modification
					Version upgrade
						+
					OS Patching
					DB Instance patching
					DB Scaling 
					Security fix 
					Other fixes
				~ RDS Software Update, OS Patching, DB Scaling, General Maintenance:
					Single-AZ Aproach:
						Do it during maintenance window as this will require an outage
					Multi-AZ Approach: 
						1) Do the necessary changes on STANDBY first 
						2) Promote the STANDBY to PRIMARY 
						3) Do the necessary changes on the old primary (Once step 3 is done, remember that there is NO NEED to flip the primary status and confer it on old primary) 

			- Backups, Snapshots & Restoration:
					~ What gets backed-up in the 'Backup Snapshot'?
							~ The DB instance (including all databases within the instance)
								+
							  The Transaction Logs
					~ Constraints during backup:
							~ REMEMBER I/O is suspended when backup is taken to maintain consistency
					~ Modes of Backups: 
							- Automated Backups
							- Manual Backups - User Initiated
					~ Where are the backups stored?
							~ Backup snapshots of the DB Instance and Tx Logs are stored in S3 buckets
					~ What is the backup retention period
							~ Automated Snapshots are retained for 7(default) to 35 days (MAX)
							~ Manual Snapshots can be stored indefinitely
					~ Durability of snapshots:
							~ Multiple copies of the snapshots are stored in various AZs for durability
							~ Automated backups are deleted when the source database is deleted 
							~ Manual backups are NOT deleted when the source database is deleted
					~ What if RETENTION PERIOD is ZERO?
							~ The Backups will be DISABLED
					~ Recovery Point Objective:
							~ Automated snapshots gives you point-in-time recovery to upto 5-minutes in the past
					~ Automated Backup:
							~ I/O is always suspended on the DB instance from where the backup is taken
							~ In Single-AZ setup, backup is taken from primary database
							~ In Multi-AZ setup, backup is taken from the standby to avoid I/O suspension on the Primary
							~ Automated backups are deleted when the DB instance is deleted
							~ RPO = System state at (CURRENT TIME - 5 minutes)
					~ Manual Backup: 
							~ User initiated snapshot / Saved in S3
							~ Since I/O is suspended, it is preferred to take backups during maintenance window
							~ If the DB instance gets deleted, manually created DB backup snapshots are not deleted
							~ Manual snapshots restore to the point when the backup was taken
					~ Restoration:
							~ Restoration always creates a new DB instance with a new endpoint 
							~ Restoration does not over-write the original source db

			- Data import into RDS:
					MySQL: mysqldump, mysqlimport
					Oracle: SQL Loader, data pump
					Postgresql: pg_dump
				
			- Scaling Factors:
					~ Compute and Storage can be scaled
					~ Scaling Storage:
							~ Storage can be scaled UP with no downtime 
							~ Storage CANNOT be scaled down
							~ *** EXCEPTION - MS SQL Server DOES NOT SUPPORT changing the Storage Capacity and the Storage Type / You need to take a snapshot and then restore into a new database with desired configuration
					~ Scaling Compute:
							~ scaling results in DB instance unavailability as the instance is moved from from one instance to another. AWS claims it to be few minutes, so do it during maintenance window

			- Query slowness:
					~ MYSQL: slow query logs
					~ MariaDB: slow query logs
					~ Oracle: Trace files
					~ SQL Server: Traces - client-side and server-side
		
			- RDS Notification to Interested Parties:
					~ RDS uses SNS to notify RDS events to interested parties
					~ You MUST EXPLICILY SUBSCRIBE to receive the events 
					~ Using CONSOLE, you can view last 24 hours of events
					~ Using CLI, you can view last 14 days of events
			
			- RDS on Outpost:
					~ Run RDS on-premise
			
			- RDS Proxy:
					Use-cases:
						connection pooling: maintains a pool of database connections to avoid unnecessary stress of open and close connections
						connection sharing: enables multiple application connections to share a database connection for efficient use of database resources
						IAM support: enforce IAM based authentication with relational databases
						Predictable perf: allows you to maintain predictable db performance by regulating the number of database connections that are opened
						Removes requests: that cannot be served to preserve overall performance and availability of the application
						Failover: during transient failure, automatically routes traffic to a new database instance while preserving application connections
						Integration with secrets manager: Manage database credentials through AWS Secrets Manager
					RDS Proxy can add an average of 5 milliseconds of network latency to query or transaction response time
			
			- RDS Security:
					~ RDS DB Instance - Master user account :
						is a native database user account used to control access to the DB. And from this account, you create more user credentials
						Role in Oracle: dba
						Role in SQL Server: db_owner
						Role in MySQL: create, drop, references, event, alter, delete, index, insert, select, update, create temporary tables, lock tables, trigger, create view, show view, alter routine, create routine, execute, trigger, create user, process, show databases, grant option.
					~ RDS Supports data-in-motion encryption using SSL certificates / RDS generates the certificate for the DB Instance
					~ At rest encryption using KMS is supported / SQL, Oracle - TDE / Unencrypted database->Snapshot->Copy snapshot+KMS->New snapshot->DB instance from new snapshot
					~ An ENCRYPTED DATABASE will create the following:
						- Encrypted backup snapshots (automated and manual)
						- Encrypted STANDBY in Multi-AZ
						- Encrypted Read Replica

	===================================================
	RDS Aurora
	===================================================
			- Managed RDS service compatible with MySQL (5x throughput of MySQL), Postgresql (3x throughput of Postgresql) 
					**************
					************** Base RDS concept of DB Instances, Databases are intact **************
					**************
					~ with upto 72 hours of backtracking for MySQL 
					
						
			- Aurora DB Cluster :
					~ Single-Master Replication within a REGION
					~ Multi-Master Replication between REGIONS
					~ Aurora Global Database with multiple cluster in multiple regions
					
			- Aurora Single-Master Cluster:
						###### 
						An Aurora Single-Master DB Cluster = 	1 Primary DB Instance 
																+ upto 15 Read Replica DB Instances
						######
					~ Single-Master Cluster is a cluster with one Master
					~ Cluster Volume = Virtual DB-storage volume that spans 3 AZs and each AZ has a DB instance (which is complete copy of the data)  
					~ 1 Primary DB Instance = One DB instance that manages all READ and WRITE operations and replicates to the cluster volume 
					~ Multiple DB Instance Replicas = Multiple read-only replicas that connect to the same cluster volume for read purposes
			
			- Replication within cluster volume:
					~ The DB cluster volume is physically made up of multiple copies of the data for the DB cluster
					~ The primary instance and the Aurora Replicas in the DB cluster all see the data in the cluster volume as a single logical volume
					~ As a result, all Aurora Replicas return the same data for query results with minimal replica lag
					~ This lag is usually much less than 100 milliseconds after the primary instance has written an update
					~ Replica lag varies depending on the rate of database change. That is, during periods where a large amount of write operations occur for the database, you might see an increase in replica lag.
					
			- Aurora Replicas:
					~ If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer and the 'Cluster endpoint' automatically points to the new writer
					
			- Aurora Multi-Master Cluster:
					~ Multi-Master Cluster is a cluster where all DB instances are Masters i.e. can perform read/write operations
					~ There is NO more read replicas OR the need to promote a Read replica to a master as part of Master failure 
					~ There are multiple WRITER endpoints. There are no more READER endpoints 
					~ Supports Global-Read-After-Write (GRAW):  any read operations always see the most current state of the data. Set the aurora_mm_session_consistency_level to REGIONAL_RAW
					~ WRITE CONFLICTS:
							A situation that occurs when different DB instances attempt to modify the same data page at the same time. The earliest change request is approved using a quorum voting mechanism. That change is saved to permanent storage.  The DB instance whose change isn't approved rolls back the entire transaction containing the attempted change. Rolling back the transaction ensures that data is kept in a consistent state, and applications always see a predictable view of the data. Your application can detect the deadlock condition and retry the entire transaction.
					~ For TRUE Multi-Master Active-Active workloads:
							~ It is recommended that user segment client writes based on workload i.e. specific workload writes go to a specific DB instance which is the shard for that data type
					 
			- Aurora Global Database:
						###### 
						An Aurora Global Database = 			1 Primary DB Instance in Primary Cluster  
																+ upto 5 secondary clusters AND upto 16 Reader DB Instances in each secondary cluster
						######
					~ Multiple REGION 
					
			- Aurora Cluster Volume:
					~ Contains all your user data, schema objects, and internal metadata such as the system tables and the binary log
					~ Auto-growth upto upto 128TiB
					~ Starting in Aurora MySQL 2.09.0 and 1.23.0, and Aurora PostgreSQL 3.3.0 and 2.6.0, when Aurora data is removed, such as by dropping a table or database, the overall allocated space decreases by a comparable amount
					~ Aurora preloads the buffer pool with the pages for known common queries that are stored in an in-memory page cache
					
			- Aurora Connection Mechanism:
					~ Clients connect to an intermediate handler called "Endpoint" which then connects to the DB Instance
					~ Cluster endpoint (Writer endpoint): connects to the Primary DB instance. You can't create, delete, or modify this kind of endpoint
					~ Reader endpoint: load-balances connections to one of the read replicas in the cluster 
					~ Custom endpoint: represents a set of DB instances. When you connect to the endpoint, Aurora performs load balancing and chooses one 
					~ Instance endpoint: connects to a specific DB instance within a cluster

	===================================================
	DynamoDB:
	===================================================
			- Fully-managed REGION-scope NoSQL db with provisioned & on-demand capacity models (ms latency) with optional DAX based-caching (for mus latency)
				- Max item size = 400KB
				- Supports DynamoDB Local = downloadable version of DynamoDB
				- Supports READ Replicas with 1 master and multiple replicas across multiple regions
				- Supports multi-master replication using Global DynamoDB Tables, but needs streams to be turned ON
				- Point-in-time recovery to anytime upto 35 days in the past
			- Global Tables:
					- Multi-Region, Multi-Master replica tables owned by a single AWS account 
					- Global tables use DynamoDB Streams to propagate changes between replicas
					- Global Table does not support strongly consistent reads across Regions. 
							~ Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region
					- For conflicting writes across regions, the last writer wins. 
			- Attributes 
					- Scalar Types – A scalar type can represent exactly one value. The scalar types are number, string, binary, Boolean, and null
					- Set Types – A set type can represent multiple scalar values. The set types are number set, string set, binary set
					- Document Types – list and map
			- Partition: 
					- Partition is an allocation of storage for a table, backed by SSD auto-replicated across multiple AZs within an AWS Region 
					- "Partition Key": partition to store an item determined by the internal hash of the Partition Key
					- "Partition Key + Sort Key" : partition determined by the internal hash of the Partition key AND item sorted by the Sorted Key
					- To read an item from the table, you must specify its partition key value and sort key value
			- Indices:
					- A secondary index contains a subset of attributes from a table, along with an alternate key to support Query operations
					- When you create a secondary index, you need to specify the attributes that will be projected into the index. Three different options for this:
						~ KEYS_ONLY – Each item in the index consists only of the table partition key and sort key values, plus the index key values. The KEYS_ONLY option results in the smallest possible secondary index.
						~ INCLUDE – In addition to the attributes described in KEYS_ONLY, the secondary index will include other non-key attributes that you specify. 
						~ ALL – The secondary index includes all of the attributes from the source table. Because all of the table data is duplicated in the index, an ALL projection results in the largest possible secondary index.
					- GSI: 
							20 per table / An index with a PK & SK different from those on the table / A projection is set of attributes copied from a table into a secondary index. The PK & SK are always projected into the index; you can project other attributes to support your app's query requirements / GSI does not support strongly consistent reads. A global secondary index is considered "global" because queries on the index can span all of the data in the base table, across all partitions. A global secondary index is stored in its own partition space away from the base table and scales separately from the base table. Queries on global secondary indexes support eventual consistency only. Every global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.
					- Local Secondary Index: 
							5 per table / An index that has the same partition key as the table, but a different sort key / LSI is created when table is created / LSI is deleted when table is deleted. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. A local secondary index is "local" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. When you query a local secondary index, you can choose either eventual consistency or strong consistency. Queries or scans on a local secondary index consume read capacity units from the base table. When you write to a table, its local secondary indexes are also updated; these updates consume write capacity units from the base table.
			- Streams:
					- Asynchronous generation of stream records which is encrypted at rest
					- No duplicate records in the stream
					- Stream records retained for 24 hours
					- Read data using KCL. Upto 2 simultaneous shard consumers  
					- Number of shards:
							~ number_of_shards = ceiling( ((write_throughput * (1+percentage_of_updates) * average_record_size_in_bytes) /1024 /1024), 1)
			- Read/Write Measurement Metrics:
					- A strongly consistent read might not be available if there is a network delay or outage. In this case, DynamoDB may return a server error (HTTP 500). Strongly consistent reads are not supported on global secondary indexes.
					- 1 RU = 1 strongly consistent read of 4KB item = 2 eventually consistent reads of 4KB item = Half of transactional read of 4KB item
					- 1 WU = 1 write of a 1KB item = Half of transactional write of 1KB item
					- If your application reads or writes larger items (up to the DynamoDB maximum item size of 400 KB), it will consume more capacity units.
			- On-demand Capacity Mode:
					- Newly created able starts with 2000 WU, 6000 RU. Capacity can double immediately, but then ramp-up will freeze for 30 minutes 
			- Provisioned Mode:
					- You specify the number of reads and writes per second that you require for your application
			- Actions:
					- CreateTable, DescribeTable, PutItem , BatchWriteItem (Batch of 25 PutItem), GetItem (provide Table name, PK, SK, "ConsistentRead" param), BatchGetItem(100 GetItem), Query (all items with a Partition Key), Scan (all items in table and return all attributes), UpdateItem , DeleteItem (conditional delete), DeleteTable
					- Projections can be applied to identify the attributes to return
					- Expressions:
							~ denote the attributes that you want to read from an item
							~ aws dynamodb get-item --table-name ProductCatalog --projection-expression "Description, RelatedItems[0], ProductReviews.FiveStar"
			- ACID Transactions:
					- TransactWriteItems  = group multiple Put, Update, Delete, and ConditionCheck
					- TransactGetItems = multiple Get actions
			- Time To Live:
					- Define a per-item timestamp to determine when an item is no longer needed. DynamoDB deletes the item without consuming any WU
					- Expiry time expressed in unix epoch time
					- TTL can be enabled and disabled



	===================================================
	AWS DMS:
	===================================================
		Service to migrate database: 
			- From On-premise DB to AWS DB
			- From AWS DB to On-premise DB
			- From AWS DB to AWS DB
				- Source, Target DBs and Replication Service in the same VPCs
				- Source DB in one VPC, Target DB in another VPCs, Replication Instance in either one of the VPCs
				- Source (or Target) is in On-premise, Target (or source) is in AWS and Replication Instance is in VPC
			- Remember to open up Security Groups and NACLs appropriately
		Source and Target DB engines could be the same 
		Source and Target DB engines could be different
		Migration can be One-Time OR Continuous replication 
		DMS just requires DB connection information for Source and Target
		Migration service is fully managed and provides internal failover 
			DMS Replication nodes are hosted on AWS
			DMS provides two nodes for redundancy and failover
			Multi-AZ nodes: Primary and Secondary DMS node can be spread in different AZs
			DMS nodes are nothing but EC2 instances
			DMS nodes run Replication Tasks
			A DMS node can run one or more REPLICATION Tasks
			You can create the target table or have the Replication task create it for you
			Most of the source schema can be migrated
			Homogeneous migration:
				DB Native tools can be used for export-import schema
				DMS does the actual data move
			Heterogeneous migration:
				DMS used Schema Conversion Tool (SCT) to export/import the schema
				DMS does the actual data move
		DMS phases:
			The full load of source data
			The application of cached changes
			Write to target database
			Ongoing replication
				
	===================================================
	EMR:
	===================================================
		Elastic Map Reduce = AWS version of Hadoop cluster running on EC2(Compute) and S3(Storage)
		Map (read in parallel, tokenize, category) followed by Reduce (aggregate, write)
		EMR can use two file systems for storage:
			- EMRFS built on top of S3 and the data (input, intermediate and output is persistent)
			- HDFS file system on top of any POSIX compliant fs (like EFS) (intermediate data is not persisted after the lifecycle of your cluster)
		EMR Cluster components:
			EMR uses EC2 for compute and you can login to compute nodes
			Ideal to have nodes in the CLUSTER placement group with SRIOV or EFA ENI or Nitro instances
			Core Node - Node that runs tasks and store data in the HDFS. Mandatory to have atleast one in the cluster
			Task Node - Node that runs tasks, but does not store data in the HDFS. Optional to have any in the cluster
			Master Node - Node that manages cluster, distributes data and tasks amongst Core and Task nodes, monitors the tasks that were distributed
			Mix different EC2 tiers for cost management (like On-Demand or RI for base processing AND spot instances for acceleration)
		
	===================================================
	ElastiCache:
	===================================================
		ElastiCache cluster is hosted in a VPC
			Cluster is composed of nodes
			A cluster MUST have atleast one node
			A node is either a REDIS or Memcached node 
			Cluster cannot have a mix of REDIS and Memcached nodes
			Nodes outside the VPC cannot access the cache
			ElastiCache nodes are On-Demand, Reserved Instance, but not Spot Instances
			Cluster is represented by endpoint
		Engine:
			- REDIS (in-memory + persistent NOSQL backend)
			- Memcached (fully in-memory, data is lost when node is stopped)
		Memcached Engine:
			Fully in-memory nodes
			No persistence mechanism at the node level
			Does not support Multi-AZ setup
			Does not support replication
			Does not support snapshot for backup/restore
			Nodes in cluster can be spread across multiple AZs:
				In case of node failure, there is partial data loss corresponding to the failed node
		REDIS
			In-memory nodes + file system persistence (This is not a write-through cache like DAX; as in the persistence part, you need to take a snapshot)
			Persistence mechanism exists at the node level
			Supports Multi-AZ setup (similar to RDS)
				REDIS service detects failure
				Promotes a Read Replica to be the Primary R/W no
				Switches DNS CNSM record to point to the new replica
				Other read replicas will auto-sync from the newly promoted node
			Supports master-slave replication
			Supports snapshot for backup and restore
			Nodes in cluster can be spread across multiple AZs
			REDIS cluster shards the data
				REDIS CLUSTERING DISABLED
					Only 1 shard 
					1 shard = 1 R/W Primary node + O-5 read only replicas
				REDIS CLUSTERING ENABLED
					1 to 15 shards
					1 shard = 1 R/W primay node + 0-5 read only replicas
			Better to take backup snapshot of the read replica instead of the R/W replica

	===================================================
	Kinesis:
	===================================================
		PRODUCERS -> Kinesis Ecosystem-> CONSUMERS
		Use-cases
			Click-stream
			IoT sensors
			Log files
			Game player activities
			Financial trading floor
			Stock markets
			Telemetry
		Kinesis can process (collect, make available, read, transform, analyze, shard, high-throughput load+transfer) millions of DATA RECORDs (each in KB or MB) being sent to it continuously by 1000s of PRODUCERSs
		Kinesis Ecosystem has 3 services within it:
			* STREAMS
			* ANALYTICS
			* FIREHOSE
		STREAMS:
				- COLLECT data records from 1000s of PRODUCERSs
				- Think of streaming data collection mechanism
				- Map the data records into multiple SHARDs
				- Therefore, a STREAM gets divided into multiple SHARDs
				- Transfer data records (or Shard of data records) to corresponding CONSUMERs or Streams Applications
				Producer and Consumer:
					- PRODUCER - custom application that is written using the Kinesis Producer Library and hosted on say EC2
					- CONSUMER - custom application that is written using the Kinesis Consumer Library and hosted on say EC2
				SHARD:
					- Stream is divided into 1 or more Shards
					- Data records are hashed/mapped into 1 shard
					- Input throughput = 1Mb/sec or 1000 PUTS/sec
					- Output throughput = 2Mb/sec
					- Data record retained in streams for 24 hours (default). Can be extended to 7 days for addl charges
				Streams Encryption:
					- Data records can be encrypted using CMK from KMS
					- Producer and Consumer needs access to the CMK
				Streams durability:
					- Auto-replicated to 3 AZs for durability and availability
		FIREHOSE:
				- This of streaming data INGESTION SERVICE 
				- Load streaming data into FOUR services:
						S3
						RedShift
						ElastiSearch and 
						Splunk
				- A Kinesis Stream can be an input into Kinesis FireHose
				- You do not write a custom Producer or Consumer application (like how you do in Streams)
				- Scales automatically to match the throughput of the source
				- Does not retain the data records for 24 hours or 7 days like Streams across 3 AZs
				- Can buffers the data for upto 24 hours, if the destination is not available
				- Firehose can TRANSFORM(using Lambda), BATCH, COMPRESS, ENCRYPT
				- During transformation process, the source data records can be configured to be saved in a backup S3 bucket
				- During ingestion into Redshift cluster, the intermediate transformed record is saved in a backup S3 bucket
				- Firehose encryption:
					Firehose can ingest encrypted data from encrypted Kinesis Streams
		ANALYTICS:
				- Analyze streaming data with SQL
				- Can ingest data from KINESIS STREAMS or KINESIS FIREHOSE only
				- Can output data into KINESIS STREAMS or KINESIS FIREHOSE only

	===================================================
	RedShift:
	===================================================
		AWS fully-managed STRUCTURED OLAP COLUMNAR database supporting SQL, Advanced Compression, Data Storage & Query Parallelization
		Fully-managed
			- supports replication
			- auto patched
			- cannot log into the node
		Data ingestion
			RedShift can store petabyte-scale data
			Min size = 160GB
			Redshift cannot ingest large volumes of data in real data. Therefore, Kinesis is used for data load into Redshift
		Availability:
			Redshift does not support multi-AZs
			Redshift does not support Spot instances
			All nodes in a cluster are stored within a single AZ
		Cluster:
			Cluster consists of nodes
			Cluster is structured as Leader Node + upto 128 Compute Nodes
			Redshift can sense node failure and spin up a new replacement node
				- Limitaton: The entire CLUSTER is unavailable for queries and update until the replacement node is provisioned
				- Redshift loads most frequently accessed data from S3 into replacement node
			Leader node:
				Receives client connections
				Receives queries
				Since Redshift is fully managed, you cannot log into node
			Compute node:
				Stores data
				Execute the query
				Runs computations
				Since Redshift is fully managed, you cannot log into node
				Replicates data to other compute nodes thus increasing durability due to additional copies
		Backup & Restore:
			Backs up data in 24 hours as the default configuration
			Modes of Backups: 
				- Automated Backups
				- Manual Backups - User Initiated
			Backup snapshots stored in S3 can be replicated to other region
			Automated Backups:
				Similar to RDS
				Automated backup can happen at frequency of 24 hours (Default) to 35 days
				If backup frequency is 0 days, then no automated backups happen
				Backup snapshots stored in S3
				Deletion of Redshift cluster deletes all automated backup snapshots from S3
			Manual Backups:
				User initiated backup
				Backup snapshots stored in S3
				Deletion of Redshift cluster does not delete manual backup snapshots from S3
		Data Encryption:
			Supports data-at-rest encryption using hardware accelerated AES-256 bits
			Supports data-in-motion SSL encryption between client apps and cluster
			Keys can be:	
				Managed by Redshift
				Managed by user in KMS
				Managed by user in HSM
		Billing:
			You don't pay for Leader node
			Pay for compute node hours
			Pay for storage
			Pay for backup storage
			Pay for data transfer from S3 in other regions
		
	===================================================
	DynamoDB + Redshift
	===================================================
		Serverless combo to provide OLTP and OLAP
		Remember that DynamoDB is NOSQL database, while Redshift is SQL database
			- There are rules to keep in mind whil you copy data from Dynamo to Redshift
			
	===================================================
	DynamoDB + Apache Hive (EMR)
	===================================================
		- DynamoDB natively does not support SQL, but Hive QL can leverage SQL-like language on top of DynamoDB
		- DynamoDB does not support join operations, but Hive QL can join tables in DynamoDB
		
	===================================================
	AWS Glue:
	===================================================
		Extract-Tranform-Load (TL) serverless service
		Three sub-services:
			- Automatic crawling:  	populate central meta-data repo called Glue Data Catalog with schmea, location, business attribs, data change history
			- Job authoring: 	   	data transformation code in Python and Scala; setup, monitoring and orchestration of complex data flows
			- Job execution:	   	scheduler that handles dependency resolution, job monitoring and retries
									Apache Spark EMR jobs to load data from source into destination
									
	===================================================
	AWS SWF:
	===================================================
		Simple WorkFlow Service	
		Concepts:	
			Domain: Scoping mechanism / Contains 1 or more workflows / Workflows in different domains cannot interact / AWS Account can have multiple Domains
			Workflow: Set of activities


	===================================================
	SQS:
	===================================================
		Supports HTTPS, TLS, JMS
		Consumer polls the queue / default upto 10 messages in a single pull
			Free tier: 1 Million request / month - $0 charge
		Long polling - 20 seconds timeout 
		Visibility timeout: Default - 30 seconds / Max - 12 hours
		Message retention in queue: 1 minute to 14 days (4 days default)
		Message: 
			- Size=1KB to 256KB / XML - JSON - Text
			- 10 meta-data attributes
			- SenderID: usually the AWS account id; but anonymous sends will have the IP address embedded
			- Poison pills are special messages that can be received, but not processed. They are a mechanism used in order to signal a consumer to end its work so it is no longer waiting for new inputs, and is similar to closing a socket in a client/server model.
			- All messages have a global unique ID that Amazon SQS returns when the message is delivered to the message queue.
		Message Sequence: No guarantee in standard / guaranteed sequeunce in FIFO
		Message delivery: Atleast once delivery in standard / exactly once in FIFO
		Message duplication: Potential dupliction in standard / No duplication in FIFO
		Queue delay: maximum 15 minutes
		Queue length: Unlimited / 120K for inflight in standard queue / 20K for inflight in FIFO queue
		Queue name: 80 characters, alphanumeric,-,_ (.fifo suffix for FIFO queues)
		Throughput: Unlimited for standard || 300/sec for FIFO with no batching || 3000/sec for FIFO with batching
		Batching during messaging:
			Batch operations (SendMessageBatch, DeleteMessageBatch, and ChangeMessageVisibilityBatch) all cost the same as other Amazon SQS requests
		FIFO queue
			Message groups:
				Messages are grouped into distinct, ordered "bundles" within a FIFO queue
				All messages need a group ID. If you don't provide a message group ID, the action fails. Therefore, if you are not using the group concept, ensure that all messages have distinct group ID
				For each message group ID, all messages are sent and received in strict order. 
				However, messages with different message group ID values might be sent and received out of order. 
			Not comptible with FIFO queues:
				Auto Scaling Lifecycle Hooks
				AWS IoT Rule Actions
				AWS Lambda Dead Letter Queues
				Amazon SQS Buffered Asynchronous Client
		Server-side Encryption:
			Encrypted: Message Body
			Not encrypted: Queue meta-data / Message meta-data / Per-queue metrics 
			To send messages to an encrypted queue, the producer must have the kms:GenerateDataKey and kms:Decrypt permissions for the CMK.
			To receive messages from an encrypted queue, the consumer must have the kms:Decrypt permission for any CMK that is used to encrypt the messages in the specified queue.
			If the queue acts as a dead letter queue, the consumer must also have the kms:Decrypt permission for any CMK that is used to encrypt the messages in the source queue.
		Message Filtering:
			Not supported in SQS
		

	===================================================
	SNOWBALLFamily:
	===================================================
		Snowball:
				Tamper-resistant device to transfer large amounts of data
					Data into AWS S3
					Data out of AWS S3
				Supports 256-bit encryption
				Sizes=50TB (42TB-USA), 80TB 72(TB-USA)
				Jobs:
					Configuration in the console
					Import job:
						1:1 association with a Snowball
						Transfer data into AWS S3
						Each file in Snowball becomes an object in S3
						Each directory in Snowball becomes a prefix in S3
					Export job:
						Transfer data out of AWS S3
						Data is split into multiple 72TB parts
						1:m association with multiple Snowballs
		Snowball Edge:
				Similar to Snowball but with ability to run compute on the device
				Supports running EC2 instances using AMIs on the device
				These EC2 instances can contain functionality that transform the data whil it is in motion to AWS data center or to the Customer location
				Sizes=100TB (83TB), 100TB (45TBper node clustered)
				Supports running Python Lambda functions when you put into S3
				Supports:
					Import job into AWS S3
					Export job from AWS S3
					5 to 10 Snowball Edges be clustered to form local storage
					Can be used with AWS IOT GreenGrass
					Supports file transfers through NFS
		Snowball Mobile:
				Can transfer exabyte-scale data
				Upto 100PB per Snowmobile
				Data imported into S3 or Glacier
				Mobile protected by GPS Tracking, Security Guards, Alarm Monitoring, 24/7 Video Surveillance, 
		

	===================================================
	SNS
	===================================================
		SNS Topic / Pub-Sub model / Push mechanism / No polling
		100K topics per account / 1 Million subscriptions per topic
		Message 
			- Size=1KB to 256KB / XML - JSON(default) - Text - RAW messages
			- RAW messages for HTTP(S) are embedded into body of the POST
			- Each SMS message can contain up to 140 bytes
			- Upto 10 name-value type meta-data attributes
		Topic name: 256 characters, alphanumeric,-,_  (ARN example: arn:aws:sns:us-east-1:1234567890123456:mytopic)
		Message delivery: 
			A SINGLE topic can support subscriptions and notification deliveries over multiple transports.
			“HTTP”, “HTTPS” – Subscribers specify a URL as part of the subscription registration; notifications will be delivered through an HTTP POST to the specified URL.
			”Email”, “Email-JSON” – Messages are sent to registered addresses as email. Email-JSON sends notifications as a JSON object, while Email sends text-based email.
			“SQS” – Users can specify an SQS standard queue as the endpoint; Amazon SNS will enqueue a notification message to the specified queue (which subscribers can then process using SQS APIs such as ReceiveMessage, DeleteMessage, etc.). Note that FIFO queues are not currently supported.
			“SMS” – Messages are sent to registered phone numbers as SMS text messages.
		Message Filtering:
			Supported in SNS
			Topic subscribers selectively receive only a subset of the messages they are interested in, as opposed to receiving all messages published to a topic
		SNS does not support forwarding messages to FIFO queues
		Users can receive notifications from Amazon SNS in two ways:
			Users with AWS IDs: Subscribers with valid AWS IDs (please refer to this link for details on obtaining AWS IDs) can subscribe to any topic directly – as long as the topic owner has granted them permissions to do so. The AWS IDs will be validated as part of the subscription registration.
			Other users: Topic owners can subscribe and register end-points on behalf of users without AWS IDs.
			In both cases, the owner of the subscription endpoint needs to explicitly opt-in and confirm the subscription by replying to confirmation message sent by Amazon SNS.
			You no longer need to subscribe a phone number to an Amazon SNS topic before you publish messages to it
		Subscription confirmation:
			The exact mechanism of confirming the subscription varies by the transport protocol selected:
			For HTTP/HTTPS notifications, Amazon SNS will first POST the confirmation message (containing a token) to the specified URL. The application monitoring the URL will have to call the ConfirmSubscription API with the token included token.
			For Email and Email-JSON notifications, Amazon SNS will send an email to the specified address containing an embedded link. The user will need to click on the embedded link to confirm the subscription request.
			For SQS notifications, Amazon SNS will enqueue a challenge message containing a token to the specified queue. The application monitoring the queue will have to call the ConfirmSubscription API with the token.
			Note: The explicit “opt-in” steps described above are not required for the specific case where you subscribe your Amazon SQS queue to your Amazon SNS topic – and both are “owned” by the same AWS account.
					When users opt-out, the subscription is disabled (but still retained and not completely deleted)	
		Message delivery:
			All notification messages will contain a single published message
			No non-duplication guarantee / occasional duplicate messages at the subscriber end
			Sequence: Best effort to deliver in order,  network issues could potentially result in out-of-order messages
			Once a message has been successfully published to a topic, it cannot be recalled
			Amazon SNS guarantee that messages are delivered to the subscribed endpoint, as long as the subscribed endpoint is accessible
				If a message cannot be successfully delivered on the first attempt, Amazon SNS executes a 4-phase retry policy: 
					1) retries with no delay in between attempts, 
					2) retries with minimum delay between attempts, 
					3) retries according to a back-off model, and 
					4) retries with maximum delay between attempts. 
				When the message delivery retry policy is exhausted, Amazon SNS can move the message to a dead-letter queue (DLQ)
		SNS Mobile Push 
			Use SNS to deliver push notifications to Apple, Google, Fire OS, and Windows devices, as well as Android devices in China with Baidu Cloud Push.
			Push notifications can only be sent to devices that have your app installed, and whose users have opted in to receive them
			With push notifications, an installed mobile application can notify its users immediately by popping a notification about an event, without opening the application
		SNS Direct Addressing:
			At this time, direct addressing is only supported for mobile push endpoints (APNS, FirebaseCM, Amazon Device Messaging, Windows NS, Microsoft PNS, Baidu) and SMS. Email messaging requires the use of topics.

	===================================================
	CloudWatch:
	===================================================
		The CloudWatch Logs Agent is supported on Amazon Linux, Ubuntu, CentOS, Red Hat Enterprise Linux, and Windows
			Windows: custom text logs, Event (Application, Custom, Security, System) logs, Event Tracing (ETW) logs, and Performance Counter (PCW)
			Linux: 
		Amazon CloudWatch Vended logs are logs that are natively published by AWS services on behalf of the customer. 
			VPC Flow logs is the first Vended log type that will benefit from this tiered model.
		Amazon CloudWatch Logs Insights is an interactive, pay-as-you-go, and integrated log analytics capability for CloudWatch Logs
		Amazon CloudWatch Anomaly Detection applies machine-learning algorithms to continuously analyze single time series of systems and applications, determine a normal baseline, and surface anomalies with minimal user intervention
		Amazon Contributor Insights which analyzes time-series data to provide a view of the top contributors influencing system performance
		CloudWatch ServiceLens ties together CloudWatch metrics and logs as well as traces from AWS X-Ray to give you a complete view of your applications and their dependencies
		Amazon CloudWatch Synthetics allows you to monitor application endpoints more easily. It runs tests on your endpoints every minute, 24x7, and alerts you as soon as your application endpoints don’t behave as expected
		A custom metric can be one of the following:
			Standard resolution, with data having one-minute granularity
			High resolution, with data at a granularity of one second
			You can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds
		Alarm:
			Resolution = 10 seconds or 30 seconds
			Alarm history available for 14 days
		Amazon CloudWatch Events (CWE) is a stream of system events describing changes in your AWS resources
			Your applications can emit custom events by using the PutEvents API, with a payload uniquely suited to your needs
			When an event matches a rule you've created in the system, you can automatically invoke an AWS Lambda function, relay the event to an Amazon Kinesis stream, notify an Amazon SNS topic, or invoke a built-in workflow.
			CloudWatch Events is able to generate events on a schedule you set by using the popular Unix cron syntax. By monitoring for these events, you can implement a scheduled application.

	APIs
		AWS implements many services, exposing it via API which in turn are used by clients like the AWS console
		
	===================================================
	AWS WAF:
	===================================================
		How it works?
			Conditions checked: IP address, HTTP header, HTTP body, URI string, SQL injection, XSS
			Action: ALLOW, BLOCK, MONITOR (i.e. Count web requests)
		Integrated with: ALB, CloudFront, REST API-API Gateway
		Since CloudFront supports non-AWS site, WAF can protect non-AWS site
		Rules:
			Managed rules: OWASP, BOT, CVE
			Custom rules: user defined rules
			Rate based rule is IP-based and is used for DDOS protection, Bad bots, brute force login
				for example, the threshold for the Rate-based Rule is set to (say) 2,000, the rule will block all IPs that have more than 2,000 requests in the last 5 minute interval. 
				A Rate-based Rule can also contain any other AWS WAF Condition that is available for a regular rule
				You can relax rate control by using IP Allow List
			You can use a combination of Managed + Custom + Rate-based rules
		Testing of WAF config can be done counting web requests meeting the various WAF rules
		

	===================================================
	AWS Batch:
	===================================================
		- Jobs runs computing workloads within Docker container
		- Jobs can reference other jobs by name or ID
		- Jobs can be dependent on successful execution of other jobs
		- Job Definition defines blueprint for the job - Docker container image, CPU, memory, env variables, mount points, container properties, IAM role for AWS permissions
		- Job Queues: priotization mechanism (example: high priority queue submits time-sensitive jobs / low priority queue runs jobs when resources are cheaper) 
		- Regional service - spread jobs across multiple AZs
		- Used On-Demand and Spot instances
		- AWS Batch cloudwatch events to monitor progress of jobs

		
	===================================================
	AWS Resource Access Manager:
	===================================================
		- Service that enables sharing of resources from an account to another account OR from an account to Organization
		- Sharing is done via a "Resource Share"
		- "Resource Share" identifies the Principals like AWS Accounts, OUs, Entire organization
		- Sharing Account retains full ownership of the shared resource
		- In the Shared Account, account policies and Organization SCPs are still applicable and may restrict usage of shared resources
		- "Shareable Resources"
			- Subnet Sharing: Allows accounts to create resources in the Shared subnets of a VPC
			- Transit Gateway: Share transit gateway across accounts or an organization
			- Route 53 Forwarding Rules
			- License Manager: Sharing license configurations from a master account to member accounts
		

	===================================================
	AWS Config:
	===================================================
		- Service Tracks creates a map of relationship between resources
		- Service Tracks history of changes to a resource
		- Config Recorder: Stores configuration of supported resources
		- Config Item: point-in-time view of the various attributes of a supported AWS resource. CI retained for Minimum 30 days and Maximum 7 years
		- Config History: Collection of CIs for a given resource over a time period. CH file delivered to AWS S3
		- Config Snapshot: Collection of CIs for supported resources in a given account
		- Config Stream: SNS based stream of CIs for a given resource. Used to notify other resources that might be interested

			
	===================================================
	Well Architected Framework
	===================================================
		Framework = Pillars + Design Principles at pillar level + Questions to evaluate how well architecture is aligned to best pratices
		General Design Principles: (SPADEG)
			S:top guessing infrastructure capacity
			P:roduction scale
			A:rchitecture evolution
			D:data-driven architecture
			E:experiment
			G:ame days
		Pillars: (CORPS BCDR MESH)
			Cost Optimization: [Key service-CostAllocation Tags] CFM / Pick cost-effective resources / Match supply & demand / Expenditure awareness / Optimizing over time [CF-ER-M-EA-O]
			Operational Excellence: [Key service-CloudFormation] Organization, Prepare (BusinessOutcomes-Priority-OpsReadiness) / Operate(Metrics,Planned,Unplanned) / Evolve [OPOE]
			Reliability: [Key service-CloudWatch] Requirements-Foundations (Limits/NetwrkTopology) / Change Mgmt / Failure Mgmt(Backup/DR) [RFC]
			Performance Efficiency: [Key service-CloudWatch] Selection Right Resources / Review / Monitoring / Trade-offs [SMRT]
			Security: [Key service-IAM] IAM / Detective Controls / Infrastructure Protection / Data Protection(CIA) / Incident Response [IAM DIDI]
			Business Continuity & DISASTER recovery: Resilience, Fault Tolerance, Redundancy, High Availability
			Scalability and Elasticity(S)
			Ease of Deployment (E)
			Migration (M)
			Hybrid Architecture (H)

	===================================================
	DR strategies:
	===================================================
		- Backup + Restore
		- Pilot Light [Scaled-down version of the production environment. NOT running. Is fired up and scaled only after a disaster happens]
		- Warm Standby[Scaled-down version of the production environment. Always Running and Fired up. Is scaled only after a disaster happens]
		- Hot Standby [ Active - Active sites setup. Route 53 splits the traffic between sites. Web and App servers on both sites are used. But the database used in PRIMARY site. But there is database replication from PRIMARY to SECONDARY.


